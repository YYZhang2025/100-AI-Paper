<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuyang Zhang">

<title>Attention is All You Need(Transformer) – 100 Papers with Codes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../posts/01-transformer/Transformer.html" rel="next">
<link href="../../00-how-to-read-paper.html" rel="prev">
<link href="../.././images/icon.avif" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-11036f1d7bb1f3bddbaef1cc470abb0d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-c63e62e8489b5033ede8f35917775e7b.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-11036f1d7bb1f3bddbaef1cc470abb0d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../style/style.css">
<link rel="stylesheet" href="../../style/callout.css">
<meta property="og:title" content="Attention is All You Need(Transformer) – 100 Papers with Codes">
<meta property="og:description" content="Introduce the Transformer architecture, which relies entirely on self-attention mechanisms for sequence modeling, enabling parallel computation and significantly improving performance on natural language processing tasks.">
<meta property="og:image" content="01-attention.assets/transformer.png">
<meta property="og:site_name" content="100 Papers with Codes">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">100 Papers with Codes</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title">Attention is All You Need(<strong>Transformer</strong>)</h1>
        </a>     
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../.././images/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main tools-wide">
    <a href="https://yuyang.info/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-globe"></i></a>
    <a href="https://www.linkedin.com/in/zhang-yuyang/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-linkedin"></i></a>
    <a href="https://github.com/YYZhang2025" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="mailto:zhangyuyang1211@gmail.com" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-envelope"></i></a>
    <a href="https://x.com/YUYANGZHAN2025" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-twitter"></i></a>
    <a href="https://www.facebook.com/yuyang.zhang.202512" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-facebook"></i></a>
</div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-how-to-read-paper.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">00 Preparation for following</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">01 Transformer</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/01-transformer/post.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Attention is All You Need(<strong>Transformer</strong>)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/01-transformer/Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention is All You Need(<strong>Transformer</strong>)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">02 Vision Transformer</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/02-vision-transformer/Vision-Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(<strong>ViT</strong>)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">03 Swin Transformer</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/03-swin-transformer/03-swin-transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(<strong>Swin Transformer</strong>)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/03-swin-transformer/Swin-Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(<strong>Swin Transformer</strong>)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">04 Clip</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/04-clip/CLIP.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">04: Learning Transferable Visual Models From Natural Language Supervision(<strong>CLIP</strong>)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#transformer-model" id="toc-transformer-model" class="nav-link active" data-scroll-target="#transformer-model"><span class="header-section-number">1</span> Transformer Model</a>
  <ul>
  <li><a href="#sec-input-embedding" id="toc-sec-input-embedding" class="nav-link" data-scroll-target="#sec-input-embedding"><span class="header-section-number">1.1</span> Word Embedding</a></li>
  <li><a href="#sec-positional-encoding" id="toc-sec-positional-encoding" class="nav-link" data-scroll-target="#sec-positional-encoding"><span class="header-section-number">1.2</span> Positional Encoding</a></li>
  <li><a href="#sec-multi-head-attention" id="toc-sec-multi-head-attention" class="nav-link" data-scroll-target="#sec-multi-head-attention"><span class="header-section-number">1.3</span> Multi-Head Attention</a>
  <ul class="collapse">
  <li><a href="#time-complexity-of-multi-head-attention" id="toc-time-complexity-of-multi-head-attention" class="nav-link" data-scroll-target="#time-complexity-of-multi-head-attention"><span class="header-section-number">1.3.1</span> Time Complexity of Multi-Head Attention</a></li>
  <li><a href="#causal-attention" id="toc-causal-attention" class="nav-link" data-scroll-target="#causal-attention"><span class="header-section-number">1.3.2</span> Causal Attention</a></li>
  <li><a href="#cross-attention" id="toc-cross-attention" class="nav-link" data-scroll-target="#cross-attention"><span class="header-section-number">1.3.3</span> Cross Attention</a></li>
  </ul></li>
  <li><a href="#sec-layer-normalization" id="toc-sec-layer-normalization" class="nav-link" data-scroll-target="#sec-layer-normalization"><span class="header-section-number">1.4</span> Layer Normalization</a></li>
  <li><a href="#residual-connection" id="toc-residual-connection" class="nav-link" data-scroll-target="#residual-connection"><span class="header-section-number">1.5</span> Residual Connection</a></li>
  <li><a href="#sec-point-wise-ffn" id="toc-sec-point-wise-ffn" class="nav-link" data-scroll-target="#sec-point-wise-ffn"><span class="header-section-number">1.6</span> Point-Wise Feed Forward Network</a></li>
  <li><a href="#output-linear-projection-softmax" id="toc-output-linear-projection-softmax" class="nav-link" data-scroll-target="#output-linear-projection-softmax"><span class="header-section-number">1.7</span> Output Linear Projection &amp; Softmax</a></li>
  <li><a href="#full-model" id="toc-full-model" class="nav-link" data-scroll-target="#full-model"><span class="header-section-number">1.8</span> Full Model</a></li>
  </ul></li>
  <li><a href="#experiment" id="toc-experiment" class="nav-link" data-scroll-target="#experiment"><span class="header-section-number">2</span> Experiment</a>
  <ul>
  <li><a href="#dataset-preparation-tokenization" id="toc-dataset-preparation-tokenization" class="nav-link" data-scroll-target="#dataset-preparation-tokenization"><span class="header-section-number">2.1</span> Dataset Preparation &amp; Tokenization</a></li>
  <li><a href="#optimizer-learning-rate-scheduler" id="toc-optimizer-learning-rate-scheduler" class="nav-link" data-scroll-target="#optimizer-learning-rate-scheduler"><span class="header-section-number">2.2</span> Optimizer &amp; Learning Rate Scheduler</a></li>
  <li><a href="#loss-curve" id="toc-loss-curve" class="nav-link" data-scroll-target="#loss-curve"><span class="header-section-number">2.3</span> Loss Curve</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">3</span> Summary</a></li>
  <li><a href="#qa" id="toc-qa" class="nav-link" data-scroll-target="#qa"><span class="header-section-number">4</span> Q&amp;A</a>
  <ul>
  <li><a href="#question-1" id="toc-question-1" class="nav-link" data-scroll-target="#question-1"><span class="header-section-number">4.1</span> Question 1</a></li>
  <li><a href="#question-2" id="toc-question-2" class="nav-link" data-scroll-target="#question-2"><span class="header-section-number">4.2</span> Question 2</a></li>
  <li><a href="#question-3" id="toc-question-3" class="nav-link" data-scroll-target="#question-3"><span class="header-section-number">4.3</span> Question 3</a></li>
  <li><a href="#question-4" id="toc-question-4" class="nav-link" data-scroll-target="#question-4"><span class="header-section-number">4.4</span> Question 4</a></li>
  </ul></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">5</span> Further Reading</a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources"><span class="header-section-number">6</span> Additional Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Attention is All You Need(<strong>Transformer</strong>)</h1>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">CV</div>
    <div class="quarto-category">Transformer</div>
    <div class="quarto-category">Attention</div>
  </div>
  </div>



<div class="quarto-title-meta column-body">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yuyang Zhang </p>
          </div>
  </div>
    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p><i>Introduce the Transformer architecture, which relies entirely on self-attention mechanisms for sequence modeling, enabling parallel computation and significantly improving performance on natural language processing tasks.</i></p>
  </div>
</div>


</header>


<ul>
<li>Original Paper: <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></li>
<li>My Implementation: <a href="https://github.com/YYZhang2025/100-AI-Code">GitHub Repository</a></li>
</ul>
<hr>
<p>Transformer Model, as introduced in the paper Attention is all you Need <span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span> has revolutionized the field of natural language processing (NLP) and beyond. This architecture is built entirely on <strong>attention mechanisms</strong>, dispensing with recurrence and convolutions entirely, which allows for <em>greater parallelization and efficiency</em> in training. Nowadays, it has become the backbone of many state-of-the-art models in NLP, computer vision, and other domains. For example, ChatGPT, DeepSeek, and many other large language models (LLMs) are based on the Transformer architecture.</p>
<blockquote class="blockquote">
<p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations <cite> Attention is all you need </cite></p>
</blockquote>
<p>doubt, the Transformer paper will be our first paper to read in this series. In this chapter, we will dive deep into this paper, starting with understanding its background and main contributions.</p>
<section id="transformer-model" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Transformer Model</h1>
<p>In this section, we will delve into the architecture of the Transformer model, breaking down its components and explaining how they work together to process sequential data effectively.</p>
<blockquote class="blockquote">
<p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks … We propose a new simple network architecture, the Transformer, based <strong>solely on attention mechanisms</strong>, dispensing with recurrence and convolutions entirely.</p>
</blockquote>
<p>Below is the architecture of the Transformer model as presented in the original paper:</p>
<div id="fig-transformer-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/transformer.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The Transformer Family Version 2.0 | Lil’Log
</figcaption>
</figure>
</div>
<p>As in the <a href="#fig-transformer-architecture" class="quarto-xref">Figure&nbsp;1</a> , the Transformer model is combined with:</p>
<ul>
<li>Input Embedding: <a href="#sec-input-embedding" class="quarto-xref">Section&nbsp;1.1</a></li>
<li>Position Embedding: <a href="#sec-positional-encoding" class="quarto-xref">Section&nbsp;1.2</a></li>
<li>Multi-Head Attention: <a href="#sec-multi-head-attention" class="quarto-xref">Section&nbsp;1.3</a></li>
<li>Layer Normalization: <a href="#sec-layer-normalization" class="quarto-xref">Section&nbsp;1.4</a></li>
<li>Point-Wise Feed Forward Network: <a href="#sec-point-wise-ffn" class="quarto-xref">Section&nbsp;1.6</a></li>
</ul>
<p>Each small module are combined together to form an Encoder Block and a Decoder Block. The Encoder Block and Decoder Block are then stacked together to form the complete Transformer model. Looks like LEGO blocks, right? Let’s explore each component in detail.</p>
<section id="sec-input-embedding" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sec-input-embedding"><span class="header-section-number">1.1</span> Word Embedding</h2>
<p>Word Embedding is the process of <u>converting tokens into vectors</u>. It is a way of <u>representing words as dense vectors in a continuous vector space</u>. Each word(token) is mapped to a unique vector, and <u>similar words are mapped to similar vectors</u>. This allows the model to capture the semantic meaning of words and their relationships with other words. The Word Embedding is typically learned during the training process of the model.</p>
<hr>
</section>
<section id="sec-positional-encoding" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="sec-positional-encoding"><span class="header-section-number">1.2</span> Positional Encoding</h2>
<blockquote class="blockquote">
<p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.</p>
</blockquote>
<p>One of the problems with transformer is that the model lack the information of the sequence order. To solve this problem, they add “positional encodings” to the input embeddings. The positional encodings have the same dimension <span class="math inline">\(d_\text{model}\)</span> as the word embeddings, so that the two can be summed. There are many choices of positional encodings, <strong>learned</strong> and <strong>fixed</strong> (sinusoidal). In this transformer, they use <em>sine and cosine functions</em> of different frequencies:</p>
<p><span id="eq-position-encoding"><span class="math display">\[
\begin{split}
PE_{(pos,2i)} &amp;= sin(pos / 10000^{2i/d_{model}}) \\
PE_{(pos,2i+1)} &amp;= cos(pos / 10000^{2i/d_{model}})
\end{split}
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(pos\)</span> is the position and <span class="math inline">\(i\)</span> is the dimension. They chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear function of <span class="math inline">\(PE_{pos}\)</span>.</p>
<p>The Transformer uses positional encoding to provide <em>unique positional information for each word</em>, enabling the model to capture the relative positional relationships between words in a sequence.</p>
<div id="fig-position-encoding" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-position-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-position-encoding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-position-100" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-position-100-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/position-100.png" class="img-fluid figure-img" data-ref-parent="fig-position-encoding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-position-100-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Position Encoding with max sequence length 100
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-position-encoding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-position-200" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-position-200-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/position-200.png" class="img-fluid figure-img" data-ref-parent="fig-position-encoding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-position-200-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Position Encoding with max sequence length 100
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-position-encoding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-position-max-len-gif" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-position-max-len-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/positional_encoding.gif" class="img-fluid figure-img" data-ref-parent="fig-position-encoding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-position-max-len-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Position Encoding with max sequence length from 100 to 200
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-position-encoding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-position-log-base-gif" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-position-log-base-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/position-logbase_encoding.gif" class="img-fluid figure-img" data-ref-parent="fig-position-encoding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-position-log-base-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) Position Encoding with log base from 10,000 to 1,000
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-position-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Illustration of Position Encoding with different max sequence lengths, the horizontal red line represent the encoding at position 50. The encoding is consistent across different max sequence lengths, which allows the model to generalize to longer sequences.
</figcaption>
</figure>
</div>
<p>From the <a href="#fig-position-encoding" class="quarto-xref">Figure&nbsp;2</a> , we can see that the positional encoding changes continuously as the max sequence length increases (<a href="#fig-position-max-len-gif" class="quarto-xref">Figure&nbsp;2 (c)</a>). And under different max sequence lengths, the positional encoding changes are the same, the <a href="#fig-position-100" class="quarto-xref">Figure&nbsp;2 (a)</a>, <a href="#fig-position-200" class="quarto-xref">Figure&nbsp;2 (b)</a> show the positional encoding changes at position 50 under different max sequence lengths. We can see that the positional encoding is the same under different max sequence lengths, which allows the model to better generalize to longer sequences.</p>
<div id="fig-position-detail" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-position-detail-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/position-detail.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-position-detail-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Details of Position Encoding
</figcaption>
</figure>
</div>
<p>In the <a href="#fig-position-detail" class="quarto-xref">Figure&nbsp;3</a> , we can see the details of different dimensions of the positional encoding. Compared to dimension (4, 5), dimension (6, 7) changes more with position. From the figure, we can see that:</p>
<ul>
<li>Low <span class="math inline">\(i\)</span> (the earlier dimensions) — short wavelength, changes quickly with position pos → easy to distinguish adjacent tokens;</li>
<li>High <span class="math inline">\(i\)</span> (the later dimensions) — long wavelength, changes slowly with position pos → captures global positional information.</li>
</ul>
<p>Besides, positional encoding can also achieve different effects by changing the base of the sine and cosine functions. For example, the base can be changed from 10000 to 1000, which shortens the wavelength of the positional encoding and makes it easier for the model to capture information from adjacent positions. Figure <a href="#fig-position-log-base-gif" class="quarto-xref">Figure&nbsp;2 (d)</a> shows the changes in positional encoding under different bases.</p>
<p>Combine with Word Embedding, we can get the final input embedding:</p>
<hr>
</section>
<section id="sec-multi-head-attention" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="sec-multi-head-attention"><span class="header-section-number">1.3</span> Multi-Head Attention</h2>
<p>Multi Head Attention is the core module of the Transformer. Its function is to perform multi-head attention calculations on the input vectors, thereby capturing different semantic information. Attention is essentially a <strong>weighted sum</strong> process, which can be seen as a weighted average of the input vectors. The core formula is:</p>
<p><span id="eq-attention"><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\tag{2}\]</span></span></p>
<p>where <span class="math inline">\(Q\)</span>、<span class="math inline">\(K\)</span>、<span class="math inline">\(V\)</span> are the Query, Key, and Value matrices respectively, and <span class="math inline">\(\sqrt{d_k}\)</span> is a scaling factor used to <u>prevent the dot product values from becoming too large, which could lead to vanishing gradients</u>. The <span class="math inline">\(QK^T\)</span> operation computes the similarity between the query and key vectors, resulting in a <strong>score matrix</strong> that indicates how much attention each position should pay to every other position. The softmax function is then applied to this score matrix to obtain the attention weights, which are used to compute a <em>weighted sum</em> of the value vectors.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="#eq-attention" class="quarto-xref">Equation&nbsp;2</a> is the core formula of Attention, and Attention is the core module of Transformer. Understanding this formula is key to understanding Transformer. Many subsequent innovations, such as Linear Attention <span class="citation" data-cites="LinformerSelfAttentionLinear2020wang">(<a href="#ref-LinformerSelfAttentionLinear2020wang" role="doc-biblioref">Wang et al. 2020</a>)</span> and Multi-head Latent Attention(MLA)<span class="citation" data-cites="DeepSeekV2StrongEconomical2024deepseek-ai">(<a href="#ref-DeepSeekV2StrongEconomical2024deepseek-ai" role="doc-biblioref">DeepSeek-AI et al. 2024</a>)</span>, are based on improvements to this formula.</p>
</div>
</div>
<blockquote class="blockquote">
<p>Instead of performing a single attention function with <span class="math inline">\(d_\text{model}\)</span>-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values <span class="math inline">\(h\)</span> times with different, learned linear projections to <span class="math inline">\(d_k\)</span>, <span class="math inline">\(d_k\)</span> and <span class="math inline">\(d_v\)</span> dimensions, respectively.</p>
</blockquote>
<p>Multi-Head Attention is an extension of Attention <a href="#eq-attention" class="quarto-xref">Equation&nbsp;2</a> that splits the input vectors into multiple subspaces (heads) and computes attention independently in each subspace. Finally, the outputs of all subspaces are concatenated to obtain the final output. The formula for Multi-Head Attention is:</p>
<p><span id="eq-multi-head-attention"><span class="math display">\[
\begin{split}
\text{MultiHead}(Q, K, V) &amp;= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{where}\ \text{head}_i &amp;= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{split}
\tag{3}\]</span></span></p>
<p>where <span class="math inline">\(W_i^Q, W_i^K \in \mathbb{R}^{d_\text{model} \times d_k}, W_i^V \in \mathbb{R}^{d_\text{model} \times d_v}\)</span> are weights matrices used to project the input vectors into subspaces, and <span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_\text{model} }\)</span> is a weight matrix used to concatenate the outputs of all subspaces. The purpose of Multi-Head Attention is to <u>capture different semantic information through multiple subspaces, thereby improving the model’s expressive power</u>。</p>
<blockquote class="blockquote">
<p>Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.</p>
</blockquote>
<p>As mentioned in the paper, the use of multiple heads allows the model to jointly attend to information from different representation subspaces at different positions. This is particularly useful for capturing complex patterns in the data.</p>
<section id="time-complexity-of-multi-head-attention" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="time-complexity-of-multi-head-attention"><span class="header-section-number">1.3.1</span> Time Complexity of Multi-Head Attention</h3>
<p>Before we continue to explore other components, let’s analyze the time complexity of Multi-Head Attention. Assuming the input length is <span class="math inline">\(n\)</span> and the dimension of each head is <span class="math inline">\(d_k\)</span>, the time complexity for computing <span class="math inline">\(QK^T\)</span> is <span class="math inline">\(\mathcal{O}(n^2 d_k)\)</span>, this is the standard <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication">matrix multiplication complexity</a>.</p>
<p>Next is the computation of Softmax, which has a time complexity of <span class="math inline">\(\mathcal{O}(n)\)</span> (<a href="https://en.wikipedia.org/wiki/Softmax_function#Computational_complexity_and_remedies">wiki</a>). For each row of the score matrix <span class="math inline">\(QK^T \in \mathbb{R}^{n \times n}\)</span>, we need to compute softmax, which requires <span class="math inline">\(n\)</span> computations. Therefore, the time complexity of Softmax is <span class="math inline">\(\mathcal{O}(n^2)\)</span>.</p>
<p>Then comes the weighted sum for the value, which also has a computational complexity of <span class="math inline">\(\mathcal{O}(n^2d)\)</span>.</p>
<p>The total time complexity of Multi-Head Attention is the sum of the time complexities of these three steps, which is <span class="math inline">\(\mathcal{O}(n^2d)\)</span>: <span id="eq-multi-head-attention-complexity"><span class="math display">\[
\begin{array}{|l|l|}
\hline
\textbf{Step} &amp; \textbf{Time Complexity} \\
\hline
QK^\top &amp; \mathcal{O}(n^2 d) \\
\text{softmax}(QK^\top) &amp; \mathcal{O}(n^2) \\
\text{attention} \times V &amp; \mathcal{O}(n^2 d) \\
\hline
\textbf{Total} &amp; \mathcal{O}(n^2 d) \\
\hline
\end{array}
\tag{4}\]</span></span></p>
<p>Here is the compare of time complexity between Multi-Head Attention, RNN and CNN: <img src="01-attention.assets/time-complexity.png" id="fig-time-complexity" class="img-fluid" alt="The comparison of time complexity between Multi-Head Attention, RNN and CNN"></p>
</section>
<section id="causal-attention" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="causal-attention"><span class="header-section-number">1.3.2</span> Causal Attention</h3>
<p>Causal Attention is a special type of attention mechanism used in the Transformer model, particularly in the decoder part. The purpose of Causal Attention is to <u>prevent the model from seeing future information during training</u>, thereby ensuring the model’s autoregressive property. Specifically, Causal Attention <u>masks future information</u> when computing attention, allowing the model to only attend to current and past information.</p>
<p>The formula for Causal Attention is as follows: <span id="eq-causal-attention"><span class="math display">\[
\text{CausalAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
\tag{5}\]</span></span></p>
<p>In this formula, <span class="math inline">\(M\)</span> is a mask matrix that serves to block out future information. <span class="math inline">\(M\)</span> is an <strong>upper triangular matrix</strong> where the elements below the diagonal are 0, and the elements above the diagonal are <span class="math inline">\(-\infty\)</span>. This way, when computing the Softmax, the elements above the diagonal are masked out, ensuring that the model can only see current and past information.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Masking in Transformer">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Masking in Transformer
</div>
</div>
<div class="callout-body-container callout-body">
<p>When apply masking in the transformer, there is also a <strong>padding mask</strong>. The padding mask is used to mask out the padding tokens in the input sequence, which are added to make all sequences in a batch have the same length. The padding mask is a binary matrix where the elements corresponding to padding tokens are 0, and the elements corresponding to non-padding tokens are 1. When computing attention score, we need to apply padding mask and causal mask together <code>causal_mask | padding_mask</code>. The combined mask is obtained by taking the element-wise minimum of the padding mask and the causal mask. This way, we can ensure that the model only attends to valid tokens in the input sequence.</p>
</div>
</div>
<div id="fig-attention-mask" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/attention_mask.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Illustration of Attention Mask with Padding mask
</figcaption>
</figure>
</div>
</section>
<section id="cross-attention" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="cross-attention"><span class="header-section-number">1.3.3</span> Cross Attention</h3>
<p>Cross Attention is another important attention mechanism used in the Transformer model, particularly in the decoder part. The purpose of Cross Attention is to <u>allow the decoder to attend to the encoder’s output</u>, thereby enabling the model to generate output sequences based on the input sequences. The cross attention has same structure as self-attention <a href="#eq-attention" class="quarto-xref">Equation&nbsp;2</a>, but the query comes from the previous decoder layer, and the key and value come from the output of the encoder.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Application of Cross Attention">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Application of Cross Attention
</div>
</div>
<div class="callout-body-container callout-body">
<p>Cross Attention can be applied in various tasks, such as Visual Question Answering (VQA), where the model needs to attend to both image features and question text. By using Cross Attention, the model can effectively integrate information from different modalities, leading to improved performance in tasks that require understanding of both visual and textual information.</p>
</div>
</div>
</section>
</section>
<section id="sec-layer-normalization" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="sec-layer-normalization"><span class="header-section-number">1.4</span> Layer Normalization</h2>
<p>Layer Normalization (LayerNorm) is a technique used to normalize the inputs of a neural network layer. It is similar to Batch Normalization, but instead of normalizing across the batch dimension, it normalizes across the feature dimension. This means that for each position in the sequence, LayerNorm computes the mean and variance of the features and normalizes them accordingly.</p>
<div id="fig-layer-normalization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-layer-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/layer_batch_norm.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-layer-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Illustration of Layer Normalization and Batch Normalization, where <span class="math inline">\(P_i\)</span> is the position index, <span class="math inline">\(i_j\)</span> is the hidden feature dimension index
</figcaption>
</figure>
</div>
<p>The formula for Layer Normalization is: <span id="eq-layer-normalization"><span class="math display">\[
\text{LayerNorm}(\mathrm{x}_i) = \frac{\mathrm{x}_i - \mu_i}{\sigma_i + \epsilon} \cdot \gamma + \beta
\tag{6}\]</span></span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is a small constant added to the variance to prevent division by zero, and <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are <em>learnable parameters</em> that allow the model to scale and shift the normalized output.</p>
<p>For a batch of inputs, Layer Normalization normalizes the vectors at each position <a href="#fig-layer-normalization" class="quarto-xref">Figure&nbsp;5</a>, rather than normalizing across the entire batch. This allows Layer Normalization to better adapt to sequences of varying lengths, thereby improving model performance. Specifically, for <span class="math inline">\(x \in \mathbb{R}^{B \times H \times S \times d_v}\)</span>, we normalize along the <span class="math inline">\(d_v\)</span> dimension for each position, rather than normalizing across the entire batch. So, there is no <code>running mean</code> and <code>running variance</code> in LayerNorm, which is different from BatchNorm.</p>
<hr>
</section>
<section id="residual-connection" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="residual-connection"><span class="header-section-number">1.5</span> Residual Connection</h2>
<p>Residual Connection, also known as skip connection, is a technique used to improve the training of deep neural networks. It allows the input of a layer to be added directly to the output of the layer, thereby creating a shortcut for the gradient to flow through. This helps to alleviate the problem of <strong>vanishing gradients</strong> and allows for deeper networks to be trained effectively.</p>
<div id="fig-residual-connection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/residuakl-connection.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Illustration of Residual Connection
</figcaption>
</figure>
</div>
<p>The formula for Residual Connection is: <span id="eq-residual-connection"><span class="math display">\[
\mathbf{y} = \text{LayerNorm}(\mathbf{x} + \mathrm{Sublayer}(\mathbf{x}))
\tag{7}\]</span></span></p>
<p>During training, the residual connection provides a “shortcut” for the gradient, allowing it to bypass the complex nonlinear transformations of the sub-layer and be directly passed back to the input of the previous layer. This effectively alleviates the problem of vanishing gradients, as shown in the following formula:</p>
<p><span id="eq-residual-connection-gradient"><span class="math display">\[
\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{x}}
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \\
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \left( \mathbf{I} + \frac{\partial \mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}} \right) \\
&amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}}}_{\text{straight path}} +
\underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot
\frac{\partial\,\mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}}}_{\text{through the sub-layer}}
\end{split}
\tag{8}\]</span></span></p>
<p><a href="#eq-residual-connection-gradient" class="quarto-xref">Equation&nbsp;8</a> shows the effect of residual connections on gradients. Here, <span class="math inline">\(\mathcal{L}\)</span> is the loss function. We can see that due to the presence of the first term, even if the gradient of the sub-layer approaches 0, the information of the gradient will not be completely lost.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Pre-Normalization vs Post-Normalization">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pre-Normalization vs Post-Normalization
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the original Transformer, the normalization is placed after the Residual Connection (Post-Normalization) <a href="#eq-residual-connection" class="quarto-xref">Equation&nbsp;7</a> . However, in subsequent research, many models (such as BERT) place the normalization before the Residual Connection (Pre-Normalization). It shows that the pre-normalization can has more stable training process, and no need warm up learning rate <span class="citation" data-cites="LayerNormalizationTransformer2020xiong">(<a href="#ref-LayerNormalizationTransformer2020xiong" role="doc-biblioref">Xiong et al. 2020</a>)</span>. The formula for Pre-Normalization is: <span class="math display">\[
\text{Output} = \mathrm{Sublayer}(\mathbf{x}) + \mathrm{LayerNorm}(\mathbf{x})
\]</span></p>
</div>
</div>
</section>
<section id="sec-point-wise-ffn" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="sec-point-wise-ffn"><span class="header-section-number">1.6</span> Point-Wise Feed Forward Network</h2>
<blockquote class="blockquote">
<p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.</p>
</blockquote>
<p>Point-Wise Feed Forward Network is used to further process the output of the Multi-Head Attention. It consists of two linear transformations with a ReLU activation in between. The output of the first linear transformation is passed through the ReLU activation function, and then the result is passed through the second linear transformation. The purpose of Point-Wise Feed Forward Network is to <u>introduce non-linearity and increase the model’s capacity to capture complex patterns in the data</u>. It has formula as follows:</p>
<p><span id="eq-point-wise-ffn"><span class="math display">\[
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\tag{9}\]</span></span></p>
<p>where <span class="math inline">\(W_1\)</span> <span class="math inline">\(\in \mathbb{R}^{d_\text{model} \times d_\text{ff}}\)</span> and <span class="math inline">\(W_2 \in \mathbb{R}^{d_\text{ff} \times d_\text{model}}\)</span> are weight matrices, and <span class="math inline">\(b_1 \in \mathbb{R}^{d_\text{ff}}\)</span> and <span class="math inline">\(b_2 \in \mathbb{R}^{d_\text{model}}\)</span> are bias terms. The ReLU function is applied element-wise, which introduces non-linearity to the model.</p>
</section>
<section id="output-linear-projection-softmax" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="output-linear-projection-softmax"><span class="header-section-number">1.7</span> Output Linear Projection &amp; Softmax</h2>
<p>In the Transformer model, after the input sequence has been processed by the Encoder and Decoder blocks, the final step is to generate the output sequence. This is done by applying a linear transformation followed by a Softmax function to the output of the Decoder. The purpose of this step is to convert the output vectors into a probability distribution over the vocabulary, allowing the model to predict the next word in the sequence. The formula for this step is as follows: <span id="eq-output-softmax"><span class="math display">\[
\text{Output} = \text{Softmax}(xW + b)
\tag{10}\]</span></span></p>
<p>where <span class="math inline">\(x\)</span> is the output of the Decoder, and <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span> are the weight matrix and bias term of the linear transformation, respectively. The Softmax function is applied to the output of the linear transformation to obtain a probability distribution over the vocabulary.</p>
<p>The weight matrix <span class="math inline">\(W\)</span> has a shape of <span class="math inline">\(\mathbb{R}^{d_\text{model} \times V}\)</span>, where <span class="math inline">\(V\)</span> is the size of the vocabulary. This means that for each output vector, the linear transformation produces a vector of logits with a length equal to the vocabulary size. The Softmax function then converts these logits into probabilities, which can be interpreted as the likelihood of each word in the vocabulary being the next word in the sequence. And author use <strong>weight tying</strong><span class="citation" data-cites="UsingOutputEmbedding2017press">(<a href="#ref-UsingOutputEmbedding2017press" role="doc-biblioref">Press and Wolf 2017</a>)</span> to share the weights between the input embedding of decoder and the output projection, which can reduce the number of parameters and improve the model’s performance.</p>
</section>
<section id="full-model" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="full-model"><span class="header-section-number">1.8</span> Full Model</h2>
<p>The full transformer model is composed of multiple Encoder Blocks and Decoder Blocks, each containing the components we have discussed so far. The full transformer model is illustrated in the figure <a href="#fig-transformer-architecture" class="quarto-xref">Figure&nbsp;1</a> .</p>
<p>Transformer的完整模型架构如图 <a href="#fig-transformer-architecture" class="quarto-xref">Figure&nbsp;1</a> 所示。Transformer由多个Encoder Block和Decoder Block组成，每个Encoder Block和Decoder Block都包含了前面介绍的模块。Encoder Block和Decoder Block的结构是相似的，都是由Multi-Head Attention、Point-Wise Feed Forward Network、Layer Normalization和Residual Connection组成的。</p>
<p>下图是整个Transformer的编码和解码过程的示意图：</p>
<div id="fig-full-transformer" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-full-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-full-transformer" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-transformer-encoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-transformer-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/transformer-encoding.gif" class="img-fluid figure-img" data-ref-parent="fig-full-transformer">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-transformer-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Transformer Encoding Process
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-full-transformer" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-transformer-decoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-transformer-decoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/transformer-decoding.gif" class="img-fluid figure-img" data-ref-parent="fig-full-transformer">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-transformer-decoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Transformer Decoding Process
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-full-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Illustrate of Transformer Encoding and Decoding Process (Image Source: <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>)
</figcaption>
</figure>
</div>
<p>As in the <a href="#fig-full-transformer" class="quarto-xref">Figure&nbsp;7</a> , the encoding process and decoding process of the Transformer are similar. The encoding process converts the input tokens into vectors, then encodes them through multiple Encoder Blocks, and finally converts them into a probability distribution over the vocabulary through a linear transformation and Softmax function. The decoding process, on the other hand, performs cross-attention between the output of the Encoder and the input of the Decoder, then decodes them through multiple Decoder Blocks, and finally converts them into a probability distribution over the vocabulary through a linear transformation and Softmax function.</p>
</section>
</section>
<section id="experiment" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Experiment</h1>
<p>The training of the Transformer model is similar to other neural network models, and mainly includes the following steps:</p>
<ul>
<li>Data Preprocessing: Convert the input text data into tokens, and perform tokenization and encoding. Typically, Word Embedding is used to convert each token into a fixed-dimensional vector. In the transformer, they use <strong>Byte Pair Encoding (BPE)</strong> to tokenize the input text data.</li>
<li>Model Initialization: Initialize the parameters of the Transformer model, including the parameters of the Word Embedding, Position Embedding, Multi-Head Attention, Point-Wise Feed Forward Network, and other modules.</li>
<li>Forward Propagation: Pass the input tokens through the Word Embedding and Position Embedding to convert them into vectors, then encode them through multiple Encoder Blocks, and finally convert them into a probability distribution over the vocabulary through a linear transformation and Softmax function.</li>
<li>Loss Calculation: Use the Cross-Entropy Loss function with <strong>label smoothing</strong> to calculate the difference between the model’s output and the true labels.</li>
<li>Backpropagation: Calculate the gradients of the loss function with respect to the model parameters, and use gradient descent algorithms (such as <strong>Adam optimizer</strong>) to update the model parameters.</li>
</ul>
<section id="dataset-preparation-tokenization" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="dataset-preparation-tokenization"><span class="header-section-number">2.1</span> Dataset Preparation &amp; Tokenization</h2>
<p>The dataset we are using is the <a href="https://www.kaggle.com/datasets/jeanonghuifang/iwslt2017-en-zh">iwslt2017-en-zh</a>, which is a small dataset for English to Chinese translation. You can download the dataset:</p>
<p>After downloading the dataset, we need to preprocess the data and convert the text data into tokens. We use the <a href="https://huggingface.co/docs/tokenizers/index">Hugging Face Tokenizers</a> library to perform tokenization and encoding. We use <strong>Byte Pair Encoding (BPE)</strong> to tokenize the input text data. The code is as follows:</p>
</section>
<section id="optimizer-learning-rate-scheduler" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="optimizer-learning-rate-scheduler"><span class="header-section-number">2.2</span> Optimizer &amp; Learning Rate Scheduler</h2>
<p>We use the Adam optimizer to optimize the model parameters. The learning rate scheduler is the same as the one used in the original Transformer paper, which is a warm-up learning rate scheduler: <span id="eq-learning-rate-scheduler"><span class="math display">\[
\text{lrate} = d_\text{model}^{-0.5} \cdot \min(\text{step\_num}^{-0.5}, \text{step\_num} \cdot \text{warmup\_steps}^{-1.5})
\tag{11}\]</span></span></p>
</section>
<section id="loss-curve" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="loss-curve"><span class="header-section-number">2.3</span> Loss Curve</h2>
<p>Here is the Loss Curve after training <code>10,000</code> steps: <img src="./01-attention.assets/loss_curve.png" id="fig-loss-curve" class="img-fluid" alt="The Loss Curve"></p>
<p>Congratulations! You have successfully implemented the Transformer, which is currently the most important AI model framework. By understanding it, you can comprehend most AI models. The popular models like ChatGPT and DeepSeek are all based on variations of the Transformer (we will read about these models in the upcoming articles). The complete code can be found on <a href="https://github.com/YYZhang2025/100-AI-Code">GitHub</a>.</p>
</section>
</section>
<section id="summary" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Summary</h1>
<p>In this article, we have learned about the Transformer model, which is a powerful model for processing sequential data. We have explored its components, including Word Embedding, Positional Encoding, Multi-Head Attention, Layer Normalization, Residual Connection, and Point-Wise Feed Forward Network. We have also discussed the training process of the Transformer model, including data preprocessing, model initialization, forward propagation, loss calculation, and backpropagation.</p>
</section>
<section id="qa" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Q&amp;A</h1>
<section id="question-1" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="question-1"><span class="header-section-number">4.1</span> Question 1</h2>
<p>Why is the dot product scaled by <span class="math inline">\(\sqrt{d_k}\)</span>?</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If not scaled, when <span class="math inline">\(d_k\)</span> is large, the variance of QK also increases, causing the softmax to fall into regions with very small gradients. Dividing by <span class="math inline">\(\sqrt{d_k}\)</span> helps keep the activation values in a range suitable for training.</p>
</div>
</div>
</div>
</section>
<section id="question-2" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="question-2"><span class="header-section-number">4.2</span> Question 2</h2>
<p>What problem does Multi-Head Attention solve?</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>It allows the model to attend to information from different representation subspaces at different positions, overcoming the tendency of single-head self-attention to “average out” information.</p>
</div>
</div>
</div>
</section>
<section id="question-3" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="question-3"><span class="header-section-number">4.3</span> Question 3</h2>
<p>What is the purpose of positional encoding in the Transformer?</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>It provides the model with information about the order of the sequence, enabling it to capture positional relationships between tokens.</p>
</div>
</div>
</div>
</section>
<section id="question-4" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="question-4"><span class="header-section-number">4.4</span> Question 4</h2>
<p>What is the time complexity of Multi-Head Attention compare to RNN/CNN?</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The time complexity of Multi-Head Attention is <span class="math inline">\(\mathcal{O}(n²d)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length and <span class="math inline">\(d\)</span> is the model dimension. In contrast, RNNs have a time complexity of <span class="math inline">\(\mathcal{O}(n)\)</span> due to their sequential nature, while CNNs require stacking multiple layers to capture long-range dependencies.</p>
</div>
</div>
</div>
</section>
</section>
<section id="further-reading" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Further Reading</h1>
<p>Now, we have understand what is the transformer model, and how it works. Based on the transformer model, there are many variants and improvements. As we mentioned before, the time-complexity of Multi-Head Attention is <span class="math inline">\(\mathcal{O}(n^2d)\)</span>, which is <strong>quadratic to the sequence length</strong>. This makes it difficult to apply the transformer model to long sequences. To solve this problem, many researchers have proposed various efficient transformer models, such as:</p>
<ul>
<li>Flash Attention <span class="citation" data-cites="FlashAttention2FasterAttention2023dao">(<a href="#ref-FlashAttention2FasterAttention2023dao" role="doc-biblioref">Dao 2023</a>)</span>: An efficient implementation of the attention mechanism that reduces memory usage and speeds up training.</li>
<li>Linformer <span class="citation" data-cites="LinformerSelfAttentionLinear2020wang">(<a href="#ref-LinformerSelfAttentionLinear2020wang" role="doc-biblioref">Wang et al. 2020</a>)</span>: A linear transformer model that reduces the time complexity of attention from <span class="math inline">\(\mathcal{O}(n^2d)\)</span> to <span class="math inline">\(\mathcal{O}(nd)\)</span></li>
<li>Grouped-Query Attention (GQA) <span class="citation" data-cites="GQATrainingGeneralized2023ainslie">(<a href="#ref-GQATrainingGeneralized2023ainslie" role="doc-biblioref">Ainslie et al. 2023</a>)</span>: A transformer model that reduces the time complexity of attention by grouping queries.</li>
</ul>
<p>The Transformer is a model implemented in the NLP, how to apply it to other domains, such as computer vision? Here are some of the models:</p>
<ul>
<li>Vision Transformer (ViT) <span class="citation" data-cites="ImageWorth16x162021dosovitskiy">(<a href="#ref-ImageWorth16x162021dosovitskiy" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span>: A transformer model for image classification that treats images as sequences of patches.</li>
<li>Diffusion Transformer <span class="citation" data-cites="ScalableDiffusionModels2023peebles">(<a href="#ref-ScalableDiffusionModels2023peebles" role="doc-biblioref">Peebles and Xie 2023</a>)</span>: A transformer model for image generation that combines diffusion models with transformers.</li>
</ul>
<p>For the Feed Forward Network, there are also has many variants, such as:</p>
<ul>
<li>Gated Linear Units (GLU) <span class="citation" data-cites="GLUVariantsImprove2020shazeer">(<a href="#ref-GLUVariantsImprove2020shazeer" role="doc-biblioref">Shazeer 2020</a>)</span>: A variant of the feed forward network that uses gating mechanisms.</li>
<li>Mixture of Experts (MoE) <span class="citation" data-cites="SwinTransformerHierarchical2021liu">(<a href="#ref-SwinTransformerHierarchical2021liu" role="doc-biblioref">Liu et al. 2021</a>)</span>: A variant of the feed forward network that uses multiple experts to improve performance.</li>
</ul>
<p>There are many to explore, and we will read about these models in the upcoming articles.</p>
</section>
<section id="additional-resources" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Additional Resources</h1>
<p>There are many great resources to help you understand the Transformer model better. Here are some of them:</p>
<ul>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>: A visual and intuitive explanation of the Transformer model.</li>
<li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">Annotated Transformer</a>: A detailed explanation of the Transformer model with code annotations.</li>
<li><a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">The Transformer Family Version 2.0</a>: A comprehensive overview of the Transformer model and its variants.</li>
</ul>



</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-GQATrainingGeneralized2023ainslie" class="csl-entry" role="listitem">
Ainslie, Joshua, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. <span>“<span>GQA</span>: <span>Training Generalized Multi-Query Transformer Models</span> from <span>Multi-Head Checkpoints</span>.”</span> December 23, 2023. <a href="https://doi.org/10.48550/arXiv.2305.13245">https://doi.org/10.48550/arXiv.2305.13245</a>.
</div>
<div id="ref-FlashAttention2FasterAttention2023dao" class="csl-entry" role="listitem">
Dao, Tri. 2023. <span>“<span>FlashAttention-2</span>: <span>Faster Attention</span> with <span>Better Parallelism</span> and <span>Work Partitioning</span>.”</span> July 17, 2023. <a href="https://doi.org/10.48550/arXiv.2307.08691">https://doi.org/10.48550/arXiv.2307.08691</a>.
</div>
<div id="ref-DeepSeekV2StrongEconomical2024deepseek-ai" class="csl-entry" role="listitem">
DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, et al. 2024. <span>“<span>DeepSeek-V2</span>: <span>A Strong</span>, <span>Economical</span>, and <span class="nocase">Efficient Mixture-of-Experts Language Model</span>.”</span> June 19, 2024. <a href="https://doi.org/10.48550/arXiv.2405.04434">https://doi.org/10.48550/arXiv.2405.04434</a>.
</div>
<div id="ref-ImageWorth16x162021dosovitskiy" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An <span>Image</span> Is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image Recognition</span> at <span>Scale</span>.”</span> June 3, 2021. <a href="https://doi.org/10.48550/arXiv.2010.11929">https://doi.org/10.48550/arXiv.2010.11929</a>.
</div>
<div id="ref-SwinTransformerHierarchical2021liu" class="csl-entry" role="listitem">
Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. <span>“Swin <span>Transformer</span>: <span>Hierarchical Vision Transformer</span> Using <span>Shifted Windows</span>.”</span> August 17, 2021. <a href="https://doi.org/10.48550/arXiv.2103.14030">https://doi.org/10.48550/arXiv.2103.14030</a>.
</div>
<div id="ref-ScalableDiffusionModels2023peebles" class="csl-entry" role="listitem">
Peebles, William, and Saining Xie. 2023. <span>“Scalable <span>Diffusion Models</span> with <span>Transformers</span>.”</span> March 2, 2023. <a href="https://doi.org/10.48550/arXiv.2212.09748">https://doi.org/10.48550/arXiv.2212.09748</a>.
</div>
<div id="ref-UsingOutputEmbedding2017press" class="csl-entry" role="listitem">
Press, Ofir, and Lior Wolf. 2017. <span>“Using the <span>Output Embedding</span> to <span>Improve Language Models</span>.”</span> February 21, 2017. <a href="https://doi.org/10.48550/arXiv.1608.05859">https://doi.org/10.48550/arXiv.1608.05859</a>.
</div>
<div id="ref-GLUVariantsImprove2020shazeer" class="csl-entry" role="listitem">
Shazeer, Noam. 2020. <span>“<span>GLU Variants Improve Transformer</span>.”</span> February 12, 2020. <a href="https://doi.org/10.48550/arXiv.2002.05202">https://doi.org/10.48550/arXiv.2002.05202</a>.
</div>
<div id="ref-AttentionAllYou2023vaswani" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention <span>Is All You Need</span>.”</span> August 2, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-LinformerSelfAttentionLinear2020wang" class="csl-entry" role="listitem">
Wang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. <span>“Linformer: <span>Self-Attention</span> with <span>Linear Complexity</span>.”</span> June 14, 2020. <a href="https://doi.org/10.48550/arXiv.2006.04768">https://doi.org/10.48550/arXiv.2006.04768</a>.
</div>
<div id="ref-LayerNormalizationTransformer2020xiong" class="csl-entry" role="listitem">
Xiong, Ruibin, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. <span>“On <span>Layer Normalization</span> in the <span>Transformer Architecture</span>.”</span> June 29, 2020. <a href="https://doi.org/10.48550/arXiv.2002.04745">https://doi.org/10.48550/arXiv.2002.04745</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../../00-how-to-read-paper.html" class="pagination-link" aria-label="00 Preparation for following">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">00 Preparation for following</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../posts/01-transformer/Transformer.html" class="pagination-link" aria-label="Attention is All You Need(**Transformer**)">
        <span class="nav-page-text">Attention is All You Need(<strong>Transformer</strong>)</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>