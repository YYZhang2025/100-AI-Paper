[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This website aims to collect and showcase 100 important AI papers and their code implementations. Each paper is accompanied by a link for readers to explore further.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This website aims to collect and showcase 100 important AI papers and their code implementations. Each paper is accompanied by a link for readers to explore further.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#why-build-this-website",
    "href": "index.html#why-build-this-website",
    "title": "100 Papers with Code",
    "section": "Why build this website?",
    "text": "Why build this website?\nOn the one head, during my study, reading, and practice in the field of AI, I found that many important papers and code implementations were scattered in various places. To facilitate access for myself and others, I decided to build this website to consolidate these important papers and my implementations in one place. On the other hand, there is a gap between topic in papers and practical application. By providing code implementations, I hope to help bridge this gap and make it easier for practitioners to apply the latest research results.\nThere are some awesome resource about the paper and implementation such as:\n\nAnnotated Research Paper: Collection of simple PyTorch implementations of neural networks and related algorithms.\nPapers with Code (It was replace by Hugging Face now): The largest resource for finding machine learning papers, code and evaluation tables.\n\nBut why I still build this website?\n\nThe Annotated Research Paper focuses more on each component of the paper, it didnâ€™t provide the whole modeling and training process.\nThe Papers with Code provides the paper and code, but the code is sometime hard to understand, and it didnâ€™t provide the explanation of the code.\n\nThis website is built to fill these gaps by providing clear explanations and easy-to-understand code implementations for each paper.Each page(paper) will come with a self-contained Jupyter Notebook that can be run directly, making it easier for readers to understand and apply the concepts presented in the papers.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-website",
    "href": "index.html#how-to-use-this-website",
    "title": "100 Papers with Code",
    "section": "How to use this website?",
    "text": "How to use this website?\nThis website is designed to be user-friendly and accessible for anyone interested in understanding and implementing the latest research in AI and machine learning. Each paper has its own dedicated page that includes\n\nThe explanation of the paper.\nCode implementation.\n\nQ & A part (you can use as flash cards).\nThe further direction might be interesting to explore (According to my understanding).\nBelow is the list of papers that have been implemented or are planned to be documented. You can click on the paper title to view detailed content.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need (Transformer )\nTransformer æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ï¼Œåœ¨è¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ä½œä¸º GPTã€BERT ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒåŸºç¡€ï¼Œæ¨åŠ¨äº†å½“ä»Šç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ã€‚\n\nNLP / Transformer\n\n\n02\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ( Vision Transformer )\nVision Transformer (ViT) æ˜¯ä¸€ç§å°†å›¾åƒåˆ’åˆ†ä¸ºå°å—ï¼ˆpatchesï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸º token è¾“å…¥æ ‡å‡† Transformer æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»çš„æ¶æ„ï¼Œé¦–æ¬¡å®ç°äº†çº¯æ³¨æ„åŠ›æœºåˆ¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨ã€‚\n\nComputer Vision / Transformer\n\n\n03\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows ( Swin Transformer )\nSwin Transformer æ˜¯ä¸€ç§ä½¿ç”¨å±‚æ¬¡åŒ–ç»“æ„å’Œæ»‘åŠ¨çª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è§†è§‰ Transformer æ¨¡å‹ï¼Œæ—¢ä¿ç•™äº†å±€éƒ¨å»ºæ¨¡çš„é«˜æ•ˆæ€§ï¼Œåˆé€šè¿‡çª—å£åç§»å®ç°è·¨åŒºåŸŸä¿¡æ¯äº¤äº’ï¼Œé€‚ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¤šç§è§†è§‰ä»»åŠ¡ã€‚\n\nComputer Vision / Transformer\n\n\n04\nLearning Transferable Visual Models From Natural Language Supervision ( CLIP )\nCLIP æ˜¯ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡å›¾æ–‡å¯¹æ¯”å­¦ä¹ ï¼Œå°†å›¾åƒä¸è‡ªç„¶è¯­è¨€æ˜ å°„åˆ°åŒä¸€è¯­ä¹‰ç©ºé—´ï¼Œä»è€Œå®ç°é›¶æ ·æœ¬å›¾åƒè¯†åˆ«ä¸è·¨æ¨¡æ€æ£€ç´¢çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹\n\nComputer Vision / Transformer\n\n\n05\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness ( Flash Attention )\nFlashAttention æ˜¯ä¸€ç§ä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°ï¼Œé€šè¿‡å‡å°‘å†…å­˜è®¿é—®å’Œæå‡è®¡ç®—æ•ˆç‡ï¼Œå®ç°æ›´å¿«ã€æ›´èŠ‚çœèµ„æºçš„ Transformer æ¨ç†ä¸è®­ç»ƒã€‚\n\nTransformer / AI Engine",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/01-transformer/Transformer.html",
    "href": "posts/01-transformer/Transformer.html",
    "title": "Attention is All You Need(Transformer)",
    "section": "",
    "text": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations  Attention_is_all_you_need, p.2  THis\n\n\n\n\n Back to top",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/04-clip/CLIP.html",
    "href": "posts/04-clip/CLIP.html",
    "title": "04: Learning Transferable Visual Models From Natural Language Supervision(CLIP)",
    "section": "",
    "text": "1 CLIP\nThis is the link to the Vision-Transformer\nThis is another link to the Transformer\nTHis is link to the Swin-Transformer\n\n\n2 Summary\n\n\n3 Key Concepts\n\n\n4 Q & A\n\n\n5 æ‰©å±•\n\n\n\n\n Back to top",
    "crumbs": [
      "04 Clip",
      "04: Learning Transferable Visual Models From Natural Language Supervision(**CLIP**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/03-swin-transformer.html",
    "href": "posts/03-swin-transformer/03-swin-transformer.html",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "åœ¨é˜…è¯»Swin Transformerä¹‹å‰ï¼Œæˆ‘ä»¬å›é¡¾ä¸€ä¸‹Vision Transformeræ˜¯ä»€ä¹ˆã€‚ ViT åœ¨å¤„ç†å›¾åƒæ—¶ï¼Œä¼šå°†æ•´å¼ å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„patchï¼Œå¹¶è¿›è¡Œglobal self-attentionçš„è®¡ç®—ï¼Œä»è€Œæ•æ‰å›¾åƒä¸­çš„å…¨å±€ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š\nä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒSwin Transformer (Liu et al. 2021) æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼Œå®ƒé€šè¿‡å¼•å…¥å±‚æ¬¡åŒ–çš„ç‰¹å¾è¡¨ç¤ºå’Œç§»åŠ¨çª—å£æœºåˆ¶ï¼Œæ¥æœ‰æ•ˆåœ°æ•æ‰å›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚Swin Transformer çš„ä¸»è¦è´¡çŒ®æ˜¯ï¼š\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥è¯¦ç»†ä»‹ç» Swin Transformer çš„æ¶æ„å’Œå…³é”®æŠ€æœ¯ã€‚",
    "crumbs": [
      "03 Swin Transformer",
      "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/03-swin-transformer.html#swin-transformer-æ¶æ„",
    "href": "posts/03-swin-transformer/03-swin-transformer.html#swin-transformer-æ¶æ„",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1 Swin Transformer æ¶æ„",
    "text": "1 Swin Transformer æ¶æ„\n\n\n\n\n\n\nFigureÂ 1: The Swin Transformer Architecture\n\n\n\nå¦‚å›¾ FigureÂ 1 æ‰€ç¤ºï¼ŒSwin Transformer çš„æ¶æ„ç”±ä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼š\n\nPatch Partition: å°†è¾“å…¥å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„patchï¼Œç±»ä¼¼äº ViTã€‚\nPatch Merging ï¼ˆå›¾ä¸­çº¢è‰²éƒ¨åˆ†ï¼‰: åœ¨æ¯ä¸ªé˜¶æ®µé€šè¿‡åˆå¹¶ç›¸é‚»çš„patch, é€æ­¥å‡å°‘tokenæ•°é‡ï¼Œæ„å»ºå±‚æ¬¡åŒ–çš„ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡å°†ç›¸é‚»çš„patchè¿›è¡Œæ‹¼æ¥å’Œçº¿æ€§å˜æ¢ï¼Œå‡å°‘ç‰¹å¾å›¾çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶å¢åŠ é€šé“æ•°ï¼ˆå›¾ä¸­ è“è‰² éƒ¨åˆ†ï¼‰ã€‚\nWindow-based Multi-head Self Attention (W-MSA) ï¼ˆå›¾ä¸­æ©™è‰²éƒ¨åˆ†ï¼‰: åœ¨å›ºå®šå¤§å°çš„éé‡å çª—å£å†…è¿›è¡Œè‡ªæ³¨æ„åŠ›æ“ä½œï¼Œå°†å¤æ‚åº¦ä» \\(O(( \\frac{HW}{P^2})^2)\\) é™åˆ° \\(O(M^2 \\cdot \\frac{HW}{M^2}) = O(HW \\cdot M^2)\\)ï¼Œå…¶ä¸­ \\(M\\) æ˜¯çª—å£å¤§å°.\nShifted Window Multi-head Self Attention (SW-MSA) ï¼ˆå›¾ä¸­æ©™è‰²éƒ¨åˆ†ï¼‰: é€šè¿‡åœ¨ç›¸é‚»å±‚ä¹‹é—´ç§»åŠ¨çª—å£ä½ç½®ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯æ•æ‰èƒ½åŠ›ã€‚\n\n\n1.1 Patch Partition\nå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¾“å…¥å›¾åƒ \\(X\\)ï¼Œå…¶å¤§å°ä¸º \\(H \\times W \\times 3\\)ï¼Œå…¶ä¸­ \\(H\\) æ˜¯é«˜åº¦ï¼Œ\\(W\\) æ˜¯å®½åº¦ï¼Œ\\(3\\) æ˜¯é€šé“æ•°ã€‚æˆ‘ä»¬å°†å›¾åƒåˆ†å‰²æˆå¤§å°ä¸º \\(P \\times P\\) çš„patches, ä¹‹åå°†è¿™äº›raw pixel concentration åœ¨ä¸€èµ·ï¼Œå°±å¾—åˆ°äº†ä¸€ä¸ª tokenã€‚ å‡è®¾\\(P=4\\)ï¼Œæ¯ä¸ªtokençš„å¤§å°ä¸º \\(4 \\times 4 \\times 3 = 48\\), æ€»å…±æœ‰ \\(\\frac{H}{4} \\times \\frac{W}{4}\\) ä¸ªtokenã€‚\n\n\n1.2 Linear Embedding\nåœ¨å°†å›¾åƒåˆ†å‰²æˆpatchesåï¼Œæˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªpatchè½¬æ¢ä¸ºä¸€ä¸ªå‘é‡è¡¨ç¤ºã€‚è¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºçº¿æ€§åµŒå…¥ï¼ˆLinear Embeddingï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªpatchå±•å¹³ä¸ºä¸€ä¸ªå‘é‡ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼‰å°†å…¶æ˜ å°„åˆ°ä¸€ä¸ªæ›´é«˜ç»´çš„ç©ºé—´ä¸­ã€‚è¿™ä¸ªå°±æ˜¯åœ¨ Stage 1 FigureÂ 1 ä¸­çš„ çº¢è‰² éƒ¨åˆ†ã€‚\n\n\n1.3 Patch Merging\nåœ¨æ¯ä¸ªé˜¶æ®µï¼ŒSwin Transformer é€šè¿‡åˆå¹¶ç›¸é‚»çš„patchæ¥é€æ­¥å‡å°‘tokenæ•°é‡ï¼Œæ„å»ºå±‚æ¬¡åŒ–çš„ç‰¹å¾è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å°†ç›¸é‚»çš„patchè¿›è¡Œæ‹¼æ¥å’Œçº¿æ€§å˜æ¢ï¼Œå‡å°‘ç‰¹å¾å›¾çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶å¢åŠ é€šé“æ•°ã€‚è¿™ä¸ªè¿‡ç¨‹ç§°ä¸º Patch Mergingã€‚å‡è®¾åœ¨ Stage 1 ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå¤§å°ä¸º \\(H \\times W \\times C\\) çš„ç‰¹å¾å›¾ï¼Œå…¶ä¸­ \\(C\\) æ˜¯é€šé“æ•°ã€‚é€šè¿‡ Patch Mergingï¼Œæˆ‘ä»¬å°†ç‰¹å¾å›¾çš„å¤§å°å‡å°‘ä¸º \\(\\frac{H}{2} \\times \\frac{W}{2} \\times 2C\\)ï¼Œå…¶ä¸­ \\(2C\\) æ˜¯åˆå¹¶åçš„é€šé“æ•°ã€‚\n\nTo produce a hierarchical representation, the number of tokens is reduced by patch merging layers as the network gets deeper. The first patch merging layer concatenates the features of each group of 2 Ã— 2 neighboring patches, and applies a linear layer on the 4C-dimensional concatenated features. This reduces the number of tokens by a multiple of 2 Ã— 2 = 4 (2Ã— downsampling of resolution), and the output dimension is set to 2C.\n\næ€»çš„æ¥è¯´ï¼ŒPatch Merging å°±æ˜¯ Patch Partition + Linear Embedding çš„ç»„åˆã€‚å®ƒé€šè¿‡å°†ç›¸é‚»çš„patchè¿›è¡Œæ‹¼æ¥å’Œçº¿æ€§å˜æ¢ï¼Œå‡å°‘ç‰¹å¾å›¾çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶å¢åŠ é€šé“æ•°ï¼Œä»è€Œæ„å»ºå±‚æ¬¡åŒ–çš„ç‰¹å¾è¡¨ç¤ºã€‚\n\n\n1.4 Window-based Multi-head Self Attention (W-MSA)\næ¥ä¸‹æ¥ï¼Œå°±åˆ°äº†Swin-Transformerçš„æ ¸å¿ƒéƒ¨åˆ†ï¼šçª—å£è‡ªæ³¨æ„åŠ›ï¼ˆWindow-based Multi-head Self Attention, W-MSAï¼‰ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å°†ç‰¹å¾å›¾åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„éé‡å çª—å£ï¼Œå¹¶åœ¨æ¯ä¸ªçª—å£å†…è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤§å°ä¸º \\(H \\times W \\times C\\) çš„ç‰¹å¾å›¾ï¼Œæˆ‘ä»¬å°†å…¶åˆ’åˆ†ä¸ºå¤§å°ä¸º \\(M \\times M\\) çš„çª—å£ï¼Œå…¶ä¸­ \\(M\\) æ˜¯çª—å£å¤§å°ã€‚ç„¶åï¼Œåœ¨æ¯ä¸ªçª—å£å†…ï¼Œæˆ‘ä»¬è®¡ç®—è‡ªæ³¨æ„åŠ›ã€‚\n\n\n\n\n\n\nFigureÂ 2: Window-based Multi-head Self Attention\n\n\n\nå¦‚å›¾ FigureÂ 2 æ‰€ç¤ºï¼ŒW-MSA çš„Attentionè®¡ç®—æ–¹å¼ä¸ViTç±»ä¼¼ï¼Œä½†å®ƒåªåœ¨æ¯ä¸ªçª—å£å†…è¿›è¡Œè®¡ç®—ï¼Œè€Œä¸æ˜¯åœ¨æ•´ä¸ªå›¾åƒä¸Šè¿›è¡Œå…¨å±€è®¡ç®—ã€‚è¿™ç§æ–¹æ³•å¤§å¤§å‡å°‘äº†è®¡ç®—å¤æ‚åº¦ï¼Œä» \\(O(( \\frac{HW}{P^2})^2)\\) é™åˆ° \\(O(M^2 \\cdot \\frac{HW}{M^2}) = O(HW \\cdot M^2)\\)ï¼Œå…¶ä¸­ \\(M\\) æ˜¯çª—å£å¤§å°ã€‚\næˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸åŒçª—å£ä¹‹é—´çš„Attentionæ˜¯ç‹¬ç«‹çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å¹¶è¡Œè®¡ç®—æ¯ä¸ªçª—å£çš„Attentionï¼Œè¿™æ ·å¯ä»¥å¤§å¤§æé«˜è®¡ç®—æ•ˆç‡ã€‚\n\n\n1.5 Shifted Window Multi-head Self Attention (SW-MSA)\nW-MSAè™½ç„¶åœ¨æ¯ä¸ªçª—å£å†…è¿›è¡Œäº†è‡ªæ³¨æ„åŠ›è®¡ç®—ï¼Œä½†å®ƒä»ç„¶å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šç›¸é‚»çª—å£ä¹‹é—´çš„ä¿¡æ¯æ— æ³•ç›´æ¥ä¼ é€’ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSwin Transformer å¼•å…¥äº†ç§»åŠ¨çª—å£æœºåˆ¶ï¼ˆShifted Window Multi-head Self Attention, SW-MSAï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯ä¸ªé˜¶æ®µï¼ŒSwin Transformer ä¼šå°†çª—å£çš„ä½ç½®ç§»åŠ¨ä¸€ä¸ªå›ºå®šçš„æ­¥å¹…ï¼Œç„¶åå†è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ã€‚è¿™æ ·ï¼Œç›¸é‚»çª—å£ä¹‹é—´å°±å¯ä»¥é€šè¿‡é‡å åŒºåŸŸè¿›è¡Œä¿¡æ¯ä¼ é€’ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯æ•æ‰èƒ½åŠ›ã€‚\n\n\n\n\n\n\nFigureÂ 3: Shifted Window Multi-head Self Attention\n\n\n\nä½†æ˜¯è¦å¦‚ä½•å®ç°è¿™ä¸ªShifted Attentionå‘¢ï¼Ÿ å…¶ä¸­ä¸€ç§æ–¹æ³•æ˜¯ï¼Œå°†ç‰¹å¾å›¾è¿›è¡Œpaddingï¼Œç„¶åå°†çª—å£åˆ’åˆ†ä¸ºå¤§å°ä¸º \\(M \\times M\\) çš„éé‡å çª—å£ã€‚ç„¶åï¼Œåœ¨æ¯ä¸ªçª—å£å†…è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ã€‚\n\nA naive solution is to pad the smaller windows to a size of M Ã— M and mask out the padded values when computing attention.\n\n\n\n\n\n\n\n\n\n\nIllustration of Padding\n\n\n\n\n\n\n\nIllustration of Padding\n\n\n\n\n\n\nFigureÂ 4\n\n\n\n\nè¿™ç§æ–¹æ³•è™½ç„¶å¯ä»¥å®ç°ç§»åŠ¨çª—å£æœºåˆ¶ï¼Œä½†ä¼šå¢åŠ è®¡ç®—å¤æ‚åº¦ã€‚Swin Transformer é‡‡ç”¨äº†ä¸€ç§æ›´é«˜æ•ˆçš„æ–¹å¼ï¼Œé€šè¿‡ cyclic shift æ¥å®ç°çª—å£çš„ç§»åŠ¨ã€‚è¿™ç§å¯ä»¥åœ¨ä¸å¢åŠ çª—å£æ•°é‡çš„æƒ…å†µä¸‹ï¼Œå®ç°çª—å£çš„ç§»åŠ¨ï¼Œä»è€Œå‡å°‘è®¡ç®—å¤æ‚åº¦ã€‚ä¸è¿‡éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ–¹å¼ä¼šå°†ä¸€äº›çª—å£çš„æ¶ˆæ¯æ··æ·†ï¼Œå› æ­¤éœ€è¦åœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›æ—¶è¿›è¡Œmaskæ“ä½œï¼Œä»¥ç¡®ä¿æ¯ä¸ªçª—å£åªå…³æ³¨è‡ªå·±çš„åŒºåŸŸã€‚å…·ä½“çš„å®ç°æ–¹å¼å¯ä»¥å‚è€ƒæ¥ä¸‹æ¥çš„ PyTorch å®ç°éƒ¨åˆ†ã€‚\n\n\n1.6 Position Embedding\nTransformer æ¨¡å‹é€šå¸¸éœ€è¦ä½ç½®ç¼–ç ï¼ˆPosition Embeddingï¼‰æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­å…ƒç´ çš„ä½ç½®ä¿¡æ¯ã€‚Transformeræ˜¯ç”¨æ¥ sin cos å‡½æ•°æ¥ç”Ÿæˆä½ç½®ç¼–ç çš„ã€‚ä½¿ç”¨ learned position embedding çš„æ–¹å¼æ¥ç”Ÿæˆä½ç½®ç¼–ç ã€‚Swin Transformerä½¿ç”¨äº†ç¬¬ä¸‰ç§æ–¹å¼ï¼Œ relative position embeddingã€‚å®ƒé€šè¿‡è®¡ç®—ç›¸å¯¹ä½ç½®æ¥ç”Ÿæˆä½ç½®ç¼–ç ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°æ•æ‰è¾“å…¥åºåˆ—ä¸­å…ƒç´ ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ã€‚å…·ä½“çš„\n\\[\n\\text{Attention}(Q, K, V) = \\mathrm{SoftMax}\\left(\\frac{QK^\\top}{\\sqrt{d}} + B\\right)V\n\\]\nå…¶ä¸­Bæ˜¯ç›¸å¯¹ä½ç½®ç¼–ç çŸ©é˜µã€‚è¿™ä¸ªçŸ©é˜µçš„å¤§å°ä¸º \\(M^2 \\times M^2\\)ï¼Œå…¶ä¸­ \\(M\\) æ˜¯çª—å£å¤§å°ã€‚å¯¹äºæ¯ä¸ªçª—å£çš„å…ƒç´ ï¼Œä»–ä»¬çš„ç›¸å¯¹ä½ç½®æ˜¯ \\([âˆ’M + 1, M âˆ’ 1]\\)ã€‚ å› æ­¤æˆ‘ä»¬å¯ä»¥å°†ä¸€ä¸ªè¾ƒå°å°ºå¯¸çš„åç½®çŸ©é˜µ \\(\\hat{B} \\in \\mathbb{R}^{(2M - 1) \\times (2M - 1)}\\) å‚æ•°åŒ–ï¼Œå¹¶ä¸”çŸ©é˜µ \\(B\\) ä¸­çš„å€¼æ˜¯ä» \\(\\hat{B}\\) ä¸­å–å‡ºçš„ã€‚\n\n\n1.7 Others\nSwin Transformer è¿˜å¼•å…¥äº†ä¸€äº›å…¶ä»–çš„æŠ€æœ¯æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä¾‹å¦‚ï¼š\n\nLayer Normalization (Pre-Norm): åœ¨æ¯ä¸ªé˜¶æ®µçš„è¾“å…¥å’Œè¾“å‡ºä¹‹é—´ä½¿ç”¨ Layer Normalization æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚\nDropout: åœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—å’Œå…¨è¿æ¥å±‚ä¸­ä½¿ç”¨ Dropout æ¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚\nResidual Connection: åœ¨æ¯ä¸ªé˜¶æ®µçš„è¾“å…¥å’Œè¾“å‡ºä¹‹é—´ä½¿ç”¨æ®‹å·®è¿æ¥æ¥åŠ é€Ÿæ”¶æ•›ã€‚\nMLP: åœ¨æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºåä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ¥è¿›ä¸€æ­¥å¤„ç†ç‰¹å¾ã€‚\nActivation Function: ä½¿ç”¨ GELU æ¿€æ´»å‡½æ•°æ¥æé«˜æ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚\n\nåœ¨è¿™é‡Œå°±ä¸è¿‡å¤šèµ˜è¿°äº†ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å‚è€ƒåŸè®ºæ–‡ (Liu et al. 2021)ï¼Œ æˆ–è€…æˆ‘çš„å‰ä¸¤ç¯‡æ–‡ç« ï¼š\n\n01 : Transformer\n02 : Vision Transformer",
    "crumbs": [
      "03 Swin Transformer",
      "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/03-swin-transformer.html#pytorch-implementation",
    "href": "posts/03-swin-transformer/03-swin-transformer.html#pytorch-implementation",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "2 PyTorch Implementation",
    "text": "2 PyTorch Implementation",
    "crumbs": [
      "03 Swin Transformer",
      "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/03-swin-transformer.html#key-concepts",
    "href": "posts/03-swin-transformer/03-swin-transformer.html#key-concepts",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "3 Key Concepts",
    "text": "3 Key Concepts\n\n\n\n\n\n\n\n\nConcept\nExplain\n\n\n\n\nSwin Transformer\nä¸€ç§å±‚æ¬¡åŒ–çš„è§†è§‰ Transformerï¼Œé€šè¿‡å±€éƒ¨çª—å£æ³¨æ„åŠ›ä¸çª—å£å¹³ç§»æœºåˆ¶ï¼Œå®ç°çº¿æ€§è®¡ç®—å¤æ‚åº¦ï¼Œé€‚ç”¨äºå›¾åƒåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰ä»»åŠ¡ã€‚\n\n\nWindow-based Attentionï¼ˆçª—å£æ³¨æ„åŠ›ï¼‰\nåªåœ¨å›ºå®šå¤§å°çš„éé‡å å±€éƒ¨çª—å£å†…è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—é‡ã€‚\n\n\nShifted Windowï¼ˆçª—å£å¹³ç§»ï¼‰\nå°†çª—å£åˆ’åˆ†å‘å³ä¸‹å¹³ç§»ä¸€åŠçª—å£å¤§å°ï¼Œä½¿ä¸åŒçª—å£ä¹‹é—´çš„ä¿¡æ¯å¾—ä»¥äº¤äº’ï¼›éœ€è¦ä½¿ç”¨ attention mask æ¥å¤„ç†è¾¹ç•Œé—®é¢˜ã€‚\n\n\nPatch Mergingï¼ˆè¡¥ä¸åˆå¹¶ï¼‰\nå°†ç›¸é‚»çš„ patch åˆå¹¶ä»¥é™ä½åˆ†è¾¨ç‡ã€å¢åŠ é€šé“æ•°ï¼Œæ„å»ºé‡‘å­—å¡”å¼å±‚çº§ç»“æ„ã€‚\n\n\nRelative Position Biasï¼ˆç›¸å¯¹ä½ç½®åç½®ï¼‰\nå¯å­¦ä¹ çš„åç½®é¡¹ï¼ŒåŸºäº patch é—´ç›¸å¯¹ä½ç½®æ·»åŠ åˆ° attention æƒé‡ä¸­ï¼Œå¢å¼ºç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚\n\n\nHierarchical Representationï¼ˆå±‚æ¬¡åŒ–è¡¨å¾ï¼‰\næ¨¡ä»¿ CNN ç»“æ„ï¼Œé€šè¿‡å¤šå±‚ patch merging æ„å»ºä»ç»†èŠ‚åˆ°å…¨å±€çš„å¤šå°ºåº¦ç‰¹å¾ã€‚\n\n\nLinear Complexityï¼ˆçº¿æ€§å¤æ‚åº¦ï¼‰\nç”±äºæ³¨æ„åŠ›å±€é™äºå±€éƒ¨çª—å£ï¼Œæ€»ä½“è®¡ç®—å¤æ‚åº¦éšè¾“å…¥å›¾åƒå¤§å°å‘ˆçº¿æ€§å¢é•¿ã€‚\n\n\nDense Predictionï¼ˆå¯†é›†é¢„æµ‹ï¼‰\nå¦‚è¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ï¼Œæ¨¡å‹éœ€å¯¹å›¾åƒä¸­æ¯ä¸ªåƒç´ æˆ–åŒºåŸŸè¾“å‡ºç»“æœã€‚\n\n\nAttention Maskï¼ˆæ³¨æ„åŠ›æ©ç ï¼‰\nåœ¨å¹³ç§»çª—å£åç”¨äºé™åˆ¶ä¸åˆç† token é—´æ³¨æ„åŠ›è®¡ç®—çš„æ©ç çŸ©é˜µã€‚",
    "crumbs": [
      "03 Swin Transformer",
      "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "5.1 Swin V2",
    "text": "5.1 Swin V2",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#è®­ç»ƒæŠ€å·§",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#è®­ç»ƒæŠ€å·§",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "5.2 è®­ç»ƒæŠ€å·§",
    "text": "5.2 è®­ç»ƒæŠ€å·§\n\nDropout Path\nGradient Checkpoint:\n\néœ€è¦å¯é‡ç°å‰å‘ â€¢ è¢« checkpoint çš„æ¨¡å—å¿…é¡»æ˜¯ çº¯å‡½æ•°ï¼Œå³è¾“å‡ºåªä¾èµ–è¾“å…¥ï¼Œä¸èƒ½ä¾èµ–éšæœºæ•°ã€å…¨å±€çŠ¶æ€ã€‚\n\n\nCite to CLIP",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html",
    "href": "posts/02-vision-transformer/Vision-Transformer.html",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "",
    "text": "åœ¨äº†è§£äº†ä»€ä¹ˆæ˜¯Transformerä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å°†Transformeråº”ç”¨äºComputer Visionã€‚Vision Transformerï¼ˆViTï¼‰(Dosovitskiy et al. 2021) æ˜¯ä¸€ä¸ªå°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒåˆ†ç±»çš„æ¨¡å‹ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å›¾åƒåˆ’åˆ†ä¸ºå°å—ï¼ˆpatchesï¼‰ï¼Œç„¶åå°†è¿™äº›å°å—è§†ä¸ºåºåˆ—æ•°æ®ï¼Œç±»ä¼¼äºå¤„ç†æ–‡æœ¬æ•°æ®ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.1 Patch Embedding",
    "text": "1.1 Patch Embedding\n\nThe standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches\\(x \\in \\mathbb{R}^{N \\times (P^{2} \\times C)}\\), where (\\(H, W\\)) is the resolution of the original image, \\(C\\) is the number of channels, (\\(P, P\\)) is the resolution of each image patch, and \\(N = HW/P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size \\(D\\) through all of its layers, so we flatten the patches and map to \\(D\\) dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nåœ¨Transformer è¿™ä¸€ç¯‡ï¼Œæˆ‘ä»¬äº†è§£åˆ°ï¼Œå®ƒæ˜¯ä½œç”¨äºSequence Modelingçš„ï¼Œå¾ˆæ˜¾ç„¶ï¼ŒImage ä¸æ˜¯ Sequenceçš„ã€‚å¾ˆç›´è§‚çš„ç¬¬ä¸€ç§æƒ³æ³•å°±æ˜¯ï¼Œå°†å›¾ç‰‡ç›´æ¥å±•å¼€ï¼Œä»äºŒç»´ (\\(3, H, W\\)) å±•å¼€æˆä¸€ç»´çš„ (\\(3, H \\times W\\)). è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°çš„å›¾ç‰‡çš„Sequence Modelã€‚å¦‚ä¸‹å›¾@fig-flat-imageæ‰€ç¤º\n\n\n\n\n\n\nFigureÂ 2\n\n\n\nè¿™ç§æ–¹æ³•æœ‰ä¸€ç§æ˜æ˜¾çš„é—®é¢˜å°±æ˜¯ï¼šSequenceçš„é•¿åº¦å¤ªé•¿ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äº \\(3\\times 256 \\times 256\\) çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬æœ‰ \\(256 \\times 256 = 65,336\\) ä¸ªtokensï¼Œé€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ‰€éœ€è¦çš„è®­ç»ƒæ—¶é•¿å¾ˆé•¿ã€‚å¹¶ä¸”å®ƒæ²¡æœ‰ç”¨åˆ°å›¾ç‰‡çš„ä¸€ä¸ªç‰¹æ€§ï¼šç›¸é‚»çš„pixel ä¹‹é—´ï¼Œæ˜¯æœ‰å¾ˆé«˜çš„correlationçš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¾ˆè‡ªç„¶çš„æƒ³åˆ°ï¼šå¦‚æœæŠŠç›¸é‚»çš„pixelså’Œåœ¨ä¸€ç»„ï¼Œç»„æˆä¸€ä¸ªpatchï¼Œè¿™æ ·ä¸å°±æ—¢å‡å°‘äº†tokensçš„æ•°é‡ï¼Œåˆç”¨åˆ°äº†pixelä¹‹é—´çš„correlationã€‚è¿™å°±æ˜¯Vision Transformer çš„Patch Embeddingã€‚ è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†ã€‚ æ¥ä¸‹æ¥æˆ‘ä»¬åªéœ€è¦ç”¨ï¼Œä¸€ä¸ªMLPï¼Œå°†æˆ‘ä»¬å±•å¼€çš„patchï¼Œæ˜ å°„åˆ° \\(D\\)- dimensionçš„ç©ºé—´ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä¼ å…¥Transformer æ¨¡å‹äº†ã€‚\næ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç æ€ä¹ˆå®ç°ï¼š\n# Load Image and resize it to certain size\nimage_path = IMAGE_PATN\nimg_bgr = cv2.imread(image_path)\nimg_resized = cv2.resize(img_bgr, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\nimg = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB) \n\n# Patchify \npatches = einops.rearrange(\n     img, \"(h ph) (w pw) c -&gt; (h w) ph pw c\", ph=PATCH_SIZE, pw=PATCH_SIZE\n)\né€šè¿‡è¿™ä¸ªPatchifyåªæœ‰ï¼Œæˆ‘ä»¬å¾—åˆ°å°†å›¾ç‰‡Patchåˆ°äº†\n\n\nåˆ†æˆäº†ä¸åŒçš„å°Patchã€‚\næ¥ä¸‹æ¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯ï¼Œå°†è¿™äº›Patch å±•å¼€ï¼Œç„¶åä¼ å…¥ä¸€ä¸ªMLPï¼Œ\nflat_patch = einops.rearrange(\n     patches, \"n ph pw c -&gt; n (ph pw c)\"\n)\n\n\nmlp = nn.Linear(PATCH_SIZE * PATCH_SIZE * 3, d_model)\npatch_embedding = mlp(flat_patch)\nåŒè¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å°±å¯ä»¥è§å›¾ç‰‡è½¬åŒ–ä¸ºTransformerå¯ä»¥æ¥å—çš„vectorã€‚ä¸è¿‡åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸ä¼šç”¨ä»¥ä¸Šçš„æ–¹å¼ï¼Œå› ä¸ºä¸Šé¢çš„æ–¹å¼å®ç°èµ·æ¥æ¯”è¾ƒæ…¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†Patch å’Œ Linear Projectå’Œåœ¨ä¸€èµ·ã€‚\n\n\n\n\n\n\nTip\n\n\n\nå°†å‡ ä¸ªtensor çš„operationæ“ä½œåˆæˆä¸€ä¸ªçš„æ–¹æ³•ï¼Œå«åškernel fusionï¼Œè¿™æ˜¯ä¸€ç§æé«˜è®­ç»ƒå’Œæ¨ç†ç´ çš„æ–¹æ³•\n\n\nåœ¨å®é™…çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ç”¨Convolution Layer ä»£æ›¿ Patch + Flatten+ Linear çš„æ–¹æ³•. å¦‚æœæˆ‘ä»¬ç”¨ä¸€ä¸ª å·ç§¯å±‚ï¼Œå‚æ•°è®¾ç½®ä¸ºï¼š â€¢ kernel_size = PATCH_SIZE ï¼ˆå·ç§¯æ ¸è¦†ç›–ä¸€ä¸ª patchï¼‰ â€¢ stride = PATCH_SIZE ï¼ˆä¸é‡å åœ°ç§»åŠ¨ï¼Œç›¸å½“äºåˆ‡ patchï¼‰ â€¢ in_channels = 3ï¼ˆRGBï¼‰ â€¢ out_channels = d_model\né‚£ä¹ˆå·ç§¯ä¼šï¼š 1. æŠŠè¾“å…¥å›¾ç‰‡åˆ†æˆ PATCH_SIZE x PATCH_SIZE çš„ä¸é‡å å—ï¼ˆå› ä¸º stride = kernel_sizeï¼‰ã€‚ 2. å¯¹æ¯ä¸ª patch åšä¸€æ¬¡çº¿æ€§æ˜ å°„ï¼ˆå› ä¸ºå·ç§¯æœ¬è´¨ä¸Šå°±æ˜¯å¯¹å±€éƒ¨åŒºåŸŸåšåŠ æƒæ±‚å’Œï¼Œç›¸å½“äº Linearï¼‰ã€‚ 3. è¾“å‡ºçš„ shape è‡ªåŠ¨å°±æ˜¯ (batch, num_patches, d_model)ã€‚\nè¿™æ­£å¥½ç­‰ä»·äº åˆ‡ patch + flatten + Linear çš„ç»„åˆã€‚\nä»£ç å¦‚ä¸‹ï¼š\nclass PatchEmbedding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.conv = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=config.hidden_dim,\n            kernel_size=config.patch_size,\n            stride=config.patch_size,\n            padding=\"valid\" if config.patch_size == 16 else \"same\",\n        )\n\n    def forward(self, imgs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        imgs: (batch_size, num_channels, height, width)\n        Returns: (batch_size,  num_patches_height, num_patches_width, hidden_dim)\n        \"\"\"\n        # (B, C, H, W) -&gt; (B, hidden_dim, H', W')\n        x = self.conv(imgs)\n\n        # (B, hidden_dim, H', W') -&gt; (B, hidden_dim, H' * W')\n        x = x.flatten(2)\n\n        # (B, hidden_dim, H' * W') -&gt; (B, H' * W', hidden_dim)\n        x = x.transpose(1, 2)\n        return x\nç”¨å·ç§¯çš„å¥½å¤„ï¼Œé™¤äº†å¯ä»¥æ›´é«˜æ•ˆçš„å®ç°Patch Embeddingï¼Œä»£ç æ›´åŠ ç®€æ´ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡æ”¹å˜ stride æ¥ä½¿ä¸€äº›Patch overlappingï¼Œè·å¾—ä¸€ä¸ªå¤šå°ºåº¦çš„ç»“æ„ï¼Œ\nThe image is convert along this process: \\[\n\\boxed{\n\\mathbf{x} \\in \\mathbb{R}^{C \\times H \\times W}\n\\quad \\xrightarrow{\\text{Patchify}} \\quad\n\\{ x_i \\in \\mathbb{R}^{C \\times P \\times P} \\}{i=1}^N\n\\quad \\xrightarrow{\\text{Flatten}} \\quad\n\\{ x_i \\in \\mathbb{R}^{(C \\cdot P \\cdot P)} \\}_{i=1}^N\n\\quad \\xrightarrow{\\text{Linear } W \\in \\mathbb{R}^{(C \\cdot P \\cdot P) \\times D}} \\quad\n\\{ z_i \\in \\mathbb{R}^{D} \\}_{i=1}^N\n}\n\\tag{1}\\]",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.2 Position Encoding",
    "text": "1.2 Position Encoding\nå°†å›¾ç‰‡è½¬åŒ–ä¸º Transformer çš„è¾“å…¥ä¹‹åï¼Œæ¥ä¸‹æ¥Transformerä¸­çš„å¦ä¸€ä¸ªç»„ä»¶å°±æ˜¯ä¼ å…¥ Position Informationã€‚æˆ‘ä»¬çŸ¥é“åœ¨Transformer ä¸­ï¼Œä»–ä»¬ç”¨çš„æ˜¯ sine-cosine position embeddingï¼Œåœ¨é‚£ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæåˆ°äº†ï¼Œè¿˜å­˜åœ¨å…¶ä»–ä¸åŒçš„Position Encodingçš„åŠæ³•ï¼ŒViT ç”¨çš„å°±æ˜¯å¦ä¸€ç§åŠæ³•ï¼ŒLearned Position Embeddingã€‚Learned Position Embeddingçš„æ–¹æ³•å¾ˆç®€å•ï¼Œä¹Ÿå¾ˆå¥½ç†è§£ï¼Œå¯¹äºæ¯ä¸€ä¸ªä½ç½®ï¼Œæˆ‘ä»¬ç»™ä»–ä¸€ä¸ªindexï¼Œå°†è¿™ä¸ªindexä¼ å…¥ä¸€ä¸ª Embedding Matrixï¼Œ æˆ‘ä»¬å°±å¾—åˆ°ä¸€ä¸ªPosition Embeddingã€‚ä¸è¿‡ä¸Token Embeddingä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬ä¼šç”¨åˆ°æ‰€æœ‰çš„Positionï¼Œä¹Ÿæ•´ä¸ªmatrixï¼Œ æ‰€ä»¥æˆ‘ä»¬ä¸ç”¨å®šindexï¼Œç›´æ¥å®šä¹‰æ•´ä¸ªEmbeddingï¼Œç„¶åå°†å®ƒä¼ å…¥Transformerä¸­ã€‚\nclass PositionalEncoding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.positional_embedding = nn.Parameter(\n            torch.randn(\n                1,\n                (config.image_size // config.patch_size) ** 2 + 1,\n                config.hidden_dim,\n            )\n        )\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Add positional encoding to the input tensor\n        batch_size = x.size(0)\n\n        pos_embedding = self.positional_embedding.expand(batch_size, -1, -1)\n        cls_token = self.cls_token.expand(batch_size, -1, -1)\n\n        x = torch.cat((cls_token, x), dim=1)\n        return x + pos_embedding\nä¸ºä»€ä¹ˆViTè¦ç”¨Learned Position Embeddingå‘¢ï¼Ÿåœ¨ViTè¿™ç¯‡æ–‡ç« ä¸­ï¼Œä»–ä»¬å°è¯•è¿‡ä¸åŒçš„Position Embeddingï¼Œæ¯”å¦‚ï¼š\n\nNo Positional Information\n1-dimensional Positional Embedding\n2-dimensional Positional Embedding\nRelative Positional Embedding\n\nå‘ç°ï¼Œé™¤äº†No Positional Informationä¹‹å¤–ï¼Œå…¶ä½™3ç§åœ¨Image Classificationä¸­çš„è¡¨ç°ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚\n\n\n\n\n\n\nFigureÂ 3\n\n\n\nè®ºæ–‡ä¸­è¡¨ç¤ºï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‰€éœ€è¦çš„ Positionçš„ä¿¡æ¯è¾ƒå°ï¼Œå¯¹äºä¸åŒç§ç±»çš„Position Embeddingçš„æ–¹æ³•ï¼Œå­¦ä¹ è¿™ä¸ªPosition Informationçš„èƒ½åŠ›ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚\n\nWe speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., \\(14 \\times 14\\) as opposed to \\(224 \\times 224\\), and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.18 \n\nä¸è¿‡ï¼Œå°½ç®¡Positionçš„æ–¹æ³•ä¸é‡è¦ï¼Œä½†æ˜¯ä¸åŒçš„è®­ç»ƒå‚æ•°ï¼Œè¿˜æ˜¯ä¼šå½±å“åˆ°å­¦ä¹ åˆ°çš„Position Information, ä¸‹å›¾æ‰€ç¤ºï¼š\n\n\n\n\n\n\nFigureÂ 4\n\n\n\n\n1.2.1 Extending Position Encoding\nå½“æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªPre-Trainingçš„æ¨¡å‹ï¼Œæˆ‘ä»¬æƒ³ç”¨å®ƒFine-Tuningåˆ°ä¸€ä¸ªä¸åŒå›¾ç‰‡å¤§å°çš„æ•°æ®åº“ï¼Œæˆ‘ä»¬æ”¹æ€ä¹ˆåšå‘¢ï¼Œç¬¬ä¸€ä¸ªæ–¹æ³•å½“ç„¶æ˜¯ï¼ŒResize æˆ‘ä»¬çš„å›¾ç‰‡ï¼Œåˆ°ViT Pre-trainingçš„å›¾ç‰‡å¤§å°ï¼Œä½†æ˜¯ï¼Œè¿™ä¸ªèƒ½å¯¼è‡´è¾ƒå¤§çš„å›¾ç‰‡ï¼Œå¤±å»å¾ˆå¤šç»†èŠ‚ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿æŒå›¾ç‰‡çš„å¤§å°ä¸å˜ï¼ŒåŒæ—¶è®©æ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬å°±éœ€è¦Extend Position Encodingï¼Œå› ä¸ºå½“Patch Sizeä¸å˜ï¼Œå›¾ç‰‡å¤§å°å˜äº†çš„è¯ï¼Œäº§ç”Ÿçš„Number of Patches ä¹Ÿæ˜¯ä¼šæ”¹å˜çš„ï¼Œè¿™æ ·ï¼Œå°±æ˜¯æŸå¤±ä¸€äº›ä¿¡æ¯ã€‚æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼Œæ‰¾åˆ°ä¸€ç§æ–¹æ³•ï¼Œå¢å¤§æˆ–è€…å‡å°Positionçš„æ•°é‡ã€‚ è¿™å°±æ˜¯æ‰€è°“çš„Position Interpolationã€‚\n2D interpolation of the pre-trained position embeddings â€¢ ViT åœ¨é¢„è®­ç»ƒæ—¶ï¼Œé€šå¸¸ç”¨å›ºå®šè¾“å…¥åˆ†è¾¨ç‡ï¼ˆæ¯”å¦‚ 224Ã—224ï¼‰ â†’ ç”Ÿæˆå›ºå®šæ•°é‡çš„ patchï¼ˆæ¯”å¦‚ 16Ã—16 patch â†’ 196 ä¸ª patchï¼‰ã€‚ â€¢ ä½†åœ¨ fine-tuning æ—¶ï¼Œè¾“å…¥å›¾ç‰‡å¯èƒ½å¤§å°ä¸ä¸€æ ·ï¼Œæ¯”å¦‚ 384Ã—384ï¼Œè¿™æ—¶ patch æ•°é‡å°±å˜äº†ã€‚ â€¢ è¿™ä¼šå¯¼è‡´åŸæœ¬çš„ ä½ç½®ç¼–ç  (position embeddings) å’Œæ–°çš„ patch æ•°é‡å¯¹ä¸ä¸Šã€‚ â€¢ è§£å†³åŠæ³•ï¼šå¯¹é¢„è®­ç»ƒå¥½çš„ä½ç½®ç¼–ç åš äºŒç»´æ’å€¼ (2D interpolation)ï¼Œæ ¹æ® patch åœ¨åŸå›¾ä¸­çš„ç©ºé—´ä½ç½®ï¼ŒæŠŠä½ç½®ç¼–ç æ‹‰ä¼¸/ç¼©æ”¾åˆ°æ–°çš„åˆ†è¾¨ç‡ã€‚\n\nThe Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.4 \n\ndef interpolate_pos_encoding(self, x, w, h):\n    npatch = x.shape[1] - 1\n    N = self.pos_embed.shape[1] - 1\n    if npatch == N and w == h:\n        return self.pos_embed\n    class_pos_embed = self.pos_embed[:, 0]\n    patch_pos_embed = self.pos_embed[:, 1:]\n    dim = x.shape[-1]\n    w0 = w // self.patch_embed.patch_size\n    h0 = h // self.patch_embed.patch_size\n    \n    patch_pos_embed = F.interpolate(\n        patch_pos_embed.reshape(\n            1, \n            int(math.sqrt(N)), \n            int(math.sqrt(N)), \n            dim\n        ).permute(0, 3, 1, 2),\n        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n        mode='bicubic',\n    )\n    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.3 [CLS] Tokens & MLP Head",
    "text": "1.3 [CLS] Tokens & MLP Head\nåœ¨ Transformer è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬äº†è§£åˆ°ï¼šæ¯è¾“å…¥ä¸€ä¸ªtokenï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„tokenã€‚è¿™å°±æ˜¯è¯´ï¼Œå¯¹äºæ¯ä¸ªpatchï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„Tokensï¼Œé‚£ä¹ˆï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©å“ªä¸€ä¸ªtokenä½œä¸ºæˆ‘ä»¬å›¾ç‰‡çš„è¡¨ç¤ºå‘¢ã€‚ BERT (Devlin et al. 2019)ï¼Œ ç”¨äº†ä¸€ä¸ª [CLS], æ¥è¡¨ç¤ºä¸€ä¸ªå¥å­ã€‚åŒç†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ·»åŠ ä¸€ä¸ª [CLS] token, æ¥è¡¨ç¤ºä¸€å¼ å›¾ç‰‡ã€‚åŒæ—¶ï¼Œå¯¹äº [CLS] token, æˆ‘ä»¬ä¹Ÿè¦åœ¨ç»™ä»–ä¸€ä¸ªè¡¨ç¤ºä½ç½®çš„ä¿¡æ¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨Position Encodingä¸Šï¼Œæˆ‘ä»¬æœ‰ (config.image_size // config.patch_size) ** 2 + 1, ä½ç½®ä¿¡æ¯ï¼Œå…¶ä¸­ +1 å°±æ˜¯ [CLS] çš„ä½ç½®ä¿¡æ¯ã€‚ æ€»ç»“ä¸€ä¸‹ [CLS] token çš„ä½œç”¨å°±æ˜¯ç”¨æ¥èšåˆæ‰€æœ‰çš„Patchçš„æ¶ˆæ¯ï¼Œç„¶åç”¨æ¥Image çš„Representationã€‚\næˆ‘ä»¬æƒ³ä¸€ä¸‹ï¼Œé™¤äº†åŠ ä¸€ä¸ª [CLS] tokenï¼Œä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰å…¶ä»–åŠæ³•æ¥è¡¨ç¤ºå›¾ç‰‡å—ã€‚æœ‰ä¸€ç§å¾ˆè‡ªç„¶çš„æ–¹æ³•å°±æ˜¯ï¼Œå°†æ‰€æœ‰çš„patchçš„æ¶ˆæ¯æ”¶é›†èµ·æ¥ï¼Œç„¶åå»ä¸€ä¸ªå¹³å‡å€¼æ¥è¡¨ç¤ºè¿™ä¸ªå›¾ç‰‡ã€‚ç±»ä¼¼äºä¼ ç»Ÿçš„ConvNet(e.g.Â ResNet) æˆ‘ä»¬å¯ä»¥é€šè¿‡ AvgPooling æ¥å®ç°ã€‚ ä¸è¿‡è®ºæ–‡ä¸­æåˆ°ï¼Œ å¯¹äºä¸¤ç§ä¸åŒçš„Image Representationï¼Œéœ€è¦æœ‰ä¸åŒçš„Learning Rate æ¥è®­ç»ƒè¿™ä¸ªç½‘ç»œã€‚\nOther content \næœ‰äº†Image Representä¹‹åï¼Œæˆ‘ä»¬åªéœ€è¦å°†è¿™ä¸ªä¼ å…¥ä¸€ä¸ªç®€å•çš„MLPï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªClassifierã€‚MLPçš„è¾“å…¥æ˜¯hidden dimï¼Œè¾“å…¥åˆ™æ˜¯æˆ‘ä»¬Number of Classesã€‚ä¸åŒçš„Index è¡¨ç¤ºä¸åŒçš„Classsesã€‚\n\nAn initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classifierâ€”just like ResNetâ€™s final feature mapâ€”performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate,  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.17 \n\n\nBoth during pre-training and fine-tuning, a classification head is attached to \\(\\mathrm{z}_{L}^{0}\\). The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nclass ClassifierHead(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n        self.fc2 = nn.Linear(config.mlp_dim, config.num_classes)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_classes)\n        \"\"\"\n        # Use the CLS token for classification\n        cls_token = x[:, 0, :]\n        x = F.relu(self.fc1(cls_token))\n        \n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.4 Transformer Encoder Block",
    "text": "1.4 Transformer Encoder Block\nè‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»è®²å®Œäº†ViTä¸Transformerçš„ä¸»è¦ä¸åŒä¹‹å¤„ã€‚æ¥ä¸‹æ¥ï¼Œå°±æ˜¯Transformerçš„Encoderã€‚ \nè¿™éƒ¨åˆ†ï¼Œå’ŒTransformeråŸæœ¬çš„Encoderå¾ˆç±»ä¼¼ï¼Œåªä¸è¿‡æœ‰å‡ å¤„ä¸åŒï¼š\n\nPre-Norm: åœ¨ViTåŒï¼Œè¾“å…¥å…ˆè¿›è¡Œä¸€ä¸ªLayerNormï¼Œç„¶ååœ¨ä¼ å…¥MHAæˆ–è€…MLPä¸­ï¼Œåè§‚åœ¨TransformeråŸæœ¬çš„Encoderä¸­ï¼Œæˆ‘ä»¬æ˜¯å…ˆå°†MHAæˆ–è€…MLPçš„è¾“å‡ºä¸è¾“å…¥åŠ åœ¨ä¸€èµ·ï¼Œä¹‹åå†è¿›è¡Œä¸€ä¸ªNormalizationã€‚è¿™å«åšPost-Norm\nMLPçš„å®ç°ï¼šåœ¨Transformer Encoderä¸­ï¼Œç”¨çš„æ˜¯ ReLU, è€Œåœ¨ViTä¸­ï¼Œç”¨çš„æ˜¯ GELU\n\né™¤æ­¤ä¹‹å¤–ï¼Œå…¶ä»–éƒ¨åˆ†éƒ½æ˜¯ä¸€æ ·çš„ã€‚ä¸€ä¸‹æ˜¯ViT Encoderçš„å®ç°ï¼š\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.norm1 = LayerNorm(config.hidden_dim)\n        self.mha = MHA(config)\n        \n        self.norm2 = LayerNorm(config.hidden_dim)\n        self.ffn = FFN(config)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Multi-head attention\n        redisual = x\n        x = self.norm1(x)\n        x = redisual + self.mha(x)\n\n        # Feed-forward network\n        redisual = x\n        x = self.norm2(x)\n        x = x + self.ffn(x)\n\n        return x",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.5 CNN vs.Â ViTï¼š Inductive bias",
    "text": "1.5 CNN vs.Â ViTï¼š Inductive bias\nè‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»å®Œäº†Vision Transformerï¼Œæˆ‘ä»¬æ¥ä»Inductive Bias çš„æ–¹é¢ï¼Œçœ‹çœ‹ CNN å’Œ ViT æœ‰ä»€ä¹ˆä¸åŒ\n\n\n\n\n\n\nä»€ä¹ˆæ˜¯Inductive Bias\n\n\n\nåœ¨æ·±åº¦å­¦ä¹ é‡Œï¼ŒInductive Biasï¼ˆå½’çº³åç½®ï¼‰æ˜¯æŒ‡æ¨¡å‹åœ¨å­¦ä¹ ä¹‹å‰ï¼Œå› ç»“æ„æˆ–è®¾è®¡è€Œè‡ªå¸¦çš„å‡è®¾æˆ–å…ˆéªŒã€‚\n\n\nå¯¹äºå›¾åƒæ¥è¯´ï¼Œå¸¸è§çš„å…ˆéªŒå°±æ˜¯ï¼š\n\nå±€éƒ¨åƒç´ æ˜¯ç›¸å…³çš„ï¼ˆlocalityï¼‰\nç›¸é‚»åŒºåŸŸçš„æ¨¡å¼æœ‰è§„å¾‹ï¼ˆ2D neighborhoodï¼‰\nç‰©ä½“æ— è®ºå‡ºç°åœ¨å›¾åƒå“ªé‡Œï¼Œè¯†åˆ«æ–¹å¼åº”è¯¥ä¸€æ ·ï¼ˆtranslation equivarianceï¼‰\n\nğŸ”¹ 2. CNN çš„ç»“æ„æ€ä¹ˆä½“ç°è¿™äº›åç½®ï¼Ÿ 1. å±€éƒ¨æ€§ (Locality) â€¢ å·ç§¯æ ¸ï¼ˆä¾‹å¦‚ 3Ã—3ï¼‰åªå’Œå±€éƒ¨åƒç´ æ‰“äº¤é“ï¼Œè€Œä¸æ˜¯å…¨å›¾ã€‚ â€¢ è¿™æ„å‘³ç€æ¨¡å‹â€œç›¸ä¿¡â€å›¾åƒçš„é‡è¦ç‰¹å¾æ¥è‡ªå±€éƒ¨é‚»åŸŸï¼Œè€Œä¸æ˜¯é¥è¿œåŒºåŸŸã€‚ 2. äºŒç»´é‚»åŸŸç»“æ„ (2D structure) â€¢ å·ç§¯æ“ä½œæ˜¯æ²¿ç€ å›¾åƒçš„äºŒç»´ç½‘æ ¼è¿›è¡Œçš„ï¼Œå¤©ç„¶åˆ©ç”¨äº†å›¾åƒçš„è¡Œåˆ—ç»“æ„ã€‚ â€¢ è¿™å’Œæ–‡æœ¬ï¼ˆåºåˆ— 1Dï¼‰ä¸ä¸€æ ·ï¼ŒCNN æ˜ç¡®çŸ¥é“è¾“å…¥æ˜¯ 2D æ’åˆ—çš„ã€‚ 3. å¹³ç§»ç­‰å˜æ€§ (Translation equivariance) â€¢ å·ç§¯æ ¸çš„å‚æ•°åœ¨æ•´å¼ å›¾å…±äº«ã€‚ â€¢ æ‰€ä»¥çŒ«åœ¨å·¦ä¸Šè§’è¿˜æ˜¯å³ä¸‹è§’ï¼Œå·ç§¯æ ¸éƒ½èƒ½æ£€æµ‹åˆ°â€œçŒ«è€³æœµâ€ã€‚ â€¢ è¿™è®© CNN è‡ªåŠ¨å…·æœ‰â€œè¯†åˆ«ä½ç½®æ— å…³â€çš„èƒ½åŠ›ã€‚\nè¿™äº›æ€§è´¨ä¸æ˜¯æ¨¡å‹é€šè¿‡è®­ç»ƒå­¦å‡ºæ¥çš„ï¼Œè€Œæ˜¯å› ä¸º å·ç§¯æ“ä½œæœ¬èº«çš„æ•°å­¦ç»“æ„å°±å¸¦æ¥çš„ï¼š â€¢ kernel çš„å±€éƒ¨è¿æ¥ â†’ å±€éƒ¨æ€§ â€¢ kernel æ»‘åŠ¨è¦†ç›–å…¨å›¾ â†’ å¹³ç§»ç­‰å˜æ€§ â€¢ æ“ä½œåœ¨äºŒç»´ç©ºé—´å®šä¹‰ â†’ é‚»åŸŸç»“æ„ â€¢ æ‰€ä»¥ï¼Œå“ªæ€•ä½ ä¸ç»™ CNN å–‚å¤ªå¤šæ•°æ®ï¼Œå®ƒä¹Ÿä¼šåˆ©ç”¨è¿™äº›åç½®å»å­¦ä¹ ç‰¹å¾ã€‚\nè€Œå¯¹äº ViT æ¥è¯´ï¼š ViT çš„å½’çº³åç½®éå¸¸å¼±ï¼Œå‡ ä¹å®Œå…¨ä¾èµ–æ•°æ®å’Œè®­ç»ƒæ¥å­¦ä¹ ã€‚ 1. Patch åˆ‡åˆ† (Patchification) â€¢ ViT å”¯ä¸€çš„â€œå›¾åƒå…ˆéªŒâ€ä¹‹ä¸€å°±æ˜¯æŠŠè¾“å…¥å›¾ç‰‡åˆ‡æˆ patchã€‚ â€¢ è¿™ä¸€æ“ä½œéšå«äº†ï¼šå›¾åƒæ˜¯ä¸€ä¸ªäºŒç»´ç»“æ„ï¼Œå¯ä»¥è¢«åˆ†å—å¤„ç†ã€‚ 2. ä½ç½®ç¼–ç  (Positional Embeddings) â€¢ Transformer æœ¬èº«åªå¤„ç†åºåˆ—ï¼Œæ²¡æœ‰ç©ºé—´ç»“æ„çš„æ¦‚å¿µã€‚ â€¢ ViT é€šè¿‡åŠ ä½ç½®ç¼–ç å‘Šè¯‰æ¨¡å‹ patch åœ¨å›¾åƒä¸­çš„ç›¸å¯¹ä½ç½®ã€‚ â€¢ åœ¨è¾“å…¥åˆ†è¾¨ç‡å˜åŒ–æ—¶ï¼Œä¼šåš äºŒç»´æ’å€¼ (2D interpolation) æ¥é€‚é…ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§äººå·¥å¼•å…¥çš„ 2D å…ˆéªŒã€‚ 3. å…¶ä»–éƒ¨åˆ† â€¢ é™¤äº†ä»¥ä¸Šä¸¤ç‚¹ï¼ŒViT çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ å…¨å±€çš„ (global)ï¼Œæ²¡æœ‰å±€éƒ¨æ€§çº¦æŸã€‚ â€¢ æ²¡æœ‰åƒ CNN é‚£æ ·å†…ç½®çš„å¹³ç§»ç­‰å˜æ€§æˆ–å±€éƒ¨é‚»åŸŸç»“æ„ã€‚\nè¿™æ ·å°±æ˜¯ä¸ºä»€ä¹ˆViTéœ€è¦æ›´å¤šæ•°æ®å’Œè®¡ç®—æ‰èƒ½å­¦åˆ°åŒæ ·çš„ç©ºé—´å½’çº³è§„å¾‹ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.6 ViT Model Variants",
    "text": "1.6 ViT Model Variants\nViT æœ‰3ç§ä¸åŒçš„åŸºæœ¬å˜å½¢ï¼Œ å¦‚ä¸‹å›¾æ‰€ç¤º \nViTçš„åå­—é€šå¸¸è¡¨ç¤ºä¸º: ViT-L/16: æ„æ€æ˜¯ï¼ŒViT-Largeï¼Œç„¶åç”¨çš„16 Patch Sizeã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒPatch Sizeè¶Šå¤§ï¼Œæˆ‘ä»¬å¾—åˆ°çš„tokenså°±è¶Šå°‘ï¼Œä¹Ÿå°±æ˜¯éœ€è¦æ›´å°‘çš„è®­ç»ƒæ—¶å®ç°ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#å‡å°‘tokens",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#å‡å°‘tokens",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "3.1 å‡å°‘Tokens",
    "text": "3.1 å‡å°‘Tokens\n\nPatch Merge\nPatch Shuffle",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "3.2 Vision Language Model",
    "text": "3.2 Vision Language Model\næˆ‘ä»¬ä»¥åŠå­¦ä¹ äº†ViT for computer Visionï¼Œ Transformer for NLPï¼Œ æ¥ä¸‹æ¥æœ‰ä»€ä¹ˆåŠæ³•è®©è¿™ä¸¤ç§æ¨¡å‹ç»“åˆèµ·æ¥å‘¢ï¼Ÿ CLIP (2021): å°† ViT èåˆåˆ° vision-language é¢„è®­ç»ƒä¸­ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#axial-attentionè½´å‘æ³¨æ„åŠ›",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#axial-attentionè½´å‘æ³¨æ„åŠ›",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "4.1 Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰",
    "text": "4.1 Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰\nåœ¨å¤„ç† å›¾åƒæˆ–è§†é¢‘ è¿™ç±»é«˜ç»´è¾“å…¥æ—¶ï¼Œå¦‚æœç›´æ¥å¯¹æ‰€æœ‰åƒç´ åš å…¨å±€ self-attentionï¼Œå¤æ‚åº¦æ˜¯ \\(\\mathcal{O}(H^2 W^2)\\) ï¼ˆ\\(H, W\\) æ˜¯é«˜å’Œå®½ï¼‰ã€‚å½“å›¾åƒå¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªä»£ä»·å¤ªé«˜ã€‚ æ ¸å¿ƒæƒ³æ³•ï¼šæŠŠäºŒç»´ attention æ‹†æˆä¸¤æ¬¡ä¸€ç»´ attentionï¼ˆæ²¿ç€å›¾åƒçš„ä¸¤ä¸ªâ€œè½´â€åˆ†åˆ«åšï¼‰ã€‚ 1. Row-wise Attentionï¼ˆè¡Œæ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€æ°´å¹³æ–¹å‘ï¼ˆå®½åº¦è½´ Wï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€è¡Œçš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦ï¼š\\(\\mathcal{O}(H \\cdot W^2)\\)ã€‚ 2. Column-wise Attentionï¼ˆåˆ—æ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€å‚ç›´æ–¹å‘ï¼ˆé«˜åº¦è½´ Hï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€åˆ—çš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦ï¼š \\(\\mathcal{O}(W \\cdot H^2)\\)ã€‚\nç»„åˆèµ·æ¥ï¼Œç›¸å½“äºåœ¨ H å’Œ W ä¸¤ä¸ªè½´ä¸Šéƒ½åšäº†å…¨å±€ä¾èµ–å»ºæ¨¡ã€‚\n\n\n\n\n\n\nFigureÂ 5",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html",
    "href": "posts/01-transformer/post.html",
    "title": "Attention is All You Need(Transformer)",
    "section": "",
    "text": "Original Paper: Attention is All You Need\nMy Implementation: GitHub Repository\nTransformer Model, as introduced in the paper Attention is all you Need (Vaswani et al. 2023) has revolutionized the field of natural language processing (NLP) and beyond. This architecture is built entirely on attention mechanisms, dispensing with recurrence and convolutions entirely, which allows for greater parallelization and efficiency in training. Nowadays, it has become the backbone of many state-of-the-art models in NLP, computer vision, and other domains. For example, ChatGPT, DeepSeek, and many other large language models (LLMs) are based on the Transformer architecture.\ndoubt, the Transformer paper will be our first paper to read in this series. In this chapter, we will dive deep into this paper, starting with understanding its background and main contributions.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#sec-input-embedding",
    "href": "posts/01-transformer/post.html#sec-input-embedding",
    "title": "Attention is All You Need(Transformer)",
    "section": "1.1 Word Embedding",
    "text": "1.1 Word Embedding\nWord Embedding is the process of converting tokens into vectors. It is a way of representing words as dense vectors in a continuous vector space. Each word(token) is mapped to a unique vector, and similar words are mapped to similar vectors. This allows the model to capture the semantic meaning of words and their relationships with other words. The Word Embedding is typically learned during the training process of the model.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#sec-positional-encoding",
    "href": "posts/01-transformer/post.html#sec-positional-encoding",
    "title": "Attention is All You Need(Transformer)",
    "section": "1.2 Positional Encoding",
    "text": "1.2 Positional Encoding\n\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\n\nOne of the problems with transformer is that the model lack the information of the sequence order. To solve this problem, they add â€œpositional encodingsâ€ to the input embeddings. The positional encodings have the same dimension \\(d_\\text{model}\\) as the word embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (sinusoidal). In this transformer, they use sine and cosine functions of different frequencies:\n\\[\n\\begin{split}\nPE_{(pos,2i)} &= sin(pos / 10000^{2i/d_{model}}) \\\\\nPE_{(pos,2i+1)} &= cos(pos / 10000^{2i/d_{model}})\n\\end{split}\n\\tag{1}\\]\nwhere \\(pos\\) is the position and \\(i\\) is the dimension. They chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k\\), \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\).\nThe Transformer uses positional encoding to provide unique positional information for each word, enabling the model to capture the relative positional relationships between words in a sequence.\n\n\n\n\n\n\n\n\n\n\n\n(a) Position Encoding with max sequence length 100\n\n\n\n\n\n\n\n\n\n\n\n(b) Position Encoding with max sequence length 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Position Encoding with max sequence length from 100 to 200\n\n\n\n\n\n\n\n\n\n\n\n(d) Position Encoding with log base from 10,000 to 1,000\n\n\n\n\n\n\n\nFigureÂ 2: Illustration of Position Encoding with different max sequence lengths, the horizontal red line represent the encoding at position 50. The encoding is consistent across different max sequence lengths, which allows the model to generalize to longer sequences.\n\n\n\nFrom the FigureÂ 2 , we can see that the positional encoding changes continuously as the max sequence length increases (FigureÂ 2 (c)). And under different max sequence lengths, the positional encoding changes are the same, the FigureÂ 2 (a), FigureÂ 2 (b) show the positional encoding changes at position 50 under different max sequence lengths. We can see that the positional encoding is the same under different max sequence lengths, which allows the model to better generalize to longer sequences.\n\n\n\n\n\n\nFigureÂ 3: Details of Position Encoding\n\n\n\nIn the FigureÂ 3 , we can see the details of different dimensions of the positional encoding. Compared to dimension (4, 5), dimension (6, 7) changes more with position. From the figure, we can see that:\n\nLow \\(i\\) (the earlier dimensions) â€” short wavelength, changes quickly with position pos â†’ easy to distinguish adjacent tokens;\nHigh \\(i\\) (the later dimensions) â€” long wavelength, changes slowly with position pos â†’ captures global positional information.\n\nBesides, positional encoding can also achieve different effects by changing the base of the sine and cosine functions. For example, the base can be changed from 10000 to 1000, which shortens the wavelength of the positional encoding and makes it easier for the model to capture information from adjacent positions. Figure FigureÂ 2 (d) shows the changes in positional encoding under different bases.\nCombine with Word Embedding, we can get the final input embedding:",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#sec-multi-head-attention",
    "href": "posts/01-transformer/post.html#sec-multi-head-attention",
    "title": "Attention is All You Need(Transformer)",
    "section": "1.3 Multi-Head Attention",
    "text": "1.3 Multi-Head Attention\nMulti Head Attention is the core module of the Transformer. Its function is to perform multi-head attention calculations on the input vectors, thereby capturing different semantic information. Attention is essentially a weighted sum process, which can be seen as a weighted average of the input vectors. The core formula is:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\tag{2}\\]\nwhere \\(Q\\)ã€\\(K\\)ã€\\(V\\) are the Query, Key, and Value matrices respectively, and \\(\\sqrt{d_k}\\) is a scaling factor used to prevent the dot product values from becoming too large, which could lead to vanishing gradients. The \\(QK^T\\) operation computes the similarity between the query and key vectors, resulting in a score matrix that indicates how much attention each position should pay to every other position. The softmax function is then applied to this score matrix to obtain the attention weights, which are used to compute a weighted sum of the value vectors.\n\n\n\n\n\n\nNote\n\n\n\nEquationÂ 2 is the core formula of Attention, and Attention is the core module of Transformer. Understanding this formula is key to understanding Transformer. Many subsequent innovations, such as Linear Attention (Wang et al. 2020) and Multi-head Latent Attention(MLA)(DeepSeek-AI et al. 2024), are based on improvements to this formula.\n\n\n\nInstead of performing a single attention function with \\(d_\\text{model}\\)-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values \\(h\\) times with different, learned linear projections to \\(d_k\\), \\(d_k\\) and \\(d_v\\) dimensions, respectively.\n\nMulti-Head Attention is an extension of Attention EquationÂ 2 that splits the input vectors into multiple subspaces (heads) and computes attention independently in each subspace. Finally, the outputs of all subspaces are concatenated to obtain the final output. The formula for Multi-Head Attention is:\n\\[\n\\begin{split}\n\\text{MultiHead}(Q, K, V) &= \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\\\\n\\text{where}\\ \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\end{split}\n\\tag{3}\\]\nwhere \\(W_i^Q, W_i^K \\in \\mathbb{R}^{d_\\text{model} \\times d_k}, W_i^V \\in \\mathbb{R}^{d_\\text{model} \\times d_v}\\) are weights matrices used to project the input vectors into subspaces, and \\(W^O \\in \\mathbb{R}^{hd_v \\times d_\\text{model} }\\) is a weight matrix used to concatenate the outputs of all subspaces. The purpose of Multi-Head Attention is to capture different semantic information through multiple subspaces, thereby improving the modelâ€™s expressive powerã€‚\n\nDue to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n\nAs mentioned in the paper, the use of multiple heads allows the model to jointly attend to information from different representation subspaces at different positions. This is particularly useful for capturing complex patterns in the data.\n\n1.3.1 Time Complexity of Multi-Head Attention\nBefore we continue to explore other components, letâ€™s analyze the time complexity of Multi-Head Attention. Assuming the input length is \\(n\\) and the dimension of each head is \\(d_k\\), the time complexity for computing \\(QK^T\\) is \\(\\mathcal{O}(n^2 d_k)\\), this is the standard matrix multiplication complexity.\nNext is the computation of Softmax, which has a time complexity of \\(\\mathcal{O}(n)\\) (wiki). For each row of the score matrix \\(QK^T \\in \\mathbb{R}^{n \\times n}\\), we need to compute softmax, which requires \\(n\\) computations. Therefore, the time complexity of Softmax is \\(\\mathcal{O}(n^2)\\).\nThen comes the weighted sum for the value, which also has a computational complexity of \\(\\mathcal{O}(n^2d)\\).\nThe total time complexity of Multi-Head Attention is the sum of the time complexities of these three steps, which is \\(\\mathcal{O}(n^2d)\\): \\[\n\\begin{array}{|l|l|}\n\\hline\n\\textbf{Step} & \\textbf{Time Complexity} \\\\\n\\hline\nQK^\\top & \\mathcal{O}(n^2 d) \\\\\n\\text{softmax}(QK^\\top) & \\mathcal{O}(n^2) \\\\\n\\text{attention} \\times V & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\textbf{Total} & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\end{array}\n\\tag{4}\\]\nHere is the compare of time complexity between Multi-Head Attention, RNN and CNN: \n\n\n1.3.2 Causal Attention\nCausal Attention is a special type of attention mechanism used in the Transformer model, particularly in the decoder part. The purpose of Causal Attention is to prevent the model from seeing future information during training, thereby ensuring the modelâ€™s autoregressive property. Specifically, Causal Attention masks future information when computing attention, allowing the model to only attend to current and past information.\nThe formula for Causal Attention is as follows: \\[\n\\text{CausalAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V\n\\tag{5}\\]\nIn this formula, \\(M\\) is a mask matrix that serves to block out future information. \\(M\\) is an upper triangular matrix where the elements below the diagonal are 0, and the elements above the diagonal are \\(-\\infty\\). This way, when computing the Softmax, the elements above the diagonal are masked out, ensuring that the model can only see current and past information.\n\n\n\n\n\n\nMasking in Transformer\n\n\n\nWhen apply masking in the transformer, there is also a padding mask. The padding mask is used to mask out the padding tokens in the input sequence, which are added to make all sequences in a batch have the same length. The padding mask is a binary matrix where the elements corresponding to padding tokens are 0, and the elements corresponding to non-padding tokens are 1. When computing attention score, we need to apply padding mask and causal mask together causal_mask | padding_mask. The combined mask is obtained by taking the element-wise minimum of the padding mask and the causal mask. This way, we can ensure that the model only attends to valid tokens in the input sequence.\n\n\n\n\n\n\n\n\nFigureÂ 4: Illustration of Attention Mask with Padding mask\n\n\n\n\n\n1.3.3 Cross Attention\nCross Attention is another important attention mechanism used in the Transformer model, particularly in the decoder part. The purpose of Cross Attention is to allow the decoder to attend to the encoderâ€™s output, thereby enabling the model to generate output sequences based on the input sequences. The cross attention has same structure as self-attention EquationÂ 2, but the query comes from the previous decoder layer, and the key and value come from the output of the encoder.\n\n\n\n\n\n\nApplication of Cross Attention\n\n\n\nCross Attention can be applied in various tasks, such as Visual Question Answering (VQA), where the model needs to attend to both image features and question text. By using Cross Attention, the model can effectively integrate information from different modalities, leading to improved performance in tasks that require understanding of both visual and textual information.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#sec-layer-normalization",
    "href": "posts/01-transformer/post.html#sec-layer-normalization",
    "title": "Attention is All You Need(Transformer)",
    "section": "1.4 Layer Normalization",
    "text": "1.4 Layer Normalization\nLayer Normalization (LayerNorm) is a technique used to normalize the inputs of a neural network layer. It is similar to Batch Normalization, but instead of normalizing across the batch dimension, it normalizes across the feature dimension. This means that for each position in the sequence, LayerNorm computes the mean and variance of the features and normalizes them accordingly.\n\n\n\n\n\n\nFigureÂ 5: Illustration of Layer Normalization and Batch Normalization, where \\(P_i\\) is the position index, \\(i_j\\) is the hidden feature dimension index\n\n\n\nThe formula for Layer Normalization is: \\[\n\\text{LayerNorm}(\\mathrm{x}_i) = \\frac{\\mathrm{x}_i - \\mu_i}{\\sigma_i + \\epsilon} \\cdot \\gamma + \\beta\n\\tag{6}\\]\nwhere \\(\\epsilon\\) is a small constant added to the variance to prevent division by zero, and \\(\\gamma\\) and \\(\\beta\\) are learnable parameters that allow the model to scale and shift the normalized output.\nFor a batch of inputs, Layer Normalization normalizes the vectors at each position FigureÂ 5, rather than normalizing across the entire batch. This allows Layer Normalization to better adapt to sequences of varying lengths, thereby improving model performance. Specifically, for \\(x \\in \\mathbb{R}^{B \\times H \\times S \\times d_v}\\), we normalize along the \\(d_v\\) dimension for each position, rather than normalizing across the entire batch. So, there is no running mean and running variance in LayerNorm, which is different from BatchNorm.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#residual-connection",
    "href": "posts/01-transformer/post.html#residual-connection",
    "title": "Attention is All You Need(Transformer)",
    "section": "1.5 Residual Connection",
    "text": "1.5 Residual Connection\nResidual Connection, also known as skip connection, is a technique used to improve the training of deep neural networks. It allows the input of a layer to be added directly to the output of the layer, thereby creating a shortcut for the gradient to flow through. This helps to alleviate the problem of vanishing gradients and allows for deeper networks to be trained effectively.\n\n\n\n\n\n\nFigureÂ 6: Illustration of Residual Connection\n\n\n\nThe formula for Residual Connection is: \\[\n\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x} + \\mathrm{Sublayer}(\\mathbf{x}))\n\\tag{7}\\]\nDuring training, the residual connection provides a â€œshortcutâ€ for the gradient, allowing it to bypass the complex nonlinear transformations of the sub-layer and be directly passed back to the input of the previous layer. This effectively alleviates the problem of vanishing gradients, as shown in the following formula:\n\\[\n\\begin{split}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\left( \\mathbf{I} + \\frac{\\partial \\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\\\\n&= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}}}_{\\text{straight path}} +\n\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot\n\\frac{\\partial\\,\\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\text{through the sub-layer}}\n\\end{split}\n\\tag{8}\\]\nEquationÂ 8 shows the effect of residual connections on gradients. Here, \\(\\mathcal{L}\\) is the loss function. We can see that due to the presence of the first term, even if the gradient of the sub-layer approaches 0, the information of the gradient will not be completely lost.\n\n\n\n\n\n\nPre-Normalization vs Post-Normalization\n\n\n\nIn the original Transformer, the normalization is placed after the Residual Connection (Post-Normalization) EquationÂ 7 . However, in subsequent research, many models (such as BERT) place the normalization before the Residual Connection (Pre-Normalization). It shows that the pre-normalization can has more stable training process, and no need warm up learning rate (Xiong et al. 2020). The formula for Pre-Normalization is: \\[\n\\text{Output} = \\mathrm{Sublayer}(\\mathbf{x}) + \\mathrm{LayerNorm}(\\mathbf{x})\n\\]",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#sec-point-wise-ffn",
    "href": "posts/01-transformer/post.html#sec-point-wise-ffn",
    "title": "Attention is All You Need(Transformer)",
    "section": "1.6 Point-Wise Feed Forward Network",
    "text": "1.6 Point-Wise Feed Forward Network\n\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n\nPoint-Wise Feed Forward Network is used to further process the output of the Multi-Head Attention. It consists of two linear transformations with a ReLU activation in between. The output of the first linear transformation is passed through the ReLU activation function, and then the result is passed through the second linear transformation. The purpose of Point-Wise Feed Forward Network is to introduce non-linearity and increase the modelâ€™s capacity to capture complex patterns in the data. It has formula as follows:\n\\[\n\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n\\tag{9}\\]\nwhere \\(W_1\\) \\(\\in \\mathbb{R}^{d_\\text{model} \\times d_\\text{ff}}\\) and \\(W_2 \\in \\mathbb{R}^{d_\\text{ff} \\times d_\\text{model}}\\) are weight matrices, and \\(b_1 \\in \\mathbb{R}^{d_\\text{ff}}\\) and \\(b_2 \\in \\mathbb{R}^{d_\\text{model}}\\) are bias terms. The ReLU function is applied element-wise, which introduces non-linearity to the model.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#output-linear-projection-softmax",
    "href": "posts/01-transformer/post.html#output-linear-projection-softmax",
    "title": "Attention is All You Need(Transformer)",
    "section": "1.7 Output Linear Projection & Softmax",
    "text": "1.7 Output Linear Projection & Softmax\nIn the Transformer model, after the input sequence has been processed by the Encoder and Decoder blocks, the final step is to generate the output sequence. This is done by applying a linear transformation followed by a Softmax function to the output of the Decoder. The purpose of this step is to convert the output vectors into a probability distribution over the vocabulary, allowing the model to predict the next word in the sequence. The formula for this step is as follows: \\[\n\\text{Output} = \\text{Softmax}(xW + b)\n\\tag{10}\\]\nwhere \\(x\\) is the output of the Decoder, and \\(W\\) and \\(b\\) are the weight matrix and bias term of the linear transformation, respectively. The Softmax function is applied to the output of the linear transformation to obtain a probability distribution over the vocabulary.\nThe weight matrix \\(W\\) has a shape of \\(\\mathbb{R}^{d_\\text{model} \\times V}\\), where \\(V\\) is the size of the vocabulary. This means that for each output vector, the linear transformation produces a vector of logits with a length equal to the vocabulary size. The Softmax function then converts these logits into probabilities, which can be interpreted as the likelihood of each word in the vocabulary being the next word in the sequence. And author use weight tying(Press and Wolf 2017) to share the weights between the input embedding of decoder and the output projection, which can reduce the number of parameters and improve the modelâ€™s performance.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#full-model",
    "href": "posts/01-transformer/post.html#full-model",
    "title": "Attention is All You Need(Transformer)",
    "section": "1.8 Full Model",
    "text": "1.8 Full Model\nThe full transformer model is composed of multiple Encoder Blocks and Decoder Blocks, each containing the components we have discussed so far. The full transformer model is illustrated in the figure FigureÂ 1 .\nTransformerçš„å®Œæ•´æ¨¡å‹æ¶æ„å¦‚å›¾ FigureÂ 1 æ‰€ç¤ºã€‚Transformerç”±å¤šä¸ªEncoder Blockå’ŒDecoder Blockç»„æˆï¼Œæ¯ä¸ªEncoder Blockå’ŒDecoder Blockéƒ½åŒ…å«äº†å‰é¢ä»‹ç»çš„æ¨¡å—ã€‚Encoder Blockå’ŒDecoder Blockçš„ç»“æ„æ˜¯ç›¸ä¼¼çš„ï¼Œéƒ½æ˜¯ç”±Multi-Head Attentionã€Point-Wise Feed Forward Networkã€Layer Normalizationå’ŒResidual Connectionç»„æˆçš„ã€‚\nä¸‹å›¾æ˜¯æ•´ä¸ªTransformerçš„ç¼–ç å’Œè§£ç è¿‡ç¨‹çš„ç¤ºæ„å›¾ï¼š\n\n\n\n\n\n\n\n\n\n\n\n(a) Transformer Encoding Process\n\n\n\n\n\n\n\n\n\n\n\n(b) Transformer Decoding Process\n\n\n\n\n\n\n\nFigureÂ 7: Illustrate of Transformer Encoding and Decoding Process (Image Source: The Illustrated Transformer)\n\n\n\nAs in the FigureÂ 7 , the encoding process and decoding process of the Transformer are similar. The encoding process converts the input tokens into vectors, then encodes them through multiple Encoder Blocks, and finally converts them into a probability distribution over the vocabulary through a linear transformation and Softmax function. The decoding process, on the other hand, performs cross-attention between the output of the Encoder and the input of the Decoder, then decodes them through multiple Decoder Blocks, and finally converts them into a probability distribution over the vocabulary through a linear transformation and Softmax function.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#dataset-preparation-tokenization",
    "href": "posts/01-transformer/post.html#dataset-preparation-tokenization",
    "title": "Attention is All You Need(Transformer)",
    "section": "2.1 Dataset Preparation & Tokenization",
    "text": "2.1 Dataset Preparation & Tokenization\nThe dataset we are using is the iwslt2017-en-zh, which is a small dataset for English to Chinese translation. You can download the dataset:\nAfter downloading the dataset, we need to preprocess the data and convert the text data into tokens. We use the Hugging Face Tokenizers library to perform tokenization and encoding. We use Byte Pair Encoding (BPE) to tokenize the input text data. The code is as follows:",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#optimizer-learning-rate-scheduler",
    "href": "posts/01-transformer/post.html#optimizer-learning-rate-scheduler",
    "title": "Attention is All You Need(Transformer)",
    "section": "2.2 Optimizer & Learning Rate Scheduler",
    "text": "2.2 Optimizer & Learning Rate Scheduler\nWe use the Adam optimizer to optimize the model parameters. The learning rate scheduler is the same as the one used in the original Transformer paper, which is a warm-up learning rate scheduler: \\[\n\\text{lrate} = d_\\text{model}^{-0.5} \\cdot \\min(\\text{step\\_num}^{-0.5}, \\text{step\\_num} \\cdot \\text{warmup\\_steps}^{-1.5})\n\\tag{11}\\]",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#loss-curve",
    "href": "posts/01-transformer/post.html#loss-curve",
    "title": "Attention is All You Need(Transformer)",
    "section": "2.3 Loss Curve",
    "text": "2.3 Loss Curve\nHere is the Loss Curve after training 10,000 steps: \nCongratulations! You have successfully implemented the Transformer, which is currently the most important AI model framework. By understanding it, you can comprehend most AI models. The popular models like ChatGPT and DeepSeek are all based on variations of the Transformer (we will read about these models in the upcoming articles). The complete code can be found on GitHub.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#question-1",
    "href": "posts/01-transformer/post.html#question-1",
    "title": "Attention is All You Need(Transformer)",
    "section": "4.1 Question 1",
    "text": "4.1 Question 1\nWhy is the dot product scaled by \\(\\sqrt{d_k}\\)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIf not scaled, when \\(d_k\\) is large, the variance of QK also increases, causing the softmax to fall into regions with very small gradients. Dividing by \\(\\sqrt{d_k}\\) helps keep the activation values in a range suitable for training.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#question-2",
    "href": "posts/01-transformer/post.html#question-2",
    "title": "Attention is All You Need(Transformer)",
    "section": "4.2 Question 2",
    "text": "4.2 Question 2\nWhat problem does Multi-Head Attention solve?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt allows the model to attend to information from different representation subspaces at different positions, overcoming the tendency of single-head self-attention to â€œaverage outâ€ information.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#question-3",
    "href": "posts/01-transformer/post.html#question-3",
    "title": "Attention is All You Need(Transformer)",
    "section": "4.3 Question 3",
    "text": "4.3 Question 3\nWhat is the purpose of positional encoding in the Transformer?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt provides the model with information about the order of the sequence, enabling it to capture positional relationships between tokens.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/post.html#question-4",
    "href": "posts/01-transformer/post.html#question-4",
    "title": "Attention is All You Need(Transformer)",
    "section": "4.4 Question 4",
    "text": "4.4 Question 4\nWhat is the time complexity of Multi-Head Attention compare to RNN/CNN?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe time complexity of Multi-Head Attention is \\(\\mathcal{O}(nÂ²d)\\), where \\(n\\) is the sequence length and \\(d\\) is the model dimension. In contrast, RNNs have a time complexity of \\(\\mathcal{O}(n)\\) due to their sequential nature, while CNNs require stacking multiple layers to capture long-range dependencies.",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html",
    "href": "00-how-to-read-paper.html",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "åœ¨æ·±å…¥å­¦ä¹ äººå·¥æ™ºèƒ½ç›¸å…³è®ºæ–‡ä¹‹å‰ï¼ŒæŒæ¡é«˜æ•ˆä¸”ç³»ç»Ÿçš„é˜…è¯»æ–¹æ³•è‡³å…³é‡è¦ã€‚è®ºæ–‡é˜…è¯»ç»å…¸æŒ‡å— How to Read a Paper ä¸­æå‡ºäº†â€œä¸‰éé˜…è¯»æ³•â€ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†æ¸…æ™°çš„å®è·µè·¯å¾„ï¼š\n\n\n\n\nç¬¬ä¸€éï¼šå¿«é€Ÿæµè§ˆï¼Œè·å–è®ºæ–‡çš„æ•´ä½“ç»“æ„å’Œæ ¸å¿ƒç»“è®ºã€‚èšç„¦äºæ ‡é¢˜ï¼ˆtitleï¼‰ã€æ‘˜è¦ï¼ˆabstractï¼‰ã€å¼•è¨€ï¼ˆintroductionï¼‰ä¸ç»“è®ºï¼ˆconclusionï¼‰ï¼Œä»å®è§‚ä¸Šäº†è§£æ–‡ç« çš„ç ”ç©¶æ–¹å‘ã€‚\nç¬¬äºŒéï¼šç»†è¯»è®ºæ–‡ï¼Œå…³æ³¨ç†è®ºæ¨å¯¼ã€å®éªŒè®¾ç½®ä¸å…³é”®å›¾è¡¨ã€‚é‡ç‚¹ç†è§£è®ºæ–‡æ‰€æå‡ºçš„æ–¹æ³•ã€å®éªŒè®¾è®¡ã€ç»“æœåˆ†æåŠå…¶æ”¯æ’‘é€»è¾‘ã€‚\nç¬¬ä¸‰éï¼šæ‰¹åˆ¤æ€§é˜…è¯»ï¼Œç³»ç»Ÿåˆ†æè®ºæ–‡çš„ä¼˜åŠ£ï¼Œæå‡ºå»ºè®¾æ€§é—®é¢˜ï¼Œåæ€è¯¥æ–¹æ³•æ˜¯å¦å…·æœ‰é€šç”¨æ€§æˆ–æ˜¯å¦èƒ½åº”ç”¨äºè‡ªèº«ç ”ç©¶ã€‚\n\n\n\nListingÂ 1: ä¸‰éé˜…è¯»æ³•\n\n\n\néœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œé˜…è¯»è®ºæ–‡æ˜¯ä¸€ä¸ªå¾ªç¯è¿­ä»£ã€é€æ­¥æ·±å…¥çš„è¿‡ç¨‹ã€‚åœ¨åå¤é˜…è¯»ä¸æ€è€ƒä¸­ï¼Œæˆ‘ä»¬ä¼šä¸æ–­ä¿®æ­£ç†è§£ã€åŠ æ·±è®¤çŸ¥ã€‚ä¸æ­¤åŒæ—¶ï¼Œé€šè¿‡ä¸ä»–äººäº¤æµã€å‚ä¸è®¨è®ºï¼Œæœ‰åŠ©äºæ‹“å®½è§†è§’ã€æ·±åŒ–æ€è€ƒã€‚ï¼ˆæœ¬é¡¹ç›®ç½‘ç«™ä¹Ÿå› æ­¤è€Œç”Ÿï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä»¬æä¾›ä¸€ä¸ªå…±åŒäº¤æµå­¦ä¹ çš„å¹³å°ï¼‰\nä¸ºæå‡é˜…è¯»æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ç²¾è¯»é˜¶æ®µï¼Œä½¿ç”¨é¢œè‰²æ ‡è®°ä¸åŒå†…å®¹ã€‚ä¾‹å¦‚åœ¨ Harvard CS197 AI Research Experiences çš„ Lecture 3 ä¸­ç»™å‡ºçš„ç­–ç•¥ï¼š\n\né»„è‰²ï¼šçªå‡ºè®ºæ–‡æ‰€è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜æˆ–æŒ‘æˆ˜ã€‚\nç»¿è‰²ï¼šæ ‡è®°è®ºæ–‡ä¸­çš„æåˆ°çš„é€šç”¨æ¦‚å¿µã€‚\nç²‰è‰²ï¼šå¯¹åº”æå‡ºçš„ç®—æ³•æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ã€‚\næ©™è‰²ï¼šç”¨äºæ ‡è®°è®ºæ–‡çš„åˆ›æ–°ç‚¹ä¸è´¡çŒ®ã€‚\n\nä¾‹å¦‚å¦‚ä¸‹ç¤ºæ„å›¾ï¼š\n\n\n\n\n\n\nFigureÂ 1: Highlight Example of paper (Attention is all you need)\n\n\n\nè¿™å¥—æ–¹æ³•å¹¶éå”¯ä¸€æ ‡å‡†ï¼Œå…³é”®åœ¨äºæ„å»ºä¸€å¥—é€‚åˆè‡ªèº«è®¤çŸ¥æ–¹å¼çš„å¯è§†åŒ–æ ‡è®°ä½“ç³»ã€‚ç»Ÿä¸€çš„æ ‡æ³¨é£æ ¼ï¼Œæœ‰åŠ©äºåœ¨åæœŸå›é¡¾æˆ–è·¨è®ºæ–‡æ¯”è¾ƒæ—¶é«˜æ•ˆå®šä½å…³é”®ä¿¡æ¯ã€‚æœ¬é¡¹ç›®æ‰€æ”¶å½•çš„æ¯ç¯‡è®ºæ–‡ç¬”è®°ä¹Ÿå°†é‡‡ç”¨è¿™ä¸€ç»“æ„åŒ–é«˜äº®æ ‡è®°æ–¹æ³•ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#è®ºæ–‡é˜…è¯»æŒ‡å—",
    "href": "00-how-to-read-paper.html#è®ºæ–‡é˜…è¯»æŒ‡å—",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "åœ¨æ·±å…¥å­¦ä¹ äººå·¥æ™ºèƒ½ç›¸å…³è®ºæ–‡ä¹‹å‰ï¼ŒæŒæ¡é«˜æ•ˆä¸”ç³»ç»Ÿçš„é˜…è¯»æ–¹æ³•è‡³å…³é‡è¦ã€‚è®ºæ–‡é˜…è¯»ç»å…¸æŒ‡å— How to Read a Paper ä¸­æå‡ºäº†â€œä¸‰éé˜…è¯»æ³•â€ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†æ¸…æ™°çš„å®è·µè·¯å¾„ï¼š\n\n\n\n\nç¬¬ä¸€éï¼šå¿«é€Ÿæµè§ˆï¼Œè·å–è®ºæ–‡çš„æ•´ä½“ç»“æ„å’Œæ ¸å¿ƒç»“è®ºã€‚èšç„¦äºæ ‡é¢˜ï¼ˆtitleï¼‰ã€æ‘˜è¦ï¼ˆabstractï¼‰ã€å¼•è¨€ï¼ˆintroductionï¼‰ä¸ç»“è®ºï¼ˆconclusionï¼‰ï¼Œä»å®è§‚ä¸Šäº†è§£æ–‡ç« çš„ç ”ç©¶æ–¹å‘ã€‚\nç¬¬äºŒéï¼šç»†è¯»è®ºæ–‡ï¼Œå…³æ³¨ç†è®ºæ¨å¯¼ã€å®éªŒè®¾ç½®ä¸å…³é”®å›¾è¡¨ã€‚é‡ç‚¹ç†è§£è®ºæ–‡æ‰€æå‡ºçš„æ–¹æ³•ã€å®éªŒè®¾è®¡ã€ç»“æœåˆ†æåŠå…¶æ”¯æ’‘é€»è¾‘ã€‚\nç¬¬ä¸‰éï¼šæ‰¹åˆ¤æ€§é˜…è¯»ï¼Œç³»ç»Ÿåˆ†æè®ºæ–‡çš„ä¼˜åŠ£ï¼Œæå‡ºå»ºè®¾æ€§é—®é¢˜ï¼Œåæ€è¯¥æ–¹æ³•æ˜¯å¦å…·æœ‰é€šç”¨æ€§æˆ–æ˜¯å¦èƒ½åº”ç”¨äºè‡ªèº«ç ”ç©¶ã€‚\n\n\n\nListingÂ 1: ä¸‰éé˜…è¯»æ³•\n\n\n\néœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œé˜…è¯»è®ºæ–‡æ˜¯ä¸€ä¸ªå¾ªç¯è¿­ä»£ã€é€æ­¥æ·±å…¥çš„è¿‡ç¨‹ã€‚åœ¨åå¤é˜…è¯»ä¸æ€è€ƒä¸­ï¼Œæˆ‘ä»¬ä¼šä¸æ–­ä¿®æ­£ç†è§£ã€åŠ æ·±è®¤çŸ¥ã€‚ä¸æ­¤åŒæ—¶ï¼Œé€šè¿‡ä¸ä»–äººäº¤æµã€å‚ä¸è®¨è®ºï¼Œæœ‰åŠ©äºæ‹“å®½è§†è§’ã€æ·±åŒ–æ€è€ƒã€‚ï¼ˆæœ¬é¡¹ç›®ç½‘ç«™ä¹Ÿå› æ­¤è€Œç”Ÿï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä»¬æä¾›ä¸€ä¸ªå…±åŒäº¤æµå­¦ä¹ çš„å¹³å°ï¼‰\nä¸ºæå‡é˜…è¯»æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ç²¾è¯»é˜¶æ®µï¼Œä½¿ç”¨é¢œè‰²æ ‡è®°ä¸åŒå†…å®¹ã€‚ä¾‹å¦‚åœ¨ Harvard CS197 AI Research Experiences çš„ Lecture 3 ä¸­ç»™å‡ºçš„ç­–ç•¥ï¼š\n\né»„è‰²ï¼šçªå‡ºè®ºæ–‡æ‰€è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜æˆ–æŒ‘æˆ˜ã€‚\nç»¿è‰²ï¼šæ ‡è®°è®ºæ–‡ä¸­çš„æåˆ°çš„é€šç”¨æ¦‚å¿µã€‚\nç²‰è‰²ï¼šå¯¹åº”æå‡ºçš„ç®—æ³•æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ã€‚\næ©™è‰²ï¼šç”¨äºæ ‡è®°è®ºæ–‡çš„åˆ›æ–°ç‚¹ä¸è´¡çŒ®ã€‚\n\nä¾‹å¦‚å¦‚ä¸‹ç¤ºæ„å›¾ï¼š\n\n\n\n\n\n\nFigureÂ 1: Highlight Example of paper (Attention is all you need)\n\n\n\nè¿™å¥—æ–¹æ³•å¹¶éå”¯ä¸€æ ‡å‡†ï¼Œå…³é”®åœ¨äºæ„å»ºä¸€å¥—é€‚åˆè‡ªèº«è®¤çŸ¥æ–¹å¼çš„å¯è§†åŒ–æ ‡è®°ä½“ç³»ã€‚ç»Ÿä¸€çš„æ ‡æ³¨é£æ ¼ï¼Œæœ‰åŠ©äºåœ¨åæœŸå›é¡¾æˆ–è·¨è®ºæ–‡æ¯”è¾ƒæ—¶é«˜æ•ˆå®šä½å…³é”®ä¿¡æ¯ã€‚æœ¬é¡¹ç›®æ‰€æ”¶å½•çš„æ¯ç¯‡è®ºæ–‡ç¬”è®°ä¹Ÿå°†é‡‡ç”¨è¿™ä¸€ç»“æ„åŒ–é«˜äº®æ ‡è®°æ–¹æ³•ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#å·¥å…·å‡†å¤‡",
    "href": "00-how-to-read-paper.html#å·¥å…·å‡†å¤‡",
    "title": "00: Preparation for Following",
    "section": "2 å·¥å…·å‡†å¤‡",
    "text": "2 å·¥å…·å‡†å¤‡\nç§‘å­¦é˜…è¯»ç¦»ä¸å¼€åˆé€‚çš„å·¥å…·æ”¯æ’‘ã€‚ä»¥ä¸‹æ˜¯æ¨èçš„å·¥å…·ä½“ç³»ï¼Œæ¶µç›–æ–‡çŒ®ç®¡ç†ã€ç¬”è®°æ•´ç†ã€ä»£ç æ‰§è¡Œç­‰å¤šä¸ªç»´åº¦ã€‚\n\n2.1 æ–‡çŒ®ç®¡ç†ï¼šZotero\néšç€è®ºæ–‡ç§¯ç´¯çš„å¢å¤šï¼Œç³»ç»Ÿçš„æ–‡çŒ®ç®¡ç†å·¥å…·ä¸å¯æˆ–ç¼ºã€‚Zotero æ˜¯ä¸€æ¬¾å…è´¹ä¸”å¼€æºçš„æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œæ”¯æŒè‡ªåŠ¨å¯¼å…¥ã€åˆ†ç»„ç®¡ç†ä¸å¤šæ ¼å¼å¼•ç”¨ï¼ˆå¦‚ BibTeXï¼‰ã€‚å…¶å¯æ‰©å±•æ€§æå¼ºï¼Œæ”¯æŒæ’ä»¶ä¸ä¸»é¢˜å®šåˆ¶ã€‚\n\n\n\n\n\n\nFigureÂ 2: Example of Zotero\n\n\n\næ¨èæ’ä»¶ï¼š\n\nBetter BibTexï¼šå¢å¼º BibTeX å¯¼å‡ºåŠŸèƒ½ï¼Œä¾¿äºä¸ LaTeX æ— ç¼é›†æˆã€‚\nEthereal Styleï¼šä¸º Zotero æä¾›ç¾è§‚çš„ UI é£æ ¼ï¼Œæå‡ä½¿ç”¨ä½“éªŒã€‚\n\nå°½ç®¡ Zotero å­˜åœ¨ä¸€å®šå­¦ä¹ æ›²çº¿ï¼Œä½†å…¶é•¿æœŸä»·å€¼è¿œè¶…åˆæœŸæŠ•å…¥ã€‚è‹¥ä»…å¸Œæœ›ä¸´æ—¶é˜…è¯»ï¼ŒPDF é˜…è¯»å™¨äº¦å¯ï¼›ä½†ä»ç§‘ç ”è§†è§’å‡ºå‘ï¼Œå»ºè®®å°½æ—©æŠ•å…¥å­¦ä¹ ä¸ä½¿ç”¨ã€‚\næ­¤å¤–ï¼ŒZotero Chrome Connector æ’ä»¶å¯å®ç°ä¸€é”®å¯¼å…¥ç½‘é¡µæ–‡çŒ®ï¼Œæå¤§æå‡æ–‡çŒ®æ”¶é›†æ•ˆç‡ï¼š\n\n\n\n\n\n\nFigureÂ 3: Zotero Chrome Connector\n\n\n\nå¦‚ FigureÂ 3 æ‰€ç¤ºï¼Œåªéœ€ç‚¹å‡»æ’ä»¶æŒ‰é’®ï¼Œå³å¯å°†å½“å‰ç½‘é¡µå†…å®¹å¯¼å…¥è‡³æ–‡çŒ®åº“ã€‚\n\n\n2.2 ç¬”è®°è®°å½•ï¼šObsidian\nObsidian æ˜¯ä¸€æ¬¾åŸºäº Markdown çš„ç¬”è®°ç³»ç»Ÿï¼Œæ”¯æŒåŒå‘é“¾æ¥ä¸å›¾è°±è§†å›¾ï¼Œç‰¹åˆ«é€‚åˆç”¨äºæ„å»ºä¸ªäººçŸ¥è¯†ä½“ç³»ã€‚\n\n\n\n\n\n\nFigureÂ 4: Obsidian Example\n\n\n\næ¨èæ’ä»¶ï¼š\n\nobsidian-latex-suiteï¼šæä¾› LaTeX å¿«æ·è¾“å…¥ä¸å…¬å¼é¢„è§ˆåŠŸèƒ½ï¼Œæ˜¾è‘—æé«˜æ•°å­¦è¡¨è¾¾æ•ˆç‡ã€‚\nHighlightr Pluginï¼šæ”¯æŒè‡ªå®šä¹‰é«˜äº®é¢œè‰²ï¼Œä¾¿äºåˆ†ç±»ä¿¡æ¯æ ‡æ³¨ã€‚\n\n \néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿‡åº¦ç¾åŒ–ç•Œé¢æˆ–æ’ä»¶å †å å¯èƒ½åè€Œåˆ†æ•£æ³¨æ„åŠ›ã€‚å»ºè®®ä»¥â€œç»“æ„æ¸…æ™°ã€å†…å®¹ä¸ºæœ¬â€ä¸ºé¦–è¦åŸåˆ™ã€‚\nå¯¹äºä¸ä½¿ç”¨ Obsidian çš„ç”¨æˆ·ï¼Œä¹Ÿå¯é€‰æ‹©ï¼š\n\n\n\n\n\n\n\n\n\n\n\n(a) Notion Home Page\n\n\n\n\n\n\n\n\n\n\n\n(b) FeiShu Home Page\n\n\n\n\n\n\n\nFigureÂ 5: Home Page of Notion and FeiShu\n\n\n\n\nNotionï¼šå¦‚ FigureÂ 5 (a) æ‰€ç¤ºï¼Œé€‚åˆå¤šäººåä½œä¸å¯è§†åŒ–ç¼–è¾‘ã€‚\né£ä¹¦ï¼šå¦‚ FigureÂ 5 (b) æ‰€ç¤ºï¼ŒåŠŸèƒ½å…¨é¢ï¼Œé€‚åˆä¼ä¸šçº§æ–‡æ¡£ç®¡ç†ã€‚\n\n\n\n2.3 ä»£ç æ‰§è¡Œï¼šJupyter Notebook\nåœ¨â€œPaper with Codeâ€ç†å¿µä¸‹ï¼Œæ¯ç¯‡è®ºæ–‡å°†é…å¥— Jupyter Notebook å®ç°æ ¸å¿ƒç®—æ³•ã€‚å…¶äº¤äº’å¼æ–‡æ¡£ç‰¹æ€§ï¼Œä½¿å…¶æˆä¸ºå­¦ä¹ ä¸éªŒè¯ä»£ç çš„ç†æƒ³å¹³å°ã€‚\n\n\n\n\n\n\nNote\n\n\n\nè‹¥å¯¹ Jupyter Notebook ä¸ç†Ÿæ‚‰ï¼Œæ¨èå‚è€ƒ å®˜æ–¹æ–‡æ¡£ï¼Œä»¥å¿«é€Ÿå…¥é—¨ã€‚\n\n\nç›¸åº”çš„ä»£ç ï¼Œæˆ‘ä¼šæ”¾åœ¨GitHubçš„ä»“åº“ä¸­\n\n\n\n\n\n\nFigureÂ 6: The preview of GitHub Page\n\n\n\n\n\n2.4 GPU å¹³å°ï¼šäº‘ç«¯æ‰§è¡Œç¯å¢ƒ\næ·±åº¦å­¦ä¹ æ¨¡å‹å¸¸éœ€ GPU åŠ é€Ÿï¼Œè‹¥æœ¬åœ°æ—  GPU å¯ä½¿ç”¨ä»¥ä¸‹å¹³å°ï¼š\n\nGoogle Colabï¼šGoogle æä¾›çš„å…è´¹äº‘ç«¯ Notebook å¹³å°ï¼Œæ”¯æŒ GPU ä¸ TPUã€‚\nKaggle Kernelsï¼šæ”¯æŒ GPU çš„æ•°æ®ç§‘å­¦å¹³å°ï¼Œé€‚åˆå¿«é€Ÿå®éªŒã€‚\n\nå›½å†…å¯é€‰å¹³å°ï¼š\n\nAutoDLï¼šé€‚åˆå›½å†…ç”¨æˆ·ï¼Œé…ç½®ç®€å•ï¼Œæ”¯æŒå®šåˆ¶åŒ–éƒ¨ç½²ã€‚\n\nå…¶ä»–æ¨èï¼š\n\nRunPodã€Lambda Labsï¼šæä¾›ç¨³å®šã€ä½å»¶è¿Ÿçš„ GPU è®­ç»ƒç¯å¢ƒï¼Œé€‚åˆä¸­å¤§å‹å®éªŒä»»åŠ¡ã€‚\n\n\né€šè¿‡åˆç†é…ç½®ä¸Šè¿°å·¥å…·ï¼Œå¯ä»¥æ„å»ºå‡ºä¸€ä¸ªç³»ç»ŸåŒ–ã€é«˜æ•ˆçš„è®ºæ–‡å­¦ä¹ ä¸ç ”ç©¶æµç¨‹ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæ¯ç¯‡è®ºæ–‡å°†é™„å¸¦ä»£ç å®ç°ã€ç»“æ„è§£æä¸æ‰¹åˆ¤æ€§æ€è€ƒï¼Œæ¬¢è¿å…±åŒå­¦ä¹ äº¤æµã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#æ€»ç»“",
    "href": "00-how-to-read-paper.html#æ€»ç»“",
    "title": "00: Preparation for Following",
    "section": "3 æ€»ç»“",
    "text": "3 æ€»ç»“\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é«˜æ•ˆé˜…è¯»è®ºæ–‡çš„æ–¹æ³•è®ºä¸å·¥å…·ä½“ç³»ã€‚é€šè¿‡â€œä¸‰éé˜…è¯»æ³•â€ ListingÂ 1ï¼Œ æˆ‘ä»¬å¯ä»¥ç³»ç»Ÿåœ°ç†è§£è®ºæ–‡å†…å®¹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ‰¹åˆ¤æ€§æ€è€ƒã€‚åŒæ—¶ï¼Œå€ŸåŠ© Zotero SectionÂ 2.1ã€ObsidianSectionÂ 2.2 ç­‰å·¥å…·ï¼Œå¯ä»¥æœ‰æ•ˆç®¡ç†æ–‡çŒ®ã€è®°å½•ç¬”è®°ä¸æ‰§è¡Œä»£ç ã€‚ åœ¨åç»­ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åº”ç”¨è¿™äº›æ–¹æ³•ä¸å·¥å…·ï¼Œæ·±å…¥åˆ†ææ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ã€å®éªŒè®¾è®¡ä¸åˆ›æ–°è´¡çŒ®ã€‚å¸Œæœ›é€šè¿‡æœ¬é¡¹ç›®çš„å­¦ä¹ ï¼Œèƒ½å¤Ÿå¸®åŠ©å¤§å®¶æ›´å¥½åœ°æŒæ¡äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‰æ²¿ç ”ç©¶åŠ¨æ€ï¼Œå¹¶åœ¨å®è·µä¸­ä¸æ–­æå‡è‡ªå·±çš„ç§‘ç ”èƒ½åŠ›ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  }
]