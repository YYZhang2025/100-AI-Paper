[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This website aims to collect and showcase 100 important AI papers and their code implementations. Each paper is accompanied by a link for readers to explore further.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This website aims to collect and showcase 100 important AI papers and their code implementations. Each paper is accompanied by a link for readers to explore further.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#why-build-this-website",
    "href": "index.html#why-build-this-website",
    "title": "100 Papers with Code",
    "section": "Why build this website?",
    "text": "Why build this website?\nOn the one head, during my study, reading, and practice in the field of AI, I found that many important papers and code implementations were scattered in various places. To facilitate access for myself and others, I decided to build this website to consolidate these important papers and my implementations in one place. On the other hand, there is a gap between topic in papers and practical application. By providing code implementations, I hope to help bridge this gap and make it easier for practitioners to apply the latest research results.\nThere are some awesome resource about the paper and implementation such as:\n\nAnnotated Research Paper: Collection of simple PyTorch implementations of neural networks and related algorithms.\nPapers with Code (It was replace by Hugging Face now): The largest resource for finding machine learning papers, code and evaluation tables.\n\nBut why I still build this website?\n\nThe Annotated Research Paper focuses more on each component of the paper, it didnâ€™t provide the whole modeling and training process.\nThe Papers with Code provides the paper and code, but the code is sometime hard to understand, and it didnâ€™t provide the explanation of the code.\n\nThis website is built to fill these gaps by providing clear explanations and easy-to-understand code implementations for each paper.Each page(paper) will come with a self-contained Jupyter Notebook that can be run directly, making it easier for readers to understand and apply the concepts presented in the papers.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-website",
    "href": "index.html#how-to-use-this-website",
    "title": "100 Papers with Code",
    "section": "How to use this website?",
    "text": "How to use this website?\nThis website is designed to be user-friendly and accessible for anyone interested in understanding and implementing the latest research in AI and machine learning. Each paper has its own dedicated page that includes\n\nThe explanation of the paper.\nCode implementation.\n\nQ & A part (you can use as flash cards).\nThe further direction might be interesting to explore (According to my understanding).\nBelow is the list of papers that have been implemented or are planned to be documented. You can click on the paper title to view detailed content.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need (Transformer )\nTransformer æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ï¼Œåœ¨è¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ä½œä¸º GPTã€BERT ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒåŸºç¡€ï¼Œæ¨åŠ¨äº†å½“ä»Šç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ã€‚\n\nNLP / Transformer\n\n\n02\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ( Vision Transformer )\nVision Transformer (ViT) æ˜¯ä¸€ç§å°†å›¾åƒåˆ’åˆ†ä¸ºå°å—ï¼ˆpatchesï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸º token è¾“å…¥æ ‡å‡† Transformer æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»çš„æ¶æ„ï¼Œé¦–æ¬¡å®ç°äº†çº¯æ³¨æ„åŠ›æœºåˆ¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨ã€‚\n\nComputer Vision / Transformer\n\n\n03\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows ( Swin Transformer )\nSwin Transformer æ˜¯ä¸€ç§ä½¿ç”¨å±‚æ¬¡åŒ–ç»“æ„å’Œæ»‘åŠ¨çª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è§†è§‰ Transformer æ¨¡å‹ï¼Œæ—¢ä¿ç•™äº†å±€éƒ¨å»ºæ¨¡çš„é«˜æ•ˆæ€§ï¼Œåˆé€šè¿‡çª—å£åç§»å®ç°è·¨åŒºåŸŸä¿¡æ¯äº¤äº’ï¼Œé€‚ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¤šç§è§†è§‰ä»»åŠ¡ã€‚\n\nComputer Vision / Transformer\n\n\n04\nLearning Transferable Visual Models From Natural Language Supervision ( CLIP )\nCLIP æ˜¯ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡å›¾æ–‡å¯¹æ¯”å­¦ä¹ ï¼Œå°†å›¾åƒä¸è‡ªç„¶è¯­è¨€æ˜ å°„åˆ°åŒä¸€è¯­ä¹‰ç©ºé—´ï¼Œä»è€Œå®ç°é›¶æ ·æœ¬å›¾åƒè¯†åˆ«ä¸è·¨æ¨¡æ€æ£€ç´¢çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹\n\nComputer Vision / Transformer\n\n\n05\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness ( Flash Attention )\nFlashAttention æ˜¯ä¸€ç§ä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°ï¼Œé€šè¿‡å‡å°‘å†…å­˜è®¿é—®å’Œæå‡è®¡ç®—æ•ˆç‡ï¼Œå®ç°æ›´å¿«ã€æ›´èŠ‚çœèµ„æºçš„ Transformer æ¨ç†ä¸è®­ç»ƒã€‚\n\nTransformer / AI Engine",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html",
    "href": "posts/02-vision-transformer/Vision-Transformer.html",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "",
    "text": "åœ¨äº†è§£äº†ä»€ä¹ˆæ˜¯Transformerä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å°†Transformeråº”ç”¨äºComputer Visionã€‚Vision Transformerï¼ˆViTï¼‰(Dosovitskiy et al. 2021) æ˜¯ä¸€ä¸ªå°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒåˆ†ç±»çš„æ¨¡å‹ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å›¾åƒåˆ’åˆ†ä¸ºå°å—ï¼ˆpatchesï¼‰ï¼Œç„¶åå°†è¿™äº›å°å—è§†ä¸ºåºåˆ—æ•°æ®ï¼Œç±»ä¼¼äºå¤„ç†æ–‡æœ¬æ•°æ®ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.1 Patch Embedding",
    "text": "1.1 Patch Embedding\n\nThe standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches\\(x \\in \\mathbb{R}^{N \\times (P^{2} \\times C)}\\), where (\\(H, W\\)) is the resolution of the original image, \\(C\\) is the number of channels, (\\(P, P\\)) is the resolution of each image patch, and \\(N = HW/P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size \\(D\\) through all of its layers, so we flatten the patches and map to \\(D\\) dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nåœ¨Transformer è¿™ä¸€ç¯‡ï¼Œæˆ‘ä»¬äº†è§£åˆ°ï¼Œå®ƒæ˜¯ä½œç”¨äºSequence Modelingçš„ï¼Œå¾ˆæ˜¾ç„¶ï¼ŒImage ä¸æ˜¯ Sequenceçš„ã€‚å¾ˆç›´è§‚çš„ç¬¬ä¸€ç§æƒ³æ³•å°±æ˜¯ï¼Œå°†å›¾ç‰‡ç›´æ¥å±•å¼€ï¼Œä»äºŒç»´ (\\(3, H, W\\)) å±•å¼€æˆä¸€ç»´çš„ (\\(3, H \\times W\\)). è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°çš„å›¾ç‰‡çš„Sequence Modelã€‚å¦‚ä¸‹å›¾@fig-flat-imageæ‰€ç¤º\n\n\n\n\n\n\nFigureÂ 2\n\n\n\nè¿™ç§æ–¹æ³•æœ‰ä¸€ç§æ˜æ˜¾çš„é—®é¢˜å°±æ˜¯ï¼šSequenceçš„é•¿åº¦å¤ªé•¿ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äº \\(3\\times 256 \\times 256\\) çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬æœ‰ \\(256 \\times 256 = 65,336\\) ä¸ªtokensï¼Œé€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ‰€éœ€è¦çš„è®­ç»ƒæ—¶é•¿å¾ˆé•¿ã€‚å¹¶ä¸”å®ƒæ²¡æœ‰ç”¨åˆ°å›¾ç‰‡çš„ä¸€ä¸ªç‰¹æ€§ï¼šç›¸é‚»çš„pixel ä¹‹é—´ï¼Œæ˜¯æœ‰å¾ˆé«˜çš„correlationçš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¾ˆè‡ªç„¶çš„æƒ³åˆ°ï¼šå¦‚æœæŠŠç›¸é‚»çš„pixelså’Œåœ¨ä¸€ç»„ï¼Œç»„æˆä¸€ä¸ªpatchï¼Œè¿™æ ·ä¸å°±æ—¢å‡å°‘äº†tokensçš„æ•°é‡ï¼Œåˆç”¨åˆ°äº†pixelä¹‹é—´çš„correlationã€‚è¿™å°±æ˜¯Vision Transformer çš„Patch Embeddingã€‚ è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†ã€‚ æ¥ä¸‹æ¥æˆ‘ä»¬åªéœ€è¦ç”¨ï¼Œä¸€ä¸ªMLPï¼Œå°†æˆ‘ä»¬å±•å¼€çš„patchï¼Œæ˜ å°„åˆ° \\(D\\)- dimensionçš„ç©ºé—´ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä¼ å…¥Transformer æ¨¡å‹äº†ã€‚\næ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç æ€ä¹ˆå®ç°ï¼š\n# Load Image and resize it to certain size\nimage_path = IMAGE_PATN\nimg_bgr = cv2.imread(image_path)\nimg_resized = cv2.resize(img_bgr, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\nimg = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB) \n\n# Patchify \npatches = einops.rearrange(\n     img, \"(h ph) (w pw) c -&gt; (h w) ph pw c\", ph=PATCH_SIZE, pw=PATCH_SIZE\n)\né€šè¿‡è¿™ä¸ªPatchifyåªæœ‰ï¼Œæˆ‘ä»¬å¾—åˆ°å°†å›¾ç‰‡Patchåˆ°äº†\n\n\nåˆ†æˆäº†ä¸åŒçš„å°Patchã€‚\næ¥ä¸‹æ¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯ï¼Œå°†è¿™äº›Patch å±•å¼€ï¼Œç„¶åä¼ å…¥ä¸€ä¸ªMLPï¼Œ\nflat_patch = einops.rearrange(\n     patches, \"n ph pw c -&gt; n (ph pw c)\"\n)\n\n\nmlp = nn.Linear(PATCH_SIZE * PATCH_SIZE * 3, d_model)\npatch_embedding = mlp(flat_patch)\nåŒè¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å°±å¯ä»¥è§å›¾ç‰‡è½¬åŒ–ä¸ºTransformerå¯ä»¥æ¥å—çš„vectorã€‚ä¸è¿‡åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸ä¼šç”¨ä»¥ä¸Šçš„æ–¹å¼ï¼Œå› ä¸ºä¸Šé¢çš„æ–¹å¼å®ç°èµ·æ¥æ¯”è¾ƒæ…¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†Patch å’Œ Linear Projectå’Œåœ¨ä¸€èµ·ã€‚\n\n\n\n\n\n\nTip\n\n\n\nå°†å‡ ä¸ªtensor çš„operationæ“ä½œåˆæˆä¸€ä¸ªçš„æ–¹æ³•ï¼Œå«åškernel fusionï¼Œè¿™æ˜¯ä¸€ç§æé«˜è®­ç»ƒå’Œæ¨ç†ç´ çš„æ–¹æ³•\n\n\nåœ¨å®é™…çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ç”¨Convolution Layer ä»£æ›¿ Patch + Flatten+ Linear çš„æ–¹æ³•. å¦‚æœæˆ‘ä»¬ç”¨ä¸€ä¸ª å·ç§¯å±‚ï¼Œå‚æ•°è®¾ç½®ä¸ºï¼š â€¢ kernel_size = PATCH_SIZE ï¼ˆå·ç§¯æ ¸è¦†ç›–ä¸€ä¸ª patchï¼‰ â€¢ stride = PATCH_SIZE ï¼ˆä¸é‡å åœ°ç§»åŠ¨ï¼Œç›¸å½“äºåˆ‡ patchï¼‰ â€¢ in_channels = 3ï¼ˆRGBï¼‰ â€¢ out_channels = d_model\né‚£ä¹ˆå·ç§¯ä¼šï¼š 1. æŠŠè¾“å…¥å›¾ç‰‡åˆ†æˆ PATCH_SIZE x PATCH_SIZE çš„ä¸é‡å å—ï¼ˆå› ä¸º stride = kernel_sizeï¼‰ã€‚ 2. å¯¹æ¯ä¸ª patch åšä¸€æ¬¡çº¿æ€§æ˜ å°„ï¼ˆå› ä¸ºå·ç§¯æœ¬è´¨ä¸Šå°±æ˜¯å¯¹å±€éƒ¨åŒºåŸŸåšåŠ æƒæ±‚å’Œï¼Œç›¸å½“äº Linearï¼‰ã€‚ 3. è¾“å‡ºçš„ shape è‡ªåŠ¨å°±æ˜¯ (batch, num_patches, d_model)ã€‚\nè¿™æ­£å¥½ç­‰ä»·äº åˆ‡ patch + flatten + Linear çš„ç»„åˆã€‚\nä»£ç å¦‚ä¸‹ï¼š\nclass PatchEmbedding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.conv = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=config.hidden_dim,\n            kernel_size=config.patch_size,\n            stride=config.patch_size,\n            padding=\"valid\" if config.patch_size == 16 else \"same\",\n        )\n\n    def forward(self, imgs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        imgs: (batch_size, num_channels, height, width)\n        Returns: (batch_size,  num_patches_height, num_patches_width, hidden_dim)\n        \"\"\"\n        # (B, C, H, W) -&gt; (B, hidden_dim, H', W')\n        x = self.conv(imgs)\n\n        # (B, hidden_dim, H', W') -&gt; (B, hidden_dim, H' * W')\n        x = x.flatten(2)\n\n        # (B, hidden_dim, H' * W') -&gt; (B, H' * W', hidden_dim)\n        x = x.transpose(1, 2)\n        return x\nç”¨å·ç§¯çš„å¥½å¤„ï¼Œé™¤äº†å¯ä»¥æ›´é«˜æ•ˆçš„å®ç°Patch Embeddingï¼Œä»£ç æ›´åŠ ç®€æ´ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡æ”¹å˜ stride æ¥ä½¿ä¸€äº›Patch overlappingï¼Œè·å¾—ä¸€ä¸ªå¤šå°ºåº¦çš„ç»“æ„ï¼Œ\nThe image is convert along this process: \\[\n\\boxed{\n\\mathbf{x} \\in \\mathbb{R}^{C \\times H \\times W}\n\\quad \\xrightarrow{\\text{Patchify}} \\quad\n\\{ x_i \\in \\mathbb{R}^{C \\times P \\times P} \\}{i=1}^N\n\\quad \\xrightarrow{\\text{Flatten}} \\quad\n\\{ x_i \\in \\mathbb{R}^{(C \\cdot P \\cdot P)} \\}_{i=1}^N\n\\quad \\xrightarrow{\\text{Linear } W \\in \\mathbb{R}^{(C \\cdot P \\cdot P) \\times D}} \\quad\n\\{ z_i \\in \\mathbb{R}^{D} \\}_{i=1}^N\n}\n\\tag{1}\\]",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.2 Position Encoding",
    "text": "1.2 Position Encoding\nå°†å›¾ç‰‡è½¬åŒ–ä¸º Transformer çš„è¾“å…¥ä¹‹åï¼Œæ¥ä¸‹æ¥Transformerä¸­çš„å¦ä¸€ä¸ªç»„ä»¶å°±æ˜¯ä¼ å…¥ Position Informationã€‚æˆ‘ä»¬çŸ¥é“åœ¨Transformer ä¸­ï¼Œä»–ä»¬ç”¨çš„æ˜¯ sine-cosine position embeddingï¼Œåœ¨é‚£ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæåˆ°äº†ï¼Œè¿˜å­˜åœ¨å…¶ä»–ä¸åŒçš„Position Encodingçš„åŠæ³•ï¼ŒViT ç”¨çš„å°±æ˜¯å¦ä¸€ç§åŠæ³•ï¼ŒLearned Position Embeddingã€‚Learned Position Embeddingçš„æ–¹æ³•å¾ˆç®€å•ï¼Œä¹Ÿå¾ˆå¥½ç†è§£ï¼Œå¯¹äºæ¯ä¸€ä¸ªä½ç½®ï¼Œæˆ‘ä»¬ç»™ä»–ä¸€ä¸ªindexï¼Œå°†è¿™ä¸ªindexä¼ å…¥ä¸€ä¸ª Embedding Matrixï¼Œ æˆ‘ä»¬å°±å¾—åˆ°ä¸€ä¸ªPosition Embeddingã€‚ä¸è¿‡ä¸Token Embeddingä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬ä¼šç”¨åˆ°æ‰€æœ‰çš„Positionï¼Œä¹Ÿæ•´ä¸ªmatrixï¼Œ æ‰€ä»¥æˆ‘ä»¬ä¸ç”¨å®šindexï¼Œç›´æ¥å®šä¹‰æ•´ä¸ªEmbeddingï¼Œç„¶åå°†å®ƒä¼ å…¥Transformerä¸­ã€‚\nclass PositionalEncoding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.positional_embedding = nn.Parameter(\n            torch.randn(\n                1,\n                (config.image_size // config.patch_size) ** 2 + 1,\n                config.hidden_dim,\n            )\n        )\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Add positional encoding to the input tensor\n        batch_size = x.size(0)\n\n        pos_embedding = self.positional_embedding.expand(batch_size, -1, -1)\n        cls_token = self.cls_token.expand(batch_size, -1, -1)\n\n        x = torch.cat((cls_token, x), dim=1)\n        return x + pos_embedding\nä¸ºä»€ä¹ˆViTè¦ç”¨Learned Position Embeddingå‘¢ï¼Ÿåœ¨ViTè¿™ç¯‡æ–‡ç« ä¸­ï¼Œä»–ä»¬å°è¯•è¿‡ä¸åŒçš„Position Embeddingï¼Œæ¯”å¦‚ï¼š\n\nNo Positional Information\n1-dimensional Positional Embedding\n2-dimensional Positional Embedding\nRelative Positional Embedding\n\nå‘ç°ï¼Œé™¤äº†No Positional Informationä¹‹å¤–ï¼Œå…¶ä½™3ç§åœ¨Image Classificationä¸­çš„è¡¨ç°ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚\n\n\n\n\n\n\nFigureÂ 3\n\n\n\nè®ºæ–‡ä¸­è¡¨ç¤ºï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‰€éœ€è¦çš„ Positionçš„ä¿¡æ¯è¾ƒå°ï¼Œå¯¹äºä¸åŒç§ç±»çš„Position Embeddingçš„æ–¹æ³•ï¼Œå­¦ä¹ è¿™ä¸ªPosition Informationçš„èƒ½åŠ›ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚\n\nWe speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., \\(14 \\times 14\\) as opposed to \\(224 \\times 224\\), and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.18 \n\nä¸è¿‡ï¼Œå°½ç®¡Positionçš„æ–¹æ³•ä¸é‡è¦ï¼Œä½†æ˜¯ä¸åŒçš„è®­ç»ƒå‚æ•°ï¼Œè¿˜æ˜¯ä¼šå½±å“åˆ°å­¦ä¹ åˆ°çš„Position Information, ä¸‹å›¾æ‰€ç¤ºï¼š\n\n\n\n\n\n\nFigureÂ 4\n\n\n\n\n1.2.1 Extending Position Encoding\nå½“æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªPre-Trainingçš„æ¨¡å‹ï¼Œæˆ‘ä»¬æƒ³ç”¨å®ƒFine-Tuningåˆ°ä¸€ä¸ªä¸åŒå›¾ç‰‡å¤§å°çš„æ•°æ®åº“ï¼Œæˆ‘ä»¬æ”¹æ€ä¹ˆåšå‘¢ï¼Œç¬¬ä¸€ä¸ªæ–¹æ³•å½“ç„¶æ˜¯ï¼ŒResize æˆ‘ä»¬çš„å›¾ç‰‡ï¼Œåˆ°ViT Pre-trainingçš„å›¾ç‰‡å¤§å°ï¼Œä½†æ˜¯ï¼Œè¿™ä¸ªèƒ½å¯¼è‡´è¾ƒå¤§çš„å›¾ç‰‡ï¼Œå¤±å»å¾ˆå¤šç»†èŠ‚ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿æŒå›¾ç‰‡çš„å¤§å°ä¸å˜ï¼ŒåŒæ—¶è®©æ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬å°±éœ€è¦Extend Position Encodingï¼Œå› ä¸ºå½“Patch Sizeä¸å˜ï¼Œå›¾ç‰‡å¤§å°å˜äº†çš„è¯ï¼Œäº§ç”Ÿçš„Number of Patches ä¹Ÿæ˜¯ä¼šæ”¹å˜çš„ï¼Œè¿™æ ·ï¼Œå°±æ˜¯æŸå¤±ä¸€äº›ä¿¡æ¯ã€‚æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼Œæ‰¾åˆ°ä¸€ç§æ–¹æ³•ï¼Œå¢å¤§æˆ–è€…å‡å°Positionçš„æ•°é‡ã€‚ è¿™å°±æ˜¯æ‰€è°“çš„Position Interpolationã€‚\n2D interpolation of the pre-trained position embeddings â€¢ ViT åœ¨é¢„è®­ç»ƒæ—¶ï¼Œé€šå¸¸ç”¨å›ºå®šè¾“å…¥åˆ†è¾¨ç‡ï¼ˆæ¯”å¦‚ 224Ã—224ï¼‰ â†’ ç”Ÿæˆå›ºå®šæ•°é‡çš„ patchï¼ˆæ¯”å¦‚ 16Ã—16 patch â†’ 196 ä¸ª patchï¼‰ã€‚ â€¢ ä½†åœ¨ fine-tuning æ—¶ï¼Œè¾“å…¥å›¾ç‰‡å¯èƒ½å¤§å°ä¸ä¸€æ ·ï¼Œæ¯”å¦‚ 384Ã—384ï¼Œè¿™æ—¶ patch æ•°é‡å°±å˜äº†ã€‚ â€¢ è¿™ä¼šå¯¼è‡´åŸæœ¬çš„ ä½ç½®ç¼–ç  (position embeddings) å’Œæ–°çš„ patch æ•°é‡å¯¹ä¸ä¸Šã€‚ â€¢ è§£å†³åŠæ³•ï¼šå¯¹é¢„è®­ç»ƒå¥½çš„ä½ç½®ç¼–ç åš äºŒç»´æ’å€¼ (2D interpolation)ï¼Œæ ¹æ® patch åœ¨åŸå›¾ä¸­çš„ç©ºé—´ä½ç½®ï¼ŒæŠŠä½ç½®ç¼–ç æ‹‰ä¼¸/ç¼©æ”¾åˆ°æ–°çš„åˆ†è¾¨ç‡ã€‚\n\nThe Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.4 \n\ndef interpolate_pos_encoding(self, x, w, h):\n    npatch = x.shape[1] - 1\n    N = self.pos_embed.shape[1] - 1\n    if npatch == N and w == h:\n        return self.pos_embed\n    class_pos_embed = self.pos_embed[:, 0]\n    patch_pos_embed = self.pos_embed[:, 1:]\n    dim = x.shape[-1]\n    w0 = w // self.patch_embed.patch_size\n    h0 = h // self.patch_embed.patch_size\n    \n    patch_pos_embed = F.interpolate(\n        patch_pos_embed.reshape(\n            1, \n            int(math.sqrt(N)), \n            int(math.sqrt(N)), \n            dim\n        ).permute(0, 3, 1, 2),\n        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n        mode='bicubic',\n    )\n    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.3 [CLS] Tokens & MLP Head",
    "text": "1.3 [CLS] Tokens & MLP Head\nåœ¨ Transformer è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬äº†è§£åˆ°ï¼šæ¯è¾“å…¥ä¸€ä¸ªtokenï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„tokenã€‚è¿™å°±æ˜¯è¯´ï¼Œå¯¹äºæ¯ä¸ªpatchï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„Tokensï¼Œé‚£ä¹ˆï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©å“ªä¸€ä¸ªtokenä½œä¸ºæˆ‘ä»¬å›¾ç‰‡çš„è¡¨ç¤ºå‘¢ã€‚ BERT (Devlin et al. 2019)ï¼Œ ç”¨äº†ä¸€ä¸ª [CLS], æ¥è¡¨ç¤ºä¸€ä¸ªå¥å­ã€‚åŒç†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ·»åŠ ä¸€ä¸ª [CLS] token, æ¥è¡¨ç¤ºä¸€å¼ å›¾ç‰‡ã€‚åŒæ—¶ï¼Œå¯¹äº [CLS] token, æˆ‘ä»¬ä¹Ÿè¦åœ¨ç»™ä»–ä¸€ä¸ªè¡¨ç¤ºä½ç½®çš„ä¿¡æ¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨Position Encodingä¸Šï¼Œæˆ‘ä»¬æœ‰ (config.image_size // config.patch_size) ** 2 + 1, ä½ç½®ä¿¡æ¯ï¼Œå…¶ä¸­ +1 å°±æ˜¯ [CLS] çš„ä½ç½®ä¿¡æ¯ã€‚ æ€»ç»“ä¸€ä¸‹ [CLS] token çš„ä½œç”¨å°±æ˜¯ç”¨æ¥èšåˆæ‰€æœ‰çš„Patchçš„æ¶ˆæ¯ï¼Œç„¶åç”¨æ¥Image çš„Representationã€‚\næˆ‘ä»¬æƒ³ä¸€ä¸‹ï¼Œé™¤äº†åŠ ä¸€ä¸ª [CLS] tokenï¼Œä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰å…¶ä»–åŠæ³•æ¥è¡¨ç¤ºå›¾ç‰‡å—ã€‚æœ‰ä¸€ç§å¾ˆè‡ªç„¶çš„æ–¹æ³•å°±æ˜¯ï¼Œå°†æ‰€æœ‰çš„patchçš„æ¶ˆæ¯æ”¶é›†èµ·æ¥ï¼Œç„¶åå»ä¸€ä¸ªå¹³å‡å€¼æ¥è¡¨ç¤ºè¿™ä¸ªå›¾ç‰‡ã€‚ç±»ä¼¼äºä¼ ç»Ÿçš„ConvNet(e.g.Â ResNet) æˆ‘ä»¬å¯ä»¥é€šè¿‡ AvgPooling æ¥å®ç°ã€‚ ä¸è¿‡è®ºæ–‡ä¸­æåˆ°ï¼Œ å¯¹äºä¸¤ç§ä¸åŒçš„Image Representationï¼Œéœ€è¦æœ‰ä¸åŒçš„Learning Rate æ¥è®­ç»ƒè¿™ä¸ªç½‘ç»œã€‚\nOther content \næœ‰äº†Image Representä¹‹åï¼Œæˆ‘ä»¬åªéœ€è¦å°†è¿™ä¸ªä¼ å…¥ä¸€ä¸ªç®€å•çš„MLPï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªClassifierã€‚MLPçš„è¾“å…¥æ˜¯hidden dimï¼Œè¾“å…¥åˆ™æ˜¯æˆ‘ä»¬Number of Classesã€‚ä¸åŒçš„Index è¡¨ç¤ºä¸åŒçš„Classsesã€‚\n\nAn initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classifierâ€”just like ResNetâ€™s final feature mapâ€”performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate,  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.17 \n\n\nBoth during pre-training and fine-tuning, a classification head is attached to \\(\\mathrm{z}_{L}^{0}\\). The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nclass ClassifierHead(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n        self.fc2 = nn.Linear(config.mlp_dim, config.num_classes)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_classes)\n        \"\"\"\n        # Use the CLS token for classification\n        cls_token = x[:, 0, :]\n        x = F.relu(self.fc1(cls_token))\n        \n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.4 Transformer Encoder Block",
    "text": "1.4 Transformer Encoder Block\nè‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»è®²å®Œäº†ViTä¸Transformerçš„ä¸»è¦ä¸åŒä¹‹å¤„ã€‚æ¥ä¸‹æ¥ï¼Œå°±æ˜¯Transformerçš„Encoderã€‚ \nè¿™éƒ¨åˆ†ï¼Œå’ŒTransformeråŸæœ¬çš„Encoderå¾ˆç±»ä¼¼ï¼Œåªä¸è¿‡æœ‰å‡ å¤„ä¸åŒï¼š\n\nPre-Norm: åœ¨ViTåŒï¼Œè¾“å…¥å…ˆè¿›è¡Œä¸€ä¸ªLayerNormï¼Œç„¶ååœ¨ä¼ å…¥MHAæˆ–è€…MLPä¸­ï¼Œåè§‚åœ¨TransformeråŸæœ¬çš„Encoderä¸­ï¼Œæˆ‘ä»¬æ˜¯å…ˆå°†MHAæˆ–è€…MLPçš„è¾“å‡ºä¸è¾“å…¥åŠ åœ¨ä¸€èµ·ï¼Œä¹‹åå†è¿›è¡Œä¸€ä¸ªNormalizationã€‚è¿™å«åšPost-Norm\nMLPçš„å®ç°ï¼šåœ¨Transformer Encoderä¸­ï¼Œç”¨çš„æ˜¯ ReLU, è€Œåœ¨ViTä¸­ï¼Œç”¨çš„æ˜¯ GELU\n\né™¤æ­¤ä¹‹å¤–ï¼Œå…¶ä»–éƒ¨åˆ†éƒ½æ˜¯ä¸€æ ·çš„ã€‚ä¸€ä¸‹æ˜¯ViT Encoderçš„å®ç°ï¼š\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.norm1 = LayerNorm(config.hidden_dim)\n        self.mha = MHA(config)\n        \n        self.norm2 = LayerNorm(config.hidden_dim)\n        self.ffn = FFN(config)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Multi-head attention\n        redisual = x\n        x = self.norm1(x)\n        x = redisual + self.mha(x)\n\n        # Feed-forward network\n        redisual = x\n        x = self.norm2(x)\n        x = x + self.ffn(x)\n\n        return x",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.5 CNN vs.Â ViTï¼š Inductive bias",
    "text": "1.5 CNN vs.Â ViTï¼š Inductive bias\nè‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»å®Œäº†Vision Transformerï¼Œæˆ‘ä»¬æ¥ä»Inductive Bias çš„æ–¹é¢ï¼Œçœ‹çœ‹ CNN å’Œ ViT æœ‰ä»€ä¹ˆä¸åŒ\n\n\n\n\n\n\nä»€ä¹ˆæ˜¯Inductive Bias\n\n\n\nåœ¨æ·±åº¦å­¦ä¹ é‡Œï¼ŒInductive Biasï¼ˆå½’çº³åç½®ï¼‰æ˜¯æŒ‡æ¨¡å‹åœ¨å­¦ä¹ ä¹‹å‰ï¼Œå› ç»“æ„æˆ–è®¾è®¡è€Œè‡ªå¸¦çš„å‡è®¾æˆ–å…ˆéªŒã€‚\n\n\nå¯¹äºå›¾åƒæ¥è¯´ï¼Œå¸¸è§çš„å…ˆéªŒå°±æ˜¯ï¼š\n\nå±€éƒ¨åƒç´ æ˜¯ç›¸å…³çš„ï¼ˆlocalityï¼‰\nç›¸é‚»åŒºåŸŸçš„æ¨¡å¼æœ‰è§„å¾‹ï¼ˆ2D neighborhoodï¼‰\nç‰©ä½“æ— è®ºå‡ºç°åœ¨å›¾åƒå“ªé‡Œï¼Œè¯†åˆ«æ–¹å¼åº”è¯¥ä¸€æ ·ï¼ˆtranslation equivarianceï¼‰\n\nğŸ”¹ 2. CNN çš„ç»“æ„æ€ä¹ˆä½“ç°è¿™äº›åç½®ï¼Ÿ 1. å±€éƒ¨æ€§ (Locality) â€¢ å·ç§¯æ ¸ï¼ˆä¾‹å¦‚ 3Ã—3ï¼‰åªå’Œå±€éƒ¨åƒç´ æ‰“äº¤é“ï¼Œè€Œä¸æ˜¯å…¨å›¾ã€‚ â€¢ è¿™æ„å‘³ç€æ¨¡å‹â€œç›¸ä¿¡â€å›¾åƒçš„é‡è¦ç‰¹å¾æ¥è‡ªå±€éƒ¨é‚»åŸŸï¼Œè€Œä¸æ˜¯é¥è¿œåŒºåŸŸã€‚ 2. äºŒç»´é‚»åŸŸç»“æ„ (2D structure) â€¢ å·ç§¯æ“ä½œæ˜¯æ²¿ç€ å›¾åƒçš„äºŒç»´ç½‘æ ¼è¿›è¡Œçš„ï¼Œå¤©ç„¶åˆ©ç”¨äº†å›¾åƒçš„è¡Œåˆ—ç»“æ„ã€‚ â€¢ è¿™å’Œæ–‡æœ¬ï¼ˆåºåˆ— 1Dï¼‰ä¸ä¸€æ ·ï¼ŒCNN æ˜ç¡®çŸ¥é“è¾“å…¥æ˜¯ 2D æ’åˆ—çš„ã€‚ 3. å¹³ç§»ç­‰å˜æ€§ (Translation equivariance) â€¢ å·ç§¯æ ¸çš„å‚æ•°åœ¨æ•´å¼ å›¾å…±äº«ã€‚ â€¢ æ‰€ä»¥çŒ«åœ¨å·¦ä¸Šè§’è¿˜æ˜¯å³ä¸‹è§’ï¼Œå·ç§¯æ ¸éƒ½èƒ½æ£€æµ‹åˆ°â€œçŒ«è€³æœµâ€ã€‚ â€¢ è¿™è®© CNN è‡ªåŠ¨å…·æœ‰â€œè¯†åˆ«ä½ç½®æ— å…³â€çš„èƒ½åŠ›ã€‚\nè¿™äº›æ€§è´¨ä¸æ˜¯æ¨¡å‹é€šè¿‡è®­ç»ƒå­¦å‡ºæ¥çš„ï¼Œè€Œæ˜¯å› ä¸º å·ç§¯æ“ä½œæœ¬èº«çš„æ•°å­¦ç»“æ„å°±å¸¦æ¥çš„ï¼š â€¢ kernel çš„å±€éƒ¨è¿æ¥ â†’ å±€éƒ¨æ€§ â€¢ kernel æ»‘åŠ¨è¦†ç›–å…¨å›¾ â†’ å¹³ç§»ç­‰å˜æ€§ â€¢ æ“ä½œåœ¨äºŒç»´ç©ºé—´å®šä¹‰ â†’ é‚»åŸŸç»“æ„ â€¢ æ‰€ä»¥ï¼Œå“ªæ€•ä½ ä¸ç»™ CNN å–‚å¤ªå¤šæ•°æ®ï¼Œå®ƒä¹Ÿä¼šåˆ©ç”¨è¿™äº›åç½®å»å­¦ä¹ ç‰¹å¾ã€‚\nè€Œå¯¹äº ViT æ¥è¯´ï¼š ViT çš„å½’çº³åç½®éå¸¸å¼±ï¼Œå‡ ä¹å®Œå…¨ä¾èµ–æ•°æ®å’Œè®­ç»ƒæ¥å­¦ä¹ ã€‚ 1. Patch åˆ‡åˆ† (Patchification) â€¢ ViT å”¯ä¸€çš„â€œå›¾åƒå…ˆéªŒâ€ä¹‹ä¸€å°±æ˜¯æŠŠè¾“å…¥å›¾ç‰‡åˆ‡æˆ patchã€‚ â€¢ è¿™ä¸€æ“ä½œéšå«äº†ï¼šå›¾åƒæ˜¯ä¸€ä¸ªäºŒç»´ç»“æ„ï¼Œå¯ä»¥è¢«åˆ†å—å¤„ç†ã€‚ 2. ä½ç½®ç¼–ç  (Positional Embeddings) â€¢ Transformer æœ¬èº«åªå¤„ç†åºåˆ—ï¼Œæ²¡æœ‰ç©ºé—´ç»“æ„çš„æ¦‚å¿µã€‚ â€¢ ViT é€šè¿‡åŠ ä½ç½®ç¼–ç å‘Šè¯‰æ¨¡å‹ patch åœ¨å›¾åƒä¸­çš„ç›¸å¯¹ä½ç½®ã€‚ â€¢ åœ¨è¾“å…¥åˆ†è¾¨ç‡å˜åŒ–æ—¶ï¼Œä¼šåš äºŒç»´æ’å€¼ (2D interpolation) æ¥é€‚é…ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§äººå·¥å¼•å…¥çš„ 2D å…ˆéªŒã€‚ 3. å…¶ä»–éƒ¨åˆ† â€¢ é™¤äº†ä»¥ä¸Šä¸¤ç‚¹ï¼ŒViT çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ å…¨å±€çš„ (global)ï¼Œæ²¡æœ‰å±€éƒ¨æ€§çº¦æŸã€‚ â€¢ æ²¡æœ‰åƒ CNN é‚£æ ·å†…ç½®çš„å¹³ç§»ç­‰å˜æ€§æˆ–å±€éƒ¨é‚»åŸŸç»“æ„ã€‚\nè¿™æ ·å°±æ˜¯ä¸ºä»€ä¹ˆViTéœ€è¦æ›´å¤šæ•°æ®å’Œè®¡ç®—æ‰èƒ½å­¦åˆ°åŒæ ·çš„ç©ºé—´å½’çº³è§„å¾‹ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.6 ViT Model Variants",
    "text": "1.6 ViT Model Variants\nViT æœ‰3ç§ä¸åŒçš„åŸºæœ¬å˜å½¢ï¼Œ å¦‚ä¸‹å›¾æ‰€ç¤º \nViTçš„åå­—é€šå¸¸è¡¨ç¤ºä¸º: ViT-L/16: æ„æ€æ˜¯ï¼ŒViT-Largeï¼Œç„¶åç”¨çš„16 Patch Sizeã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒPatch Sizeè¶Šå¤§ï¼Œæˆ‘ä»¬å¾—åˆ°çš„tokenså°±è¶Šå°‘ï¼Œä¹Ÿå°±æ˜¯éœ€è¦æ›´å°‘çš„è®­ç»ƒæ—¶å®ç°ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#å‡å°‘tokens",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#å‡å°‘tokens",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "3.1 å‡å°‘Tokens",
    "text": "3.1 å‡å°‘Tokens\n\nPatch Merge\nPatch Shuffle",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "3.2 Vision Language Model",
    "text": "3.2 Vision Language Model\næˆ‘ä»¬ä»¥åŠå­¦ä¹ äº†ViT for computer Visionï¼Œ Transformer for NLPï¼Œ æ¥ä¸‹æ¥æœ‰ä»€ä¹ˆåŠæ³•è®©è¿™ä¸¤ç§æ¨¡å‹ç»“åˆèµ·æ¥å‘¢ï¼Ÿ CLIP (2021): å°† ViT èåˆåˆ° vision-language é¢„è®­ç»ƒä¸­ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#axial-attentionè½´å‘æ³¨æ„åŠ›",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#axial-attentionè½´å‘æ³¨æ„åŠ›",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "4.1 Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰",
    "text": "4.1 Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰\nåœ¨å¤„ç† å›¾åƒæˆ–è§†é¢‘ è¿™ç±»é«˜ç»´è¾“å…¥æ—¶ï¼Œå¦‚æœç›´æ¥å¯¹æ‰€æœ‰åƒç´ åš å…¨å±€ self-attentionï¼Œå¤æ‚åº¦æ˜¯ \\(\\mathcal{O}(H^2 W^2)\\) ï¼ˆ\\(H, W\\) æ˜¯é«˜å’Œå®½ï¼‰ã€‚å½“å›¾åƒå¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªä»£ä»·å¤ªé«˜ã€‚ æ ¸å¿ƒæƒ³æ³•ï¼šæŠŠäºŒç»´ attention æ‹†æˆä¸¤æ¬¡ä¸€ç»´ attentionï¼ˆæ²¿ç€å›¾åƒçš„ä¸¤ä¸ªâ€œè½´â€åˆ†åˆ«åšï¼‰ã€‚ 1. Row-wise Attentionï¼ˆè¡Œæ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€æ°´å¹³æ–¹å‘ï¼ˆå®½åº¦è½´ Wï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€è¡Œçš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦ï¼š\\(\\mathcal{O}(H \\cdot W^2)\\)ã€‚ 2. Column-wise Attentionï¼ˆåˆ—æ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€å‚ç›´æ–¹å‘ï¼ˆé«˜åº¦è½´ Hï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€åˆ—çš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦ï¼š \\(\\mathcal{O}(W \\cdot H^2)\\)ã€‚\nç»„åˆèµ·æ¥ï¼Œç›¸å½“äºåœ¨ H å’Œ W ä¸¤ä¸ªè½´ä¸Šéƒ½åšäº†å…¨å±€ä¾èµ–å»ºæ¨¡ã€‚\n\n\n\n\n\n\nFigureÂ 5",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "â€œSwin V2â€ (Liu et al. 2022) æ˜¯åœ¨åŸå§‹ Swin Transformer çš„åŸºç¡€ä¸Šï¼Œä¸ºäº†æ›´å¥½åœ° æ‰©å±•æ¨¡å‹å®¹é‡ï¼ˆæ›´å¤šå‚æ•°ï¼‰ã€å¤„ç†é«˜åˆ†è¾¨ç‡è¾“å…¥ ä»¥åŠ æé«˜è®­ç»ƒç¨³å®šæ€§ æ‰€åšçš„ä¸€ç³»åˆ—æ”¹è¿›ã€‚ åœ¨è§†è§‰ä»»åŠ¡ä¸­ï¼ŒTransformer æ¨¡å‹è‹¥è¦å˜å¾—æ›´å¼ºï¼ˆæ›´å¤šå‚æ•°ã€æ›´é«˜åˆ†è¾¨ç‡è¾“å…¥ã€æ›´å¤šå±‚æ•°ï¼‰å°±ä¼šé‡åˆ°å‡ ä¸ªæŒ‘æˆ˜ï¼š 1. è®­ç»ƒä¸ç¨³å®šï¼šéšç€æ¨¡å‹å˜æ·±ã€é€šé“å˜å®½ï¼Œå†…éƒ¨æ¿€æ´»çš„å¹…åº¦å¯èƒ½æ€¥å‰§å¢é•¿ï¼Œå¯¼è‡´æ¢¯åº¦ã€æ•°å€¼ä¸ç¨³å®šã€‚ 2. åˆ†è¾¨ç‡è¿ç§»é—®é¢˜ï¼šæ¨¡å‹åœ¨ä½åˆ†è¾¨ç‡ä¸‹é¢„è®­ç»ƒï¼ˆä¾‹å¦‚ 224Ã—224ï¼‰åï¼Œç”¨åœ¨é«˜åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚ 1,536Ã—1,536ï¼‰æˆ–ä¸åŒçª—å£å°ºå¯¸æ—¶è¡¨ç°ä¼šä¸‹é™ã€‚ 3. å¯¹æ ‡æ³¨æ•°æ®çš„è¿‡åº¦ä¾èµ–ï¼šå¤§æ¨¡å‹éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®æ‰èƒ½è®­ç»ƒå¾—å¥½ã€‚\nSwin V2 å°±æ˜¯ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæ”¯æŒè®­ç»ƒè¶…å¤§æ¨¡å‹ï¼ˆå¦‚ 30 äº¿å‚æ•°çº§åˆ«ï¼‰ï¼ŒåŒæ—¶èƒ½å¤„ç†å¤§å°ºå¯¸è¾“å…¥ \n\n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(self.norm1(x))\n        \n        shortcut = x \n        ... # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n        \n        ...\n        \n        return x \n\n\n\nclass WindowAttention:\n    def __init__(self,):\n    \n        ...\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n        ...\n    \n    \n    def forward(self, x):\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01))).exp()\n        attn = attn * logit_scale\n\n\n\nlog-spaced continuous position bias approach to address the issue in transferring across window resolutions\n\\[\n\\begin{split}\n\\widehat{\\Delta x} &= \\operatorname{sign}(x) \\cdot \\log(1 + |\\Delta x|) \\\\\n\\widehat{\\Delta y} &= \\operatorname{sign}(y) \\cdot \\log(1 + |\\Delta y|)\n\\end{split}\n\\]\nself.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                             nn.ReLU(inplace=True),\n                             nn.Linear(512, num_heads, bias=False))\n                             \ndef forward(self, x):\n    relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#è®­ç»ƒæŠ€å·§",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#è®­ç»ƒæŠ€å·§",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "5.2 è®­ç»ƒæŠ€å·§",
    "text": "5.2 è®­ç»ƒæŠ€å·§\n\nDropout Path\nGradient Checkpoint:\n\néœ€è¦å¯é‡ç°å‰å‘ â€¢ è¢« checkpoint çš„æ¨¡å—å¿…é¡»æ˜¯ çº¯å‡½æ•°ï¼Œå³è¾“å‡ºåªä¾èµ–è¾“å…¥ï¼Œä¸èƒ½ä¾èµ–éšæœºæ•°ã€å…¨å±€çŠ¶æ€ã€‚\n\n\nCite to CLIP",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/04-clip/CLIP.html",
    "href": "posts/04-clip/CLIP.html",
    "title": "04: Learning Transferable Visual Models From Natural Language Supervision(CLIP)",
    "section": "",
    "text": "1 CLIP\nThis is the link to the Vision-Transformer\nThis is another link to the Transformer\nTHis is link to the Swin-Transformer\n\n\n2 Summary\n\n\n3 Key Concepts\n\n\n4 Q & A\n\n\n5 æ‰©å±•\n\n\n\n\n Back to top",
    "crumbs": [
      "04 Clip",
      "04: Learning Transferable Visual Models From Natural Language Supervision(**CLIP**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/Transformer.html",
    "href": "posts/01-transformer/Transformer.html",
    "title": "Attention is All You Need(Transformer)",
    "section": "",
    "text": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations  Attention_is_all_you_need, p.2  THis\n\n\n\n\n Back to top",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html",
    "href": "00-how-to-read-paper.html",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "åœ¨æ·±å…¥å­¦ä¹ äººå·¥æ™ºèƒ½ç›¸å…³è®ºæ–‡ä¹‹å‰ï¼ŒæŒæ¡é«˜æ•ˆä¸”ç³»ç»Ÿçš„é˜…è¯»æ–¹æ³•è‡³å…³é‡è¦ã€‚è®ºæ–‡é˜…è¯»ç»å…¸æŒ‡å— How to Read a Paper ä¸­æå‡ºäº†â€œä¸‰éé˜…è¯»æ³•â€ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†æ¸…æ™°çš„å®è·µè·¯å¾„ï¼š\n\n\n\n\nç¬¬ä¸€éï¼šå¿«é€Ÿæµè§ˆï¼Œè·å–è®ºæ–‡çš„æ•´ä½“ç»“æ„å’Œæ ¸å¿ƒç»“è®ºã€‚èšç„¦äºæ ‡é¢˜ï¼ˆtitleï¼‰ã€æ‘˜è¦ï¼ˆabstractï¼‰ã€å¼•è¨€ï¼ˆintroductionï¼‰ä¸ç»“è®ºï¼ˆconclusionï¼‰ï¼Œä»å®è§‚ä¸Šäº†è§£æ–‡ç« çš„ç ”ç©¶æ–¹å‘ã€‚\nç¬¬äºŒéï¼šç»†è¯»è®ºæ–‡ï¼Œå…³æ³¨ç†è®ºæ¨å¯¼ã€å®éªŒè®¾ç½®ä¸å…³é”®å›¾è¡¨ã€‚é‡ç‚¹ç†è§£è®ºæ–‡æ‰€æå‡ºçš„æ–¹æ³•ã€å®éªŒè®¾è®¡ã€ç»“æœåˆ†æåŠå…¶æ”¯æ’‘é€»è¾‘ã€‚\nç¬¬ä¸‰éï¼šæ‰¹åˆ¤æ€§é˜…è¯»ï¼Œç³»ç»Ÿåˆ†æè®ºæ–‡çš„ä¼˜åŠ£ï¼Œæå‡ºå»ºè®¾æ€§é—®é¢˜ï¼Œåæ€è¯¥æ–¹æ³•æ˜¯å¦å…·æœ‰é€šç”¨æ€§æˆ–æ˜¯å¦èƒ½åº”ç”¨äºè‡ªèº«ç ”ç©¶ã€‚\n\n\n\nListingÂ 1: ä¸‰éé˜…è¯»æ³•\n\n\n\néœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œé˜…è¯»è®ºæ–‡æ˜¯ä¸€ä¸ªå¾ªç¯è¿­ä»£ã€é€æ­¥æ·±å…¥çš„è¿‡ç¨‹ã€‚åœ¨åå¤é˜…è¯»ä¸æ€è€ƒä¸­ï¼Œæˆ‘ä»¬ä¼šä¸æ–­ä¿®æ­£ç†è§£ã€åŠ æ·±è®¤çŸ¥ã€‚ä¸æ­¤åŒæ—¶ï¼Œé€šè¿‡ä¸ä»–äººäº¤æµã€å‚ä¸è®¨è®ºï¼Œæœ‰åŠ©äºæ‹“å®½è§†è§’ã€æ·±åŒ–æ€è€ƒã€‚ï¼ˆæœ¬é¡¹ç›®ç½‘ç«™ä¹Ÿå› æ­¤è€Œç”Ÿï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä»¬æä¾›ä¸€ä¸ªå…±åŒäº¤æµå­¦ä¹ çš„å¹³å°ï¼‰\nä¸ºæå‡é˜…è¯»æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ç²¾è¯»é˜¶æ®µï¼Œä½¿ç”¨é¢œè‰²æ ‡è®°ä¸åŒå†…å®¹ã€‚ä¾‹å¦‚åœ¨ Harvard CS197 AI Research Experiences çš„ Lecture 3 ä¸­ç»™å‡ºçš„ç­–ç•¥ï¼š\n\né»„è‰²ï¼šçªå‡ºè®ºæ–‡æ‰€è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜æˆ–æŒ‘æˆ˜ã€‚\nç»¿è‰²ï¼šæ ‡è®°è®ºæ–‡ä¸­çš„æåˆ°çš„é€šç”¨æ¦‚å¿µã€‚\nç²‰è‰²ï¼šå¯¹åº”æå‡ºçš„ç®—æ³•æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ã€‚\næ©™è‰²ï¼šç”¨äºæ ‡è®°è®ºæ–‡çš„åˆ›æ–°ç‚¹ä¸è´¡çŒ®ã€‚\n\nä¾‹å¦‚å¦‚ä¸‹ç¤ºæ„å›¾ï¼š\n\n\n\n\n\n\nFigureÂ 1: Highlight Example of paper (Attention is all you need)\n\n\n\nè¿™å¥—æ–¹æ³•å¹¶éå”¯ä¸€æ ‡å‡†ï¼Œå…³é”®åœ¨äºæ„å»ºä¸€å¥—é€‚åˆè‡ªèº«è®¤çŸ¥æ–¹å¼çš„å¯è§†åŒ–æ ‡è®°ä½“ç³»ã€‚ç»Ÿä¸€çš„æ ‡æ³¨é£æ ¼ï¼Œæœ‰åŠ©äºåœ¨åæœŸå›é¡¾æˆ–è·¨è®ºæ–‡æ¯”è¾ƒæ—¶é«˜æ•ˆå®šä½å…³é”®ä¿¡æ¯ã€‚æœ¬é¡¹ç›®æ‰€æ”¶å½•çš„æ¯ç¯‡è®ºæ–‡ç¬”è®°ä¹Ÿå°†é‡‡ç”¨è¿™ä¸€ç»“æ„åŒ–é«˜äº®æ ‡è®°æ–¹æ³•ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#è®ºæ–‡é˜…è¯»æŒ‡å—",
    "href": "00-how-to-read-paper.html#è®ºæ–‡é˜…è¯»æŒ‡å—",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "åœ¨æ·±å…¥å­¦ä¹ äººå·¥æ™ºèƒ½ç›¸å…³è®ºæ–‡ä¹‹å‰ï¼ŒæŒæ¡é«˜æ•ˆä¸”ç³»ç»Ÿçš„é˜…è¯»æ–¹æ³•è‡³å…³é‡è¦ã€‚è®ºæ–‡é˜…è¯»ç»å…¸æŒ‡å— How to Read a Paper ä¸­æå‡ºäº†â€œä¸‰éé˜…è¯»æ³•â€ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†æ¸…æ™°çš„å®è·µè·¯å¾„ï¼š\n\n\n\n\nç¬¬ä¸€éï¼šå¿«é€Ÿæµè§ˆï¼Œè·å–è®ºæ–‡çš„æ•´ä½“ç»“æ„å’Œæ ¸å¿ƒç»“è®ºã€‚èšç„¦äºæ ‡é¢˜ï¼ˆtitleï¼‰ã€æ‘˜è¦ï¼ˆabstractï¼‰ã€å¼•è¨€ï¼ˆintroductionï¼‰ä¸ç»“è®ºï¼ˆconclusionï¼‰ï¼Œä»å®è§‚ä¸Šäº†è§£æ–‡ç« çš„ç ”ç©¶æ–¹å‘ã€‚\nç¬¬äºŒéï¼šç»†è¯»è®ºæ–‡ï¼Œå…³æ³¨ç†è®ºæ¨å¯¼ã€å®éªŒè®¾ç½®ä¸å…³é”®å›¾è¡¨ã€‚é‡ç‚¹ç†è§£è®ºæ–‡æ‰€æå‡ºçš„æ–¹æ³•ã€å®éªŒè®¾è®¡ã€ç»“æœåˆ†æåŠå…¶æ”¯æ’‘é€»è¾‘ã€‚\nç¬¬ä¸‰éï¼šæ‰¹åˆ¤æ€§é˜…è¯»ï¼Œç³»ç»Ÿåˆ†æè®ºæ–‡çš„ä¼˜åŠ£ï¼Œæå‡ºå»ºè®¾æ€§é—®é¢˜ï¼Œåæ€è¯¥æ–¹æ³•æ˜¯å¦å…·æœ‰é€šç”¨æ€§æˆ–æ˜¯å¦èƒ½åº”ç”¨äºè‡ªèº«ç ”ç©¶ã€‚\n\n\n\nListingÂ 1: ä¸‰éé˜…è¯»æ³•\n\n\n\néœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œé˜…è¯»è®ºæ–‡æ˜¯ä¸€ä¸ªå¾ªç¯è¿­ä»£ã€é€æ­¥æ·±å…¥çš„è¿‡ç¨‹ã€‚åœ¨åå¤é˜…è¯»ä¸æ€è€ƒä¸­ï¼Œæˆ‘ä»¬ä¼šä¸æ–­ä¿®æ­£ç†è§£ã€åŠ æ·±è®¤çŸ¥ã€‚ä¸æ­¤åŒæ—¶ï¼Œé€šè¿‡ä¸ä»–äººäº¤æµã€å‚ä¸è®¨è®ºï¼Œæœ‰åŠ©äºæ‹“å®½è§†è§’ã€æ·±åŒ–æ€è€ƒã€‚ï¼ˆæœ¬é¡¹ç›®ç½‘ç«™ä¹Ÿå› æ­¤è€Œç”Ÿï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä»¬æä¾›ä¸€ä¸ªå…±åŒäº¤æµå­¦ä¹ çš„å¹³å°ï¼‰\nä¸ºæå‡é˜…è¯»æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ç²¾è¯»é˜¶æ®µï¼Œä½¿ç”¨é¢œè‰²æ ‡è®°ä¸åŒå†…å®¹ã€‚ä¾‹å¦‚åœ¨ Harvard CS197 AI Research Experiences çš„ Lecture 3 ä¸­ç»™å‡ºçš„ç­–ç•¥ï¼š\n\né»„è‰²ï¼šçªå‡ºè®ºæ–‡æ‰€è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜æˆ–æŒ‘æˆ˜ã€‚\nç»¿è‰²ï¼šæ ‡è®°è®ºæ–‡ä¸­çš„æåˆ°çš„é€šç”¨æ¦‚å¿µã€‚\nç²‰è‰²ï¼šå¯¹åº”æå‡ºçš„ç®—æ³•æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ã€‚\næ©™è‰²ï¼šç”¨äºæ ‡è®°è®ºæ–‡çš„åˆ›æ–°ç‚¹ä¸è´¡çŒ®ã€‚\n\nä¾‹å¦‚å¦‚ä¸‹ç¤ºæ„å›¾ï¼š\n\n\n\n\n\n\nFigureÂ 1: Highlight Example of paper (Attention is all you need)\n\n\n\nè¿™å¥—æ–¹æ³•å¹¶éå”¯ä¸€æ ‡å‡†ï¼Œå…³é”®åœ¨äºæ„å»ºä¸€å¥—é€‚åˆè‡ªèº«è®¤çŸ¥æ–¹å¼çš„å¯è§†åŒ–æ ‡è®°ä½“ç³»ã€‚ç»Ÿä¸€çš„æ ‡æ³¨é£æ ¼ï¼Œæœ‰åŠ©äºåœ¨åæœŸå›é¡¾æˆ–è·¨è®ºæ–‡æ¯”è¾ƒæ—¶é«˜æ•ˆå®šä½å…³é”®ä¿¡æ¯ã€‚æœ¬é¡¹ç›®æ‰€æ”¶å½•çš„æ¯ç¯‡è®ºæ–‡ç¬”è®°ä¹Ÿå°†é‡‡ç”¨è¿™ä¸€ç»“æ„åŒ–é«˜äº®æ ‡è®°æ–¹æ³•ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#å·¥å…·å‡†å¤‡",
    "href": "00-how-to-read-paper.html#å·¥å…·å‡†å¤‡",
    "title": "00: Preparation for Following",
    "section": "2 å·¥å…·å‡†å¤‡",
    "text": "2 å·¥å…·å‡†å¤‡\nç§‘å­¦é˜…è¯»ç¦»ä¸å¼€åˆé€‚çš„å·¥å…·æ”¯æ’‘ã€‚ä»¥ä¸‹æ˜¯æ¨èçš„å·¥å…·ä½“ç³»ï¼Œæ¶µç›–æ–‡çŒ®ç®¡ç†ã€ç¬”è®°æ•´ç†ã€ä»£ç æ‰§è¡Œç­‰å¤šä¸ªç»´åº¦ã€‚\n\n2.1 æ–‡çŒ®ç®¡ç†ï¼šZotero\néšç€è®ºæ–‡ç§¯ç´¯çš„å¢å¤šï¼Œç³»ç»Ÿçš„æ–‡çŒ®ç®¡ç†å·¥å…·ä¸å¯æˆ–ç¼ºã€‚Zotero æ˜¯ä¸€æ¬¾å…è´¹ä¸”å¼€æºçš„æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œæ”¯æŒè‡ªåŠ¨å¯¼å…¥ã€åˆ†ç»„ç®¡ç†ä¸å¤šæ ¼å¼å¼•ç”¨ï¼ˆå¦‚ BibTeXï¼‰ã€‚å…¶å¯æ‰©å±•æ€§æå¼ºï¼Œæ”¯æŒæ’ä»¶ä¸ä¸»é¢˜å®šåˆ¶ã€‚\n\n\n\n\n\n\nFigureÂ 2: Example of Zotero\n\n\n\næ¨èæ’ä»¶ï¼š\n\nBetter BibTexï¼šå¢å¼º BibTeX å¯¼å‡ºåŠŸèƒ½ï¼Œä¾¿äºä¸ LaTeX æ— ç¼é›†æˆã€‚\nEthereal Styleï¼šä¸º Zotero æä¾›ç¾è§‚çš„ UI é£æ ¼ï¼Œæå‡ä½¿ç”¨ä½“éªŒã€‚\n\nå°½ç®¡ Zotero å­˜åœ¨ä¸€å®šå­¦ä¹ æ›²çº¿ï¼Œä½†å…¶é•¿æœŸä»·å€¼è¿œè¶…åˆæœŸæŠ•å…¥ã€‚è‹¥ä»…å¸Œæœ›ä¸´æ—¶é˜…è¯»ï¼ŒPDF é˜…è¯»å™¨äº¦å¯ï¼›ä½†ä»ç§‘ç ”è§†è§’å‡ºå‘ï¼Œå»ºè®®å°½æ—©æŠ•å…¥å­¦ä¹ ä¸ä½¿ç”¨ã€‚\næ­¤å¤–ï¼ŒZotero Chrome Connector æ’ä»¶å¯å®ç°ä¸€é”®å¯¼å…¥ç½‘é¡µæ–‡çŒ®ï¼Œæå¤§æå‡æ–‡çŒ®æ”¶é›†æ•ˆç‡ï¼š\n\n\n\n\n\n\nFigureÂ 3: Zotero Chrome Connector\n\n\n\nå¦‚ FigureÂ 3 æ‰€ç¤ºï¼Œåªéœ€ç‚¹å‡»æ’ä»¶æŒ‰é’®ï¼Œå³å¯å°†å½“å‰ç½‘é¡µå†…å®¹å¯¼å…¥è‡³æ–‡çŒ®åº“ã€‚\n\n\n2.2 ç¬”è®°è®°å½•ï¼šObsidian\nObsidian æ˜¯ä¸€æ¬¾åŸºäº Markdown çš„ç¬”è®°ç³»ç»Ÿï¼Œæ”¯æŒåŒå‘é“¾æ¥ä¸å›¾è°±è§†å›¾ï¼Œç‰¹åˆ«é€‚åˆç”¨äºæ„å»ºä¸ªäººçŸ¥è¯†ä½“ç³»ã€‚\n\n\n\n\n\n\nFigureÂ 4: Obsidian Example\n\n\n\næ¨èæ’ä»¶ï¼š\n\nobsidian-latex-suiteï¼šæä¾› LaTeX å¿«æ·è¾“å…¥ä¸å…¬å¼é¢„è§ˆåŠŸèƒ½ï¼Œæ˜¾è‘—æé«˜æ•°å­¦è¡¨è¾¾æ•ˆç‡ã€‚\nHighlightr Pluginï¼šæ”¯æŒè‡ªå®šä¹‰é«˜äº®é¢œè‰²ï¼Œä¾¿äºåˆ†ç±»ä¿¡æ¯æ ‡æ³¨ã€‚\n\n \néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿‡åº¦ç¾åŒ–ç•Œé¢æˆ–æ’ä»¶å †å å¯èƒ½åè€Œåˆ†æ•£æ³¨æ„åŠ›ã€‚å»ºè®®ä»¥â€œç»“æ„æ¸…æ™°ã€å†…å®¹ä¸ºæœ¬â€ä¸ºé¦–è¦åŸåˆ™ã€‚\nå¯¹äºä¸ä½¿ç”¨ Obsidian çš„ç”¨æˆ·ï¼Œä¹Ÿå¯é€‰æ‹©ï¼š\n\n\n\n\n\n\n\n\n\n\n\n(a) Notion Home Page\n\n\n\n\n\n\n\n\n\n\n\n(b) FeiShu Home Page\n\n\n\n\n\n\n\nFigureÂ 5: Home Page of Notion and FeiShu\n\n\n\n\nNotionï¼šå¦‚ FigureÂ 5 (a) æ‰€ç¤ºï¼Œé€‚åˆå¤šäººåä½œä¸å¯è§†åŒ–ç¼–è¾‘ã€‚\né£ä¹¦ï¼šå¦‚ FigureÂ 5 (b) æ‰€ç¤ºï¼ŒåŠŸèƒ½å…¨é¢ï¼Œé€‚åˆä¼ä¸šçº§æ–‡æ¡£ç®¡ç†ã€‚\n\n\n\n2.3 ä»£ç æ‰§è¡Œï¼šJupyter Notebook\nåœ¨â€œPaper with Codeâ€ç†å¿µä¸‹ï¼Œæ¯ç¯‡è®ºæ–‡å°†é…å¥— Jupyter Notebook å®ç°æ ¸å¿ƒç®—æ³•ã€‚å…¶äº¤äº’å¼æ–‡æ¡£ç‰¹æ€§ï¼Œä½¿å…¶æˆä¸ºå­¦ä¹ ä¸éªŒè¯ä»£ç çš„ç†æƒ³å¹³å°ã€‚\n\n\n\n\n\n\nNote\n\n\n\nè‹¥å¯¹ Jupyter Notebook ä¸ç†Ÿæ‚‰ï¼Œæ¨èå‚è€ƒ å®˜æ–¹æ–‡æ¡£ï¼Œä»¥å¿«é€Ÿå…¥é—¨ã€‚\n\n\nç›¸åº”çš„ä»£ç ï¼Œæˆ‘ä¼šæ”¾åœ¨GitHubçš„ä»“åº“ä¸­\n\n\n\n\n\n\nFigureÂ 6: The preview of GitHub Page\n\n\n\n\n\n2.4 GPU å¹³å°ï¼šäº‘ç«¯æ‰§è¡Œç¯å¢ƒ\næ·±åº¦å­¦ä¹ æ¨¡å‹å¸¸éœ€ GPU åŠ é€Ÿï¼Œè‹¥æœ¬åœ°æ—  GPU å¯ä½¿ç”¨ä»¥ä¸‹å¹³å°ï¼š\n\nGoogle Colabï¼šGoogle æä¾›çš„å…è´¹äº‘ç«¯ Notebook å¹³å°ï¼Œæ”¯æŒ GPU ä¸ TPUã€‚\nKaggle Kernelsï¼šæ”¯æŒ GPU çš„æ•°æ®ç§‘å­¦å¹³å°ï¼Œé€‚åˆå¿«é€Ÿå®éªŒã€‚\n\nå›½å†…å¯é€‰å¹³å°ï¼š\n\nAutoDLï¼šé€‚åˆå›½å†…ç”¨æˆ·ï¼Œé…ç½®ç®€å•ï¼Œæ”¯æŒå®šåˆ¶åŒ–éƒ¨ç½²ã€‚\n\nå…¶ä»–æ¨èï¼š\n\nRunPodã€Lambda Labsï¼šæä¾›ç¨³å®šã€ä½å»¶è¿Ÿçš„ GPU è®­ç»ƒç¯å¢ƒï¼Œé€‚åˆä¸­å¤§å‹å®éªŒä»»åŠ¡ã€‚\n\n\né€šè¿‡åˆç†é…ç½®ä¸Šè¿°å·¥å…·ï¼Œå¯ä»¥æ„å»ºå‡ºä¸€ä¸ªç³»ç»ŸåŒ–ã€é«˜æ•ˆçš„è®ºæ–‡å­¦ä¹ ä¸ç ”ç©¶æµç¨‹ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæ¯ç¯‡è®ºæ–‡å°†é™„å¸¦ä»£ç å®ç°ã€ç»“æ„è§£æä¸æ‰¹åˆ¤æ€§æ€è€ƒï¼Œæ¬¢è¿å…±åŒå­¦ä¹ äº¤æµã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#æ€»ç»“",
    "href": "00-how-to-read-paper.html#æ€»ç»“",
    "title": "00: Preparation for Following",
    "section": "3 æ€»ç»“",
    "text": "3 æ€»ç»“\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é«˜æ•ˆé˜…è¯»è®ºæ–‡çš„æ–¹æ³•è®ºä¸å·¥å…·ä½“ç³»ã€‚é€šè¿‡â€œä¸‰éé˜…è¯»æ³•â€ ListingÂ 1ï¼Œ æˆ‘ä»¬å¯ä»¥ç³»ç»Ÿåœ°ç†è§£è®ºæ–‡å†…å®¹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ‰¹åˆ¤æ€§æ€è€ƒã€‚åŒæ—¶ï¼Œå€ŸåŠ© Zotero SectionÂ 2.1ã€ObsidianSectionÂ 2.2 ç­‰å·¥å…·ï¼Œå¯ä»¥æœ‰æ•ˆç®¡ç†æ–‡çŒ®ã€è®°å½•ç¬”è®°ä¸æ‰§è¡Œä»£ç ã€‚ åœ¨åç»­ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åº”ç”¨è¿™äº›æ–¹æ³•ä¸å·¥å…·ï¼Œæ·±å…¥åˆ†ææ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ã€å®éªŒè®¾è®¡ä¸åˆ›æ–°è´¡çŒ®ã€‚å¸Œæœ›é€šè¿‡æœ¬é¡¹ç›®çš„å­¦ä¹ ï¼Œèƒ½å¤Ÿå¸®åŠ©å¤§å®¶æ›´å¥½åœ°æŒæ¡äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‰æ²¿ç ”ç©¶åŠ¨æ€ï¼Œå¹¶åœ¨å®è·µä¸­ä¸æ–­æå‡è‡ªå·±çš„ç§‘ç ”èƒ½åŠ›ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html",
    "href": "posts/03-swin-transformer/Swin-Transformer.html",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "åœ¨é˜…è¯»Swin Transformer è¿™ç¯‡æ–‡ç« ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ Vision-Transformer è®²çš„æ˜¯ä»€ä¹ˆï¼š ViT åœ¨å¤„ç†å›¾åƒæ—¶ï¼Œä¼šå°†æ•´å¼ å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„patchï¼Œå¹¶è¿›è¡Œglobal self-attentionçš„è®¡ç®—ï¼Œä»è€Œæ•æ‰å›¾åƒä¸­çš„å…¨å±€ä¿¡æ¯ã€‚ ç„¶è€Œï¼Œ è¿™ç§æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªæ ¸å¿ƒçš„é—®é¢˜ï¼š\n\nè®¡ç®—å¤æ‚åº¦é«˜: ViTçš„è®¡ç®—å¤æ‚åº¦ä¸º \\(\\mathcal{O}(( \\frac{HW}{P^2})^2)\\)ï¼Œå…¶ä¸­\\(H\\)å’Œ\\(W\\)åˆ†åˆ«æ˜¯å›¾åƒçš„é«˜åº¦å’Œå®½åº¦ï¼Œè€Œ\\(P\\)æ˜¯patchçš„å¤§å°ã€‚å¯¹é«˜åˆ†è¾¨ç‡å›¾åƒï¼ˆHigh Resolution)ï¼ˆå¦‚æ£€æµ‹æˆ–åˆ†å‰²ä»»åŠ¡ï¼‰æ¥è¯´ï¼Œtoken æ•°é‡å·¨å¤§ï¼Œè®¡ç®—å’Œæ˜¾å­˜å¼€é”€éš¾ä»¥æ‰¿å—\nç¼ºä¹å±€éƒ¨ç‰¹å¾å»ºæ¨¡: ViTåœ¨è¿›è¡Œå…¨å±€è‡ªæ³¨æ„åŠ›è®¡ç®—æ—¶ï¼Œå¯èƒ½ä¼šå¿½ç•¥å›¾åƒä¸­çš„å±€éƒ¨ç‰¹å¾\næ²¡æœ‰é‡‘å­—å¡”å¼å±‚çº§ç»“æ„: CNN çš„å±‚çº§ç»“æ„ï¼ˆä»ä½å±‚å±€éƒ¨ç‰¹å¾åˆ°é«˜å±‚è¯­ä¹‰ç‰¹å¾ï¼‰éå¸¸é€‚åˆå¤„ç†å¤šå°ºåº¦ç›®æ ‡, ViT ç›´æ¥ç”¨å›ºå®šå¤§å° patch flatten æˆåºåˆ—ï¼Œç¼ºä¹å±‚æ¬¡è¡¨ç¤ºï¼Œéš¾ä»¥é€‚åº”å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ï¼‰ã€‚\n\n\nWe observe that significant challenges in transferring its high performance in the language domain to the visual domain can be explained by differences between the two modalities. One of these differences involves scale â€¦ visual elements can vary substantially in scale â€¦ Another difference is the much higher resolution of pixels in images compared to words in passages of text â€¦ as the computational complexity of its self-attention is quadratic to image size.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.2 \n\nä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œ Swin Transformer (Liu, Lin, et al. 2021) æå‡ºä¸€ç§æ–°çš„åŸºäº Vision Transformerçš„æ¶æ„å®ƒé€šè¿‡å¼•å…¥å±‚æ¬¡åŒ–çš„ç‰¹å¾è¡¨ç¤º(Hierarchical Architecture) å’Œ ç§»åŠ¨çª—å£æœºåˆ¶(Shifted Window MSA, SW-MSA)ï¼Œæ¥æœ‰æ•ˆåœ°æ•æ‰å›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œ å¹¶ä¸”é€šè¿‡å±€éƒ¨çª—å£(Window-based Multi-head Self Attention, W-MSA) æ³¨æ„åŠ›ï¼Œæ¥é™ä½è®­ç»ƒçš„æ—¶é—´å¤æ‚åº¦ã€‚\n\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥è¯¦ç»†ä»‹ç» Swin Transformer çš„æ¶æ„å’Œå…³é”®æŠ€æœ¯ã€‚æˆ‘ä»¬é¦–å…ˆæ¥çœ‹Swin Transformer çš„Attentionçš„å®ç°ã€‚\n\n\nW-MHA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š\n\næŠŠå›¾åƒåˆ’åˆ†æˆå›ºå®šå¤§å°çš„çª—å£ï¼ˆwindowï¼‰ï¼Œæ¯”å¦‚ 7Ã—7 patch çš„çª—å£ã€‚\nåœ¨çª—å£å†…çš„ token ä¹‹é—´åšå±€éƒ¨è‡ªæ³¨æ„åŠ›ï¼Œè€Œä¸æ˜¯åœ¨æ•´å¼ å›¾åƒçš„æ‰€æœ‰ token ä¹‹é—´åšå…¨å±€æ³¨æ„åŠ›ã€‚\næ¯ä¸ªçª—å£ç‹¬ç«‹è®¡ç®— Multi-Head Attention â†’ é™ä½è®¡ç®—é‡ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥å¹¶è¡Œçš„è®¡ç®—\n\nè¿™æ ·ä¸€æ¥ï¼š\n\nå•ä¸ªçª—å£ token æ•°é‡å›ºå®š = \\(M^{2}\\)ï¼ˆå¦‚ 7Ã—7=49ï¼‰ã€‚\næ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ä» \\(\\mathcal{O}((hw)^{2}C)\\) é™ä½ä¸º \\(\\mathcal{O}(M^{2}hwC)\\)ï¼Œå…¶ä¸­ \\(M \\ll \\sqrt{ N }\\)ã€‚\n\né™¤äº†é™ä½è®¡ç®—å¤æ‚åº¦ä¹‹å¤–ï¼ŒW-MHAï¼Œè¿˜æœ‰ä¿ç•™CNN åœ¨å›¾åƒå¤„ç†ä¸­å¼ºå¤§çš„ä¸€ç‚¹æ˜¯ å±€éƒ¨æ„Ÿå—é‡ å’Œ å¹³ç§»ä¸å˜æ€§ã€‚\n\nW-MHA é€šè¿‡çª—å£é™åˆ¶ï¼Œä½¿å¾—æ³¨æ„åŠ›æœºåˆ¶ä¹Ÿå…·å¤‡ç±»ä¼¼çš„å±€éƒ¨å½’çº³åç½®ï¼ˆinductive biasï¼‰ï¼Œé€‚åˆå›¾åƒå»ºæ¨¡ã€‚\n\n\nFor efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n\n\n\nW-MHA å¾ˆå¥½ï¼Œä½†æ˜¯å®ƒå­˜åœ¨çš„ä¸€ä¸ªé—®é¢˜å°±æ˜¯ï¼š\n\nçª—å£ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œç¼ºå°‘è·¨çª—å£çš„ä¿¡æ¯äº¤æµã€‚è¿™ä¼šå¯¼è‡´ï¼Œæ¨¡å‹åªèƒ½çœ‹è§å±€éƒ¨ï¼Œä¸èƒ½è·å¾—å…¨å±€çš„ä¿¡æ¯ã€‚\n\n\nThe window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSwin- Transformeræå‡ºæ¥ Shifted Window Mulit-Head-Attention (SW-MHA) çª—å£ä½ç½®ç›¸å¯¹å‰ä¸€å±‚å¹³ç§»ï¼Œæ¯”å¦‚ 7Ã—7 çª—å£ â†’ å¹³ç§» 3 ä¸ª patchã€‚ è¿™æ ·ï¼Œæ–°çš„çª—å£ä¼šè·¨è¶ŠåŸæ¥çš„è¾¹ç•Œï¼Œtoken ä¼šå’Œç›¸é‚»çª—å£çš„ token ä¸€èµ·è®¡ç®—æ³¨æ„åŠ›ã€‚ ç›¸å½“äºå¼ºåˆ¶è·¨çª—å£äº¤äº’ï¼Œè®©ä¿¡æ¯å¯ä»¥åœ¨ä¸åŒåŒºåŸŸä¹‹é—´æµåŠ¨ã€‚\n å¦‚ä¸Šå¦‚æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†Windowé€šè¿‡å‘å·¦ä¸Šè§’ç§»åŠ¨ï¼Œé€šè¿‡ç»™å›¾ç‰‡å¢åŠ Paddingæ¥ï¼Œä½†æ˜¯è¿™ç§åŠæ³•æ˜¾ç„¶ä¼šå¢åŠ è®¡ç®—çš„å¤æ‚åº¦ã€‚Swin Transformerç”¨äº†ä¸€ç§å¾ˆèªæ˜çš„åŠæ³•ï¼Œå«åš Cycling Shiftï¼Œè¿™ç§æ–¹æ³•å°±æ˜¯å°†å°†ä¸€ä¸ªå¼ é‡æˆ–å›¾åƒåœ¨æŸä¸ªç»´åº¦ä¸Šåš å¹³ç§»ï¼Œä½†ä¸æ˜¯æŠŠç§»å‡ºå»çš„éƒ¨åˆ†ä¸¢æ‰ï¼Œè€Œæ˜¯ é‡æ–°ä»å¦ä¸€è¾¹è¡¥å›æ¥ã€‚å°±åƒâ€œç¯å½¢é˜Ÿåˆ—â€æˆ–â€œé’Ÿè¡¨èµ°ä¸€åœˆåˆå›åˆ°èµ·ç‚¹â€ã€‚ å¦‚ä¸‹å›¾æ‰€ç¤º \nå¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡Cycling Shiftï¼Œæˆ‘ä»¬å¾—åˆ°çš„æ¯ä¸ªwindowçš„å†…å®¹ï¼Œå’Œä¹‹å‰æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯æ‰€éœ€è¦çš„Windowçš„æ•°é‡ï¼Œå°äº†å¾ˆå¤šï¼Œè¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œæ‰€éœ€è¦çš„æ—¶é—´å¤æ‚åº¦ï¼Œä¹Ÿå°äº†å¾ˆå¤šã€‚\n\nä¸è¿‡Cycling Shiftä¹Ÿæœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯åŒä¸€ä¸ªçª—å£é‡Œé¢ï¼Œå¯èƒ½æœ‰æ¥è‡ªä¸åŒå›¾ç‰‡çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åœ¨åŸå›¾ç‰‡ä¸Šä¸æ˜¯ç›¸é‚»çš„ï¼Œè‡ªç„¶ä¸åº”è¯¥ç›¸äº’äº¤æµä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥å°†å›¾ç‰‡ï¼ŒæŠ½è±¡æˆä¸‹å›¾çš„å½¢å¼ã€‚ç»„ç»‡Attentionäº¤æµï¼Œå¾ˆè‡ªç„¶çš„ä¸€ç§æ–¹æ³•æ˜¯åˆ©ç”¨Maskï¼Œå°±åƒTransformeré‡Œçš„Causal Maskä¸€æ ·ã€‚ä½†æ˜¯ï¼Œè¿™ä¸ªMaské•¿ä»€ä¹ˆæ ·å­å‘¢\n\næˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹Maskï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ‰é¢œè‰²çš„åŒºåŸŸè¡¨ç¤ºMask == 1ï¼Œ åœ¨æ­¤ä¸ºäº†æ›´å¥½çš„\n\n\n\n\n\nSwin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n\n\\[\n\\begin{split}\n\\hat{z}^l &= \\text{W-MSA} \\left( \\text{LN} \\left( z^{l-1} \\right) \\right) + z^{l-1} \\\\\nz^l &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^l \\right) \\right) + \\hat{z}^l \\\\\n\\hat{z}^{l+1} &= \\text{SW-MSA} \\left( \\text{LN} \\left( z^l \\right) \\right) + z^l \\\\\nz^{l+1} &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^{l+1} \\right) \\right) + \\hat{z}^{l+1}\n\\end{split}\n\\]\nå°†W-MSA å’Œ SW-MSAå åœ¨ä¸€èµ·ï¼Œå°±å¾—åˆ°äº†Transformer Blockï¼Œå½“ç„¶ï¼Œè¿˜æœ‰ä¸€ä¸ªMLPï¼ŒLayer Normalizationï¼Œåœ¨æ­¤å°±ä¸èµ˜è¿°äº†ã€‚\n\n\n\nè®²å®Œäº†W-MHAï¼Œå’ŒSW-MHAï¼Œæˆ‘ä»¬å°±ç†è§£äº†Swin- Transformerä¸­æœ€éš¾ç†è§£ï¼Œä¹Ÿæ˜¯æœ€ç»ˆçš„éƒ¨åˆ†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹å…¶ä»–ç®€å•çš„éƒ¨åˆ†ã€‚ Patch Merge , å›¾ä¸­ç»¿è‰²çš„éƒ¨åˆ†ï¼Œé€æ­¥é™ä½ token æ•°é‡ï¼ˆé™é‡‡æ ·ï¼‰ï¼ŒåŒæ—¶å¢åŠ ç‰¹å¾ç»´åº¦çš„æ“ä½œã€‚è¿™ç±»ä¼¼äºCNNä¸­çš„æ“ä½œï¼Œéšç€å±‚æ•°çš„å¢åŠ ï¼Œåˆ†è¾¨ç‡é€æ­¥é™ä½ã€é€šé“æ•°é€æ­¥å¢åŠ ï¼Œè¿™æ ·æ—¢å‡å°‘äº†è®¡ç®—é‡ï¼Œåˆèƒ½æå–å±‚çº§ç‰¹å¾ã€‚å…·ä½“çš„å®ç°ï¼š\n\nåˆ†ç»„ï¼šå°†ç›¸é‚»çš„ 2Ã—2 patch åˆå¹¶æˆä¸€ä¸ªæ–°çš„ patchã€‚\n\nå‡è®¾è¾“å…¥ç‰¹å¾å¤§å°ä¸º (H, W, C)ã€‚\næ¯ 2Ã—2 çš„ patch â†’ åˆå¹¶ä¸º 1 ä¸ªæ–° tokenã€‚\næ–°ç‰¹å¾å›¾å¤§å°å˜ä¸º (H/2, W/2, 4C)ã€‚\n\nçº¿æ€§å˜æ¢:\n\nå°†åˆå¹¶åçš„ 4C ç»´ç‰¹å¾é€šè¿‡ä¸€ä¸ª çº¿æ€§å±‚ (Linear Projection)ï¼Œé™åˆ° 2C ç»´ã€‚\nè¾“å‡ºç»´åº¦ç¿»å€ï¼ˆ2Cï¼‰ï¼Œä»¥è¡¥å¿åˆ†è¾¨ç‡å‡åŠå¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚ ğŸ”¹ ä¸ºä»€ä¹ˆæå‡º Patch Merging\n\n\nåˆ†å±‚è¡¨ç¤º (Hierarchical Representation) â€¢ æ¨¡ä»¿ CNN çš„é‡‘å­—å¡”ç»“æ„ï¼Œä»å±€éƒ¨ç»†èŠ‚é€æ­¥èšåˆåˆ°å…¨å±€è¯­ä¹‰ã€‚ â€¢ æœ‰åˆ©äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆæ£€æµ‹ã€åˆ†å‰²ï¼‰ä¸­ä¸åŒå°ºåº¦çš„ç›®æ ‡å»ºæ¨¡ã€‚\nè®¡ç®—æ•ˆç‡ â€¢ token æ•°é‡é€å±‚å‡å°‘ â†’ Attention çš„å¤æ‚åº¦å¤§å¹…ä¸‹é™ã€‚ â€¢ ä¿è¯æ¨¡å‹å¯æ‰©å±•åˆ°å¤§åˆ†è¾¨ç‡å›¾åƒã€‚\nè¯­ä¹‰ä¿¡æ¯èšåˆ â€¢ é€šè¿‡åˆå¹¶ç›¸é‚» patchï¼Œæ¨¡å‹èƒ½æŠŠæ›´å¤§æ„Ÿå—é‡çš„ä¿¡æ¯æ•´åˆåˆ°æ–°çš„ token ä¸­ã€‚\n\n\nx = x.view(B, H, W, C)\n\nx0 = x[:, 0::2, 0::2, :]  # (B, H/2, W/2, C)\nx1 = x[:, 1::2, 0::2, :]  # (B, H/2, W/2, C)\nx2 = x[:, 0::2, 1::2, :]  # (B, H/2, W/2, C)\nx3 = x[:, 1::2, 1::2, :]  # (B, H/2, W/2, C)\n\nx = torch.cat([x0, x1, x2, x3], -1)  # (B, H/2, W/2, 4*C)\nx = x.view(B, -1, 4 * C)  # (B, H/2*W/2, 4*C)\n\nx = self.reduction(x)  # (B, H/2*W/2, 2*C)\n\n\n\nä¸Transformer å’Œ Vision-Transformer ä¸­ä¸åŒçš„æ˜¯ï¼ŒSwin Transformeråˆ©ç”¨çš„æ˜¯Relative Position Encodingã€‚\n\\[\n\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^{T}}{\\sqrt{ d }} +B\\right) V\n\\]\n1.  å®šä¹‰åç½®è¡¨ (relative_position_bias_table)\nâ€¢   å¤§å°æ˜¯ (2*Wh-1) * (2*Ww-1, num_heads)\nâ€¢   æ„å‘³ç€çª—å£å†…çš„ä»»æ„ä¸¤ä¸ª token çš„ç›¸å¯¹ä½ç½® (dx, dy)ï¼Œéƒ½æœ‰ä¸€ä¸ªå¯å­¦ä¹ çš„åç½®å€¼ï¼ˆæ¯ä¸ª head ä¸€ä»½ï¼‰ã€‚\nâ€¢   ä¾‹å¦‚çª—å£æ˜¯ 7Ã—7 â†’ ç›¸å¯¹ä½ç½®èŒƒå›´æ˜¯ [-6,6]ï¼Œæ‰€ä»¥è¡¨å¤§å°æ˜¯ 13Ã—13=169ï¼Œæ¯ä¸ªä½ç½®å­˜ä¸€ç»„åç½®\n\n\n2.  è®¡ç®—ç›¸å¯¹ä½ç½®ç´¢å¼• (relative_position_index)\nâ€¢   é¦–å…ˆç”Ÿæˆçª—å£å†…æ¯ä¸ª token çš„åæ ‡ã€‚\nâ€¢   ç„¶ååšå·®ï¼Œå¾—åˆ°ä»»æ„ä¸¤ä¸ª token çš„ç›¸å¯¹åæ ‡ (dx, dy)ã€‚\nâ€¢   å†æ˜ å°„æˆè¡¨çš„ç´¢å¼•ï¼ˆé€šè¿‡ç§»ä½å’Œå“ˆå¸Œæˆä¸€ä¸ªæ•´æ•° indexï¼‰ã€‚\nâ€¢   ç»“æœæ˜¯ä¸€ä¸ª (Wh*Ww, Wh*Ww) çš„çŸ©é˜µï¼Œæ¯ä¸ªå…ƒç´ å­˜ä¸¤ä¸ª token ä¹‹é—´åœ¨ bias è¡¨é‡Œçš„ç´¢å¼•ã€‚\n\n\nâ€¢   åœ¨å›¾åƒé‡Œï¼Œç›¸å¯¹ä½ç½®æ¯”ç»å¯¹ä½ç½®æ›´é‡è¦ï¼š\nâ€¢   æ¯”å¦‚ä¸€ä¸ªåƒç´ çš„å·¦é‚»å’Œå³é‚»å¾ˆç›¸ä¼¼ï¼Œæ— è®ºè¿™ä¸ªåƒç´ åœ¨å›¾åƒçš„å“ªä¸ªåœ°æ–¹ã€‚\n\n\nå’Œ Vision-Transformer ä¸€æ ·ï¼Œå½“è¾“å…¥çš„å›¾ç‰‡å’Œè®­ç»ƒæ—¶ä¸ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ bi-cubic interpolation æ¥å¢å¤§Relative Position\n\nThe learnt relative position bias in pre-training can be also used to initialize a model for fine-tuning with a different window size through bi-cubic interpolation  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\n\n\n\né™¤äº†ä»¥ä¸Šå‡ ä¸ªï¼ŒSwin Transformer ä¸­è¿˜æœ‰å…¶ä»–Componentï¼Œæ¯”å¦‚ ï¼š\n\nPatch Embedding\nLinear Projection\nFeedForward\nLayer Normalization åœ¨æ­¤ï¼Œå°±ä¸èµ˜è¿°äº†ï¼Œæœ‰éœ€è¦çš„åŒå­¦ï¼Œè¯·å‚è€ƒå‰ä¸€ç¯‡ Vision-Transformerï¼Œ æˆ–è€… Transformer\n\n\n\n\nå½“ä¸€åœºå›¾ç‰‡ä¼ å…¥Swin Transformerï¼Œ å®ƒå¯ä»¥æå–å‡ºå›¾ç‰‡çš„ç‰¹å¾ã€‚ \\[\n\\mathrm{z} = f_{\\theta}(\\mathrm{x}), \\quad \\text{where}\\ \\mathrm{x} \\in \\mathbb{R}^{3\\times H \\times W}, \\mathrm{z} \\in \\mathbb{R}^{H'W' \\times C}\n\\]\nä¸€å¼ å›¾ç‰‡è½¬åŒ–æˆäº† \\(H'W'\\) ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„å¤§å°ä¸º $Cã€‚\nSwin Transformer å¯ä»¥æœ‰å½“ä½œåŸºæœ¬çš„backboneï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ä¸‹æ¸¸è¿›è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š\n\nImage Classification\nObject Detection\nSemantic segmentation\n\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¦‚ä½•ç”¨Swin Transformeråœ¨ä¸åŒçš„ä»»åŠ¡ä¸­\n\n\n\nå¯¹äº \\(\\mathrm{z}\\) çš„ hidden statesï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€ä¸ªAverage Poolingï¼Œå¯¹äºæ¯ä¸€ä¸ªç‰¹å¾æ±‚å‡å€¼ï¼Œç„¶åå†å°†è¿™ä¸ªä¼ å…¥ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬Classificationäº†ã€‚ä¸ Vision-Transformer ä¸åŒçš„æ˜¯ï¼ŒSwin Transformer æ²¡æœ‰ [CLS] token æ¥å½“æ”¶é›†å…¨éƒ¨çš„ä¿¡æ¯ã€‚\n\n\n\n\nBackbone (Swin Transformer)ï¼š\n\nStage 1: [N, C1, H/4, W/4]\nStage 2: [N, C2, H/8, W/8]\nStage 3: [N, C3, H/16, W/16]\nStage 4: [N, C4, H/32, W/32]\n\nå¯ä»¥å¾—åˆ° FPN(Lin et al. 2017) to create a pyramid of feature maps suitable for detection.\n{P2: [N, C, H/4, W/4], \n P3: [N, C, H/8, W/8], \n P4: [N, C, H/16, W/16], \n P5: [N, C, H/32, W/32]}\næœ‰äº†è¿™äº›FPN ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ç»“åˆä¸åŒçš„ç®—æ³•ï¼Œæ¥è¿›è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œæ¯”å¦‚\nä¾‹å­ 1ï¼šç›®æ ‡æ£€æµ‹ (Object Detection)\nä»¥ Swin Transformer + Faster R-CNN (Ren et al. 2016) ä¸ºä¾‹ï¼š 1. è¾“å…¥å›¾åƒï¼šä¸€å¼  800Ã—1333 çš„ COCO æ•°æ®é›†å›¾ç‰‡ã€‚\n3.  FPN (ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ)ï¼šå°†å¤šå°ºåº¦ç‰¹å¾èåˆï¼Œå½¢æˆç»Ÿä¸€çš„é‡‘å­—å¡”ç‰¹å¾ã€‚\n4.  RPN (Region Proposal Network)ï¼šåœ¨ç‰¹å¾å›¾ä¸Šç”Ÿæˆå€™é€‰åŒºåŸŸã€‚\n5.  RoI Headï¼šå¯¹å€™é€‰åŒºåŸŸè¿›è¡Œåˆ†ç±» (è½¦ã€äººã€ç‹—â€¦) å’Œè¾¹æ¡†å›å½’ã€‚\n6.  è¾“å‡ºï¼šé¢„æµ‹ç»“æœï¼Œä¾‹å¦‚ï¼š\nâ€¢   â€œä¸€è¾†è½¦â€ â†’ è¾¹æ¡† (x1,y1,x2,y2) + ç±»åˆ« â€œcarâ€\nâ€¢   â€œä¸€ä¸ªäººâ€ â†’ è¾¹æ¡† + ç±»åˆ« â€œpersonâ€\n ğŸ‘‰ åœ¨ COCO æ•°æ®é›†ä¸Šï¼ŒSwin-T + Faster R-CNNæ¯” ResNet-50 + Faster R-CNN çš„ mAP æé«˜çº¦ 5~6 ä¸ªç‚¹ã€‚\nè¯­ä¹‰åˆ†å‰² (Semantic Segmentation)  ä»¥ Swin Transformer + UPerNet(Xiao et al. 2018)ä¸ºä¾‹ï¼š 1. è¾“å…¥å›¾åƒï¼šä¸€å¼  512Ã—512 çš„ ADE20K æ•°æ®é›†å›¾ç‰‡ã€‚ 2. Backbone (Swin Transformer)ï¼šåŒæ ·è¾“å‡º 1/4, 1/8, 1/16, 1/32 å››ä¸ªå°ºåº¦ç‰¹å¾ã€‚ 3. FPN/UPerNet Headï¼š â€¢ å°†å¤šå±‚ç‰¹å¾èåˆï¼Œå¯¹åº”ä¸åŒè¯­ä¹‰å±‚çº§ã€‚ â€¢ åˆ©ç”¨èåˆåçš„ç‰¹å¾ç”Ÿæˆåƒç´ çº§é¢„æµ‹ã€‚ 4. é¢„æµ‹å›¾ (segmentation map)ï¼šå¤§å° 512Ã—512ï¼Œæ¯ä¸ªåƒç´ å±äºä¸€ä¸ªç±»åˆ«ã€‚ â€¢ [0,0] åƒç´  â†’ â€œskyâ€ â€¢ [100,150] åƒç´  â†’ â€œbuildingâ€ â€¢ [200,300] åƒç´  â†’ â€œroadâ€ 5. è¾“å‡ºï¼šå®Œæ•´çš„è¯­ä¹‰åˆ†å‰²å›¾ï¼Œæ¯ä¸ªåƒç´ éƒ½æœ‰ç±»åˆ«æ ‡ç­¾ã€‚\nğŸ‘‰ åœ¨ ADE20K ä¸Šï¼ŒSwin-L + UPerNet çš„ mIoU è¾¾åˆ° 53.5+ï¼Œæ¯”ä¼ ç»Ÿ CNN backbone æå‡æ˜¾è‘—ã€‚ å…·ä½“çš„å®ç°ç»†èŠ‚ï¼Œç­‰åˆ°ä»¥åæˆ‘ä»¬é˜…è¯»åˆ°å…³äºSegmentationçš„å†…å®¹åœ¨ï¼Œå†æ¥å®ç°\n\n\n\n\nWe employ an AdamW optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We include most of the augmentation and regularization strategies of in training, except for repeated augmentation and EMA, which do not enhance performance.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\n\nè®ºæ–‡ä¸­è¿˜ç”¨åˆ°äº† DropPath æ¥å½“ä½œä¸€ç§ Regularizationã€‚ DropPath ä¹Ÿç§°ä¹‹ä¸º Stochastic Depth (Huang et al. 2016) , å®ƒæ˜¯ä¸€ç§åº”ç”¨åœ¨Residual Networkï¼Œ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºä¸¢å¼ƒæ•´ä¸ª æ®‹å·®åˆ†æ”¯ (residual branch) æˆ– æ•´ä¸ªè·¯å¾„ (path)ã€‚å‡å°‘è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶è®©æ¨¡å‹å­¦ä¼šä¾èµ–ä¸åŒæ·±åº¦çš„è·¯å¾„ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚ \nä¸Dropout ä¸åŒçš„æ˜¯ï¼Œ Dropout ä¸¢å¼ƒçš„æ˜¯å•ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼Œ è€ŒDropPath ä¸¢å¼ƒçš„æ˜¯æ•´ä¸ªæ®‹å·®åˆ†æ”¯ / æ•´å±‚ Block\n\n\n\n\n\n\n\n\nç‰¹æ€§\nDropout (ç»å…¸)\nDropPath (Stochastic Depth)\n\n\n\n\nä¸¢å¼ƒå¯¹è±¡\nå•ä¸ªç¥ç»å…ƒçš„è¾“å‡º\næ•´ä¸ªæ®‹å·®åˆ†æ”¯ / æ•´å±‚ Block\n\n\nåº”ç”¨ç²’åº¦\né€å…ƒç´  (element-wise)\nå±‚çº§ (layer-wise)\n\n\nä½¿ç”¨åœºæ™¯\nå…¨è¿æ¥å±‚ã€CNNã€RNN ç­‰\næ®‹å·®ç½‘ç»œã€Transformer ç­‰\n\n\næ¨ç†é˜¶æ®µæ•ˆæœ\nä¸ä¸¢å¼ƒï¼Œä½¿ç”¨ç¼©æ”¾è¡¥å¿\nä¸ä¸¢å¼ƒï¼Œä¿ç•™å®Œæ•´è·¯å¾„\n\n\nä½œç”¨\nå‡å°‘ç¥ç»å…ƒè¿‡æ‹Ÿåˆ\né˜²æ­¢æ·±å±‚ç½‘ç»œè¿‡æ‹Ÿåˆã€æå‡ç¨³å®šæ€§\n\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n        \n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(x)\n        \n        shortcut = x \n        ... # FFN\n        x = shortcut + self.drop_path(x)\n        \n        ...\n        \n        return x \n\nğŸ“ TAKEAWAY DropPathï¼ˆä¹Ÿå« Stochastic Depthï¼‰æ˜¯ä¸€ç§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ƒåœ¨è®­ç»ƒæ—¶éšæœºè·³è¿‡ï¼ˆä¸¢å¼ƒï¼‰æ•´ä¸ªç½‘ç»œå±‚æˆ–åˆ†æ”¯çš„è®¡ç®—ï¼Œä»¥å‡å°‘è¿‡æ‹Ÿåˆå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n\n\n\nåœ¨æ­¤ï¼Œæˆ‘ä»¬åœ¨ä»‹ç»ä¸€ä¸ªè®­ç»ƒæ–¹æ³•ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒï¼Œå«åšGradient Checkpointåˆå«åšActivation Checkpointï¼Œ ç”¨PyTorhå®ç°ï¼Œæ˜¯å¾ˆå®¹æ˜“çš„ çš„ï¼Œæˆ‘ä»¬åªéœ€è¦call utils.checkpoint\næ­£å¸¸è®­ç»ƒæµç¨‹ï¼š åœ¨å‰å‘ä¼ æ’­ï¼ˆforwardï¼‰æ—¶ï¼Œæ¯ä¸€å±‚çš„ä¸­é—´æ¿€æ´»å€¼ï¼ˆactivationï¼‰éƒ½ä¼šä¿å­˜ä¸‹æ¥ï¼Œä»¥ä¾¿åå‘ä¼ æ’­ï¼ˆbackwardï¼‰æ—¶ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚ é—®é¢˜æ˜¯ï¼šä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ä¼šæ¶ˆè€—å¤§é‡æ˜¾å­˜ï¼ˆGPU memoryï¼‰ã€‚ â€¢ Gradient Checkpoint çš„æ€è·¯ï¼š å¹¶ä¸æ˜¯ä¿å­˜æ‰€æœ‰æ¿€æ´»å€¼ï¼Œè€Œæ˜¯åªåœ¨éƒ¨åˆ†å…³é”®èŠ‚ç‚¹ï¼ˆcheckpointï¼‰ä¿å­˜æ¿€æ´»ã€‚ å¯¹äºæœªä¿å­˜çš„æ¿€æ´»å€¼ï¼Œåœ¨åå‘ä¼ æ’­æ—¶é‡æ–°å†è·‘ä¸€æ¬¡å‰å‘è®¡ç®—æ¥å¾—åˆ°å®ƒä»¬ï¼Œä»è€ŒèŠ‚çœæ˜¾å­˜ã€‚\næ¢å¥è¯è¯´ï¼šç”¨è®¡ç®—æ¢æ˜¾å­˜ã€‚\nğŸ”¹ å·¥ä½œæœºåˆ¶ 1. åœ¨å‰å‘ä¼ æ’­æ—¶ï¼š â€¢ æ¨¡å‹è¢«åˆ‡åˆ†æˆè‹¥å¹²å—ï¼ˆsegmentsï¼‰ã€‚ â€¢ åªä¿å­˜æ¯ä¸€å—çš„è¾“å…¥ï¼Œä¸¢å¼ƒä¸­é—´çš„æ¿€æ´»ã€‚ 2. åœ¨åå‘ä¼ æ’­æ—¶ï¼š â€¢ éœ€è¦ç”¨åˆ°æ¢¯åº¦æ—¶ï¼Œé‡æ–°å¯¹é‚£ä¸€å—åšä¸€æ¬¡ forward æ¥æ¢å¤æ¿€æ´»ã€‚ â€¢ ç„¶åæ­£å¸¸è®¡ç®—æ¢¯åº¦ã€‚\nâ€¢   å¢åŠ è®¡ç®—å¼€é”€ï¼šå› ä¸ºè¦åœ¨ backward æ—¶é‡æ–°åšä¸€æ¬¡ forwardã€‚\nâ€¢   ä¸€èˆ¬ä¼šå¸¦æ¥ 20%ï½30% é¢å¤–çš„è®­ç»ƒæ—¶é—´ã€‚\nimport torch\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        def custom_forward(*inputs):\n            return self.layer2(self.layer1(*inputs))\n        \n        # å¯¹è¿™éƒ¨åˆ†ä½¿ç”¨ checkpoint\n        x = checkpoint(custom_forward, x)\n        return x\n\nğŸ“ TAKEAWAY  Gradient Checkpointing æ˜¯ä¸€ç§ ç”¨é¢å¤–è®¡ç®—æ¢æ˜¾å­˜ çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨å‰å‘ä¼ æ’­æ—¶å°‘å­˜æ¿€æ´»ï¼Œåå‘ä¼ æ’­æ—¶é‡ç®—ï¼Œèƒ½è®©å¤§æ¨¡å‹åœ¨æœ‰é™æ˜¾å­˜ä¸‹å®Œæˆè®­ç»ƒã€‚\n\n\n\n\n\nâ€œSwin V2â€ (Liu et al. 2022) æ˜¯åœ¨åŸå§‹ Swin Transformer çš„åŸºç¡€ä¸Šï¼Œä¸ºäº†æ›´å¥½åœ° æ‰©å±•æ¨¡å‹å®¹é‡ï¼ˆæ›´å¤šå‚æ•°ï¼‰ã€å¤„ç†é«˜åˆ†è¾¨ç‡è¾“å…¥ ä»¥åŠ æé«˜è®­ç»ƒç¨³å®šæ€§ æ‰€åšçš„ä¸€ç³»åˆ—æ”¹è¿›ã€‚ åœ¨è§†è§‰ä»»åŠ¡ä¸­ï¼ŒTransformer æ¨¡å‹è‹¥è¦å˜å¾—æ›´å¼ºï¼ˆæ›´å¤šå‚æ•°ã€æ›´é«˜åˆ†è¾¨ç‡è¾“å…¥ã€æ›´å¤šå±‚æ•°ï¼‰å°±ä¼šé‡åˆ°å‡ ä¸ªæŒ‘æˆ˜ï¼š 1. è®­ç»ƒä¸ç¨³å®šï¼šéšç€æ¨¡å‹å˜æ·±ã€é€šé“å˜å®½ï¼Œå†…éƒ¨æ¿€æ´»çš„å¹…åº¦å¯èƒ½æ€¥å‰§å¢é•¿ï¼Œå¯¼è‡´æ¢¯åº¦ã€æ•°å€¼ä¸ç¨³å®šã€‚ 2. åˆ†è¾¨ç‡è¿ç§»é—®é¢˜ï¼šæ¨¡å‹åœ¨ä½åˆ†è¾¨ç‡ä¸‹é¢„è®­ç»ƒï¼ˆä¾‹å¦‚ 224Ã—224ï¼‰åï¼Œç”¨åœ¨é«˜åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚ 1,536Ã—1,536ï¼‰æˆ–ä¸åŒçª—å£å°ºå¯¸æ—¶è¡¨ç°ä¼šä¸‹é™ã€‚ 3. å¯¹æ ‡æ³¨æ•°æ®çš„è¿‡åº¦ä¾èµ–ï¼šå¤§æ¨¡å‹éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®æ‰èƒ½è®­ç»ƒå¾—å¥½ã€‚\nSwin V2 å°±æ˜¯ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæ”¯æŒè®­ç»ƒè¶…å¤§æ¨¡å‹ï¼ˆå¦‚ 30 äº¿å‚æ•°çº§åˆ«ï¼‰ï¼ŒåŒæ—¶èƒ½å¤„ç†å¤§å°ºå¯¸è¾“å…¥ \n\n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(self.norm1(x))\n        \n        shortcut = x \n        ... # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n        \n        ...\n        \n        return x \n\n\n\nclass WindowAttention:\n    def __init__(self,):\n    \n        ...\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n        ...\n    \n    \n    def forward(self, x):\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01))).exp()\n        attn = attn * logit_scale\n\n\n\nlog-spaced continuous position bias approach to address the issue in transferring across window resolutions\n\\[\n\\begin{split}\n\\widehat{\\Delta x} &= \\operatorname{sign}(x) \\cdot \\log(1 + |\\Delta x|) \\\\\n\\widehat{\\Delta y} &= \\operatorname{sign}(y) \\cdot \\log(1 + |\\Delta y|)\n\\end{split}\n\\]\nself.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                             nn.ReLU(inplace=True),\n                             nn.Linear(512, num_heads, bias=False))\n                             \ndef forward(self, x):\n    relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "W-MHA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š\n\næŠŠå›¾åƒåˆ’åˆ†æˆå›ºå®šå¤§å°çš„çª—å£ï¼ˆwindowï¼‰ï¼Œæ¯”å¦‚ 7Ã—7 patch çš„çª—å£ã€‚\nåœ¨çª—å£å†…çš„ token ä¹‹é—´åšå±€éƒ¨è‡ªæ³¨æ„åŠ›ï¼Œè€Œä¸æ˜¯åœ¨æ•´å¼ å›¾åƒçš„æ‰€æœ‰ token ä¹‹é—´åšå…¨å±€æ³¨æ„åŠ›ã€‚\næ¯ä¸ªçª—å£ç‹¬ç«‹è®¡ç®— Multi-Head Attention â†’ é™ä½è®¡ç®—é‡ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥å¹¶è¡Œçš„è®¡ç®—\n\nè¿™æ ·ä¸€æ¥ï¼š\n\nå•ä¸ªçª—å£ token æ•°é‡å›ºå®š = \\(M^{2}\\)ï¼ˆå¦‚ 7Ã—7=49ï¼‰ã€‚\næ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ä» \\(\\mathcal{O}((hw)^{2}C)\\) é™ä½ä¸º \\(\\mathcal{O}(M^{2}hwC)\\)ï¼Œå…¶ä¸­ \\(M \\ll \\sqrt{ N }\\)ã€‚\n\né™¤äº†é™ä½è®¡ç®—å¤æ‚åº¦ä¹‹å¤–ï¼ŒW-MHAï¼Œè¿˜æœ‰ä¿ç•™CNN åœ¨å›¾åƒå¤„ç†ä¸­å¼ºå¤§çš„ä¸€ç‚¹æ˜¯ å±€éƒ¨æ„Ÿå—é‡ å’Œ å¹³ç§»ä¸å˜æ€§ã€‚\n\nW-MHA é€šè¿‡çª—å£é™åˆ¶ï¼Œä½¿å¾—æ³¨æ„åŠ›æœºåˆ¶ä¹Ÿå…·å¤‡ç±»ä¼¼çš„å±€éƒ¨å½’çº³åç½®ï¼ˆinductive biasï¼‰ï¼Œé€‚åˆå›¾åƒå»ºæ¨¡ã€‚\n\n\nFor efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "W-MHA å¾ˆå¥½ï¼Œä½†æ˜¯å®ƒå­˜åœ¨çš„ä¸€ä¸ªé—®é¢˜å°±æ˜¯ï¼š\n\nçª—å£ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œç¼ºå°‘è·¨çª—å£çš„ä¿¡æ¯äº¤æµã€‚è¿™ä¼šå¯¼è‡´ï¼Œæ¨¡å‹åªèƒ½çœ‹è§å±€éƒ¨ï¼Œä¸èƒ½è·å¾—å…¨å±€çš„ä¿¡æ¯ã€‚\n\n\nThe window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSwin- Transformeræå‡ºæ¥ Shifted Window Mulit-Head-Attention (SW-MHA) çª—å£ä½ç½®ç›¸å¯¹å‰ä¸€å±‚å¹³ç§»ï¼Œæ¯”å¦‚ 7Ã—7 çª—å£ â†’ å¹³ç§» 3 ä¸ª patchã€‚ è¿™æ ·ï¼Œæ–°çš„çª—å£ä¼šè·¨è¶ŠåŸæ¥çš„è¾¹ç•Œï¼Œtoken ä¼šå’Œç›¸é‚»çª—å£çš„ token ä¸€èµ·è®¡ç®—æ³¨æ„åŠ›ã€‚ ç›¸å½“äºå¼ºåˆ¶è·¨çª—å£äº¤äº’ï¼Œè®©ä¿¡æ¯å¯ä»¥åœ¨ä¸åŒåŒºåŸŸä¹‹é—´æµåŠ¨ã€‚\n å¦‚ä¸Šå¦‚æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†Windowé€šè¿‡å‘å·¦ä¸Šè§’ç§»åŠ¨ï¼Œé€šè¿‡ç»™å›¾ç‰‡å¢åŠ Paddingæ¥ï¼Œä½†æ˜¯è¿™ç§åŠæ³•æ˜¾ç„¶ä¼šå¢åŠ è®¡ç®—çš„å¤æ‚åº¦ã€‚Swin Transformerç”¨äº†ä¸€ç§å¾ˆèªæ˜çš„åŠæ³•ï¼Œå«åš Cycling Shiftï¼Œè¿™ç§æ–¹æ³•å°±æ˜¯å°†å°†ä¸€ä¸ªå¼ é‡æˆ–å›¾åƒåœ¨æŸä¸ªç»´åº¦ä¸Šåš å¹³ç§»ï¼Œä½†ä¸æ˜¯æŠŠç§»å‡ºå»çš„éƒ¨åˆ†ä¸¢æ‰ï¼Œè€Œæ˜¯ é‡æ–°ä»å¦ä¸€è¾¹è¡¥å›æ¥ã€‚å°±åƒâ€œç¯å½¢é˜Ÿåˆ—â€æˆ–â€œé’Ÿè¡¨èµ°ä¸€åœˆåˆå›åˆ°èµ·ç‚¹â€ã€‚ å¦‚ä¸‹å›¾æ‰€ç¤º \nå¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡Cycling Shiftï¼Œæˆ‘ä»¬å¾—åˆ°çš„æ¯ä¸ªwindowçš„å†…å®¹ï¼Œå’Œä¹‹å‰æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯æ‰€éœ€è¦çš„Windowçš„æ•°é‡ï¼Œå°äº†å¾ˆå¤šï¼Œè¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œæ‰€éœ€è¦çš„æ—¶é—´å¤æ‚åº¦ï¼Œä¹Ÿå°äº†å¾ˆå¤šã€‚\n\nä¸è¿‡Cycling Shiftä¹Ÿæœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯åŒä¸€ä¸ªçª—å£é‡Œé¢ï¼Œå¯èƒ½æœ‰æ¥è‡ªä¸åŒå›¾ç‰‡çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åœ¨åŸå›¾ç‰‡ä¸Šä¸æ˜¯ç›¸é‚»çš„ï¼Œè‡ªç„¶ä¸åº”è¯¥ç›¸äº’äº¤æµä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥å°†å›¾ç‰‡ï¼ŒæŠ½è±¡æˆä¸‹å›¾çš„å½¢å¼ã€‚ç»„ç»‡Attentionäº¤æµï¼Œå¾ˆè‡ªç„¶çš„ä¸€ç§æ–¹æ³•æ˜¯åˆ©ç”¨Maskï¼Œå°±åƒTransformeré‡Œçš„Causal Maskä¸€æ ·ã€‚ä½†æ˜¯ï¼Œè¿™ä¸ªMaské•¿ä»€ä¹ˆæ ·å­å‘¢\n\næˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹Maskï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ‰é¢œè‰²çš„åŒºåŸŸè¡¨ç¤ºMask == 1ï¼Œ åœ¨æ­¤ä¸ºäº†æ›´å¥½çš„",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "Swin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n\n\\[\n\\begin{split}\n\\hat{z}^l &= \\text{W-MSA} \\left( \\text{LN} \\left( z^{l-1} \\right) \\right) + z^{l-1} \\\\\nz^l &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^l \\right) \\right) + \\hat{z}^l \\\\\n\\hat{z}^{l+1} &= \\text{SW-MSA} \\left( \\text{LN} \\left( z^l \\right) \\right) + z^l \\\\\nz^{l+1} &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^{l+1} \\right) \\right) + \\hat{z}^{l+1}\n\\end{split}\n\\]\nå°†W-MSA å’Œ SW-MSAå åœ¨ä¸€èµ·ï¼Œå°±å¾—åˆ°äº†Transformer Blockï¼Œå½“ç„¶ï¼Œè¿˜æœ‰ä¸€ä¸ªMLPï¼ŒLayer Normalizationï¼Œåœ¨æ­¤å°±ä¸èµ˜è¿°äº†ã€‚",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#patch-merge",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#patch-merge",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "è®²å®Œäº†W-MHAï¼Œå’ŒSW-MHAï¼Œæˆ‘ä»¬å°±ç†è§£äº†Swin- Transformerä¸­æœ€éš¾ç†è§£ï¼Œä¹Ÿæ˜¯æœ€ç»ˆçš„éƒ¨åˆ†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹å…¶ä»–ç®€å•çš„éƒ¨åˆ†ã€‚ Patch Merge , å›¾ä¸­ç»¿è‰²çš„éƒ¨åˆ†ï¼Œé€æ­¥é™ä½ token æ•°é‡ï¼ˆé™é‡‡æ ·ï¼‰ï¼ŒåŒæ—¶å¢åŠ ç‰¹å¾ç»´åº¦çš„æ“ä½œã€‚è¿™ç±»ä¼¼äºCNNä¸­çš„æ“ä½œï¼Œéšç€å±‚æ•°çš„å¢åŠ ï¼Œåˆ†è¾¨ç‡é€æ­¥é™ä½ã€é€šé“æ•°é€æ­¥å¢åŠ ï¼Œè¿™æ ·æ—¢å‡å°‘äº†è®¡ç®—é‡ï¼Œåˆèƒ½æå–å±‚çº§ç‰¹å¾ã€‚å…·ä½“çš„å®ç°ï¼š\n\nåˆ†ç»„ï¼šå°†ç›¸é‚»çš„ 2Ã—2 patch åˆå¹¶æˆä¸€ä¸ªæ–°çš„ patchã€‚\n\nå‡è®¾è¾“å…¥ç‰¹å¾å¤§å°ä¸º (H, W, C)ã€‚\næ¯ 2Ã—2 çš„ patch â†’ åˆå¹¶ä¸º 1 ä¸ªæ–° tokenã€‚\næ–°ç‰¹å¾å›¾å¤§å°å˜ä¸º (H/2, W/2, 4C)ã€‚\n\nçº¿æ€§å˜æ¢:\n\nå°†åˆå¹¶åçš„ 4C ç»´ç‰¹å¾é€šè¿‡ä¸€ä¸ª çº¿æ€§å±‚ (Linear Projection)ï¼Œé™åˆ° 2C ç»´ã€‚\nè¾“å‡ºç»´åº¦ç¿»å€ï¼ˆ2Cï¼‰ï¼Œä»¥è¡¥å¿åˆ†è¾¨ç‡å‡åŠå¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚ ğŸ”¹ ä¸ºä»€ä¹ˆæå‡º Patch Merging\n\n\nåˆ†å±‚è¡¨ç¤º (Hierarchical Representation) â€¢ æ¨¡ä»¿ CNN çš„é‡‘å­—å¡”ç»“æ„ï¼Œä»å±€éƒ¨ç»†èŠ‚é€æ­¥èšåˆåˆ°å…¨å±€è¯­ä¹‰ã€‚ â€¢ æœ‰åˆ©äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆæ£€æµ‹ã€åˆ†å‰²ï¼‰ä¸­ä¸åŒå°ºåº¦çš„ç›®æ ‡å»ºæ¨¡ã€‚\nè®¡ç®—æ•ˆç‡ â€¢ token æ•°é‡é€å±‚å‡å°‘ â†’ Attention çš„å¤æ‚åº¦å¤§å¹…ä¸‹é™ã€‚ â€¢ ä¿è¯æ¨¡å‹å¯æ‰©å±•åˆ°å¤§åˆ†è¾¨ç‡å›¾åƒã€‚\nè¯­ä¹‰ä¿¡æ¯èšåˆ â€¢ é€šè¿‡åˆå¹¶ç›¸é‚» patchï¼Œæ¨¡å‹èƒ½æŠŠæ›´å¤§æ„Ÿå—é‡çš„ä¿¡æ¯æ•´åˆåˆ°æ–°çš„ token ä¸­ã€‚\n\n\nx = x.view(B, H, W, C)\n\nx0 = x[:, 0::2, 0::2, :]  # (B, H/2, W/2, C)\nx1 = x[:, 1::2, 0::2, :]  # (B, H/2, W/2, C)\nx2 = x[:, 0::2, 1::2, :]  # (B, H/2, W/2, C)\nx3 = x[:, 1::2, 1::2, :]  # (B, H/2, W/2, C)\n\nx = torch.cat([x0, x1, x2, x3], -1)  # (B, H/2, W/2, 4*C)\nx = x.view(B, -1, 4 * C)  # (B, H/2*W/2, 4*C)\n\nx = self.reduction(x)  # (B, H/2*W/2, 2*C)",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "ä¸Transformer å’Œ Vision-Transformer ä¸­ä¸åŒçš„æ˜¯ï¼ŒSwin Transformeråˆ©ç”¨çš„æ˜¯Relative Position Encodingã€‚\n\\[\n\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^{T}}{\\sqrt{ d }} +B\\right) V\n\\]\n1.  å®šä¹‰åç½®è¡¨ (relative_position_bias_table)\nâ€¢   å¤§å°æ˜¯ (2*Wh-1) * (2*Ww-1, num_heads)\nâ€¢   æ„å‘³ç€çª—å£å†…çš„ä»»æ„ä¸¤ä¸ª token çš„ç›¸å¯¹ä½ç½® (dx, dy)ï¼Œéƒ½æœ‰ä¸€ä¸ªå¯å­¦ä¹ çš„åç½®å€¼ï¼ˆæ¯ä¸ª head ä¸€ä»½ï¼‰ã€‚\nâ€¢   ä¾‹å¦‚çª—å£æ˜¯ 7Ã—7 â†’ ç›¸å¯¹ä½ç½®èŒƒå›´æ˜¯ [-6,6]ï¼Œæ‰€ä»¥è¡¨å¤§å°æ˜¯ 13Ã—13=169ï¼Œæ¯ä¸ªä½ç½®å­˜ä¸€ç»„åç½®\n\n\n2.  è®¡ç®—ç›¸å¯¹ä½ç½®ç´¢å¼• (relative_position_index)\nâ€¢   é¦–å…ˆç”Ÿæˆçª—å£å†…æ¯ä¸ª token çš„åæ ‡ã€‚\nâ€¢   ç„¶ååšå·®ï¼Œå¾—åˆ°ä»»æ„ä¸¤ä¸ª token çš„ç›¸å¯¹åæ ‡ (dx, dy)ã€‚\nâ€¢   å†æ˜ å°„æˆè¡¨çš„ç´¢å¼•ï¼ˆé€šè¿‡ç§»ä½å’Œå“ˆå¸Œæˆä¸€ä¸ªæ•´æ•° indexï¼‰ã€‚\nâ€¢   ç»“æœæ˜¯ä¸€ä¸ª (Wh*Ww, Wh*Ww) çš„çŸ©é˜µï¼Œæ¯ä¸ªå…ƒç´ å­˜ä¸¤ä¸ª token ä¹‹é—´åœ¨ bias è¡¨é‡Œçš„ç´¢å¼•ã€‚\n\n\nâ€¢   åœ¨å›¾åƒé‡Œï¼Œç›¸å¯¹ä½ç½®æ¯”ç»å¯¹ä½ç½®æ›´é‡è¦ï¼š\nâ€¢   æ¯”å¦‚ä¸€ä¸ªåƒç´ çš„å·¦é‚»å’Œå³é‚»å¾ˆç›¸ä¼¼ï¼Œæ— è®ºè¿™ä¸ªåƒç´ åœ¨å›¾åƒçš„å“ªä¸ªåœ°æ–¹ã€‚\n\n\nå’Œ Vision-Transformer ä¸€æ ·ï¼Œå½“è¾“å…¥çš„å›¾ç‰‡å’Œè®­ç»ƒæ—¶ä¸ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ bi-cubic interpolation æ¥å¢å¤§Relative Position\n\nThe learnt relative position bias in pre-training can be also used to initialize a model for fine-tuning with a different window size through bi-cubic interpolation  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#others",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#others",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "é™¤äº†ä»¥ä¸Šå‡ ä¸ªï¼ŒSwin Transformer ä¸­è¿˜æœ‰å…¶ä»–Componentï¼Œæ¯”å¦‚ ï¼š\n\nPatch Embedding\nLinear Projection\nFeedForward\nLayer Normalization åœ¨æ­¤ï¼Œå°±ä¸èµ˜è¿°äº†ï¼Œæœ‰éœ€è¦çš„åŒå­¦ï¼Œè¯·å‚è€ƒå‰ä¸€ç¯‡ Vision-Transformerï¼Œ æˆ–è€… Transformer",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "å½“ä¸€åœºå›¾ç‰‡ä¼ å…¥Swin Transformerï¼Œ å®ƒå¯ä»¥æå–å‡ºå›¾ç‰‡çš„ç‰¹å¾ã€‚ \\[\n\\mathrm{z} = f_{\\theta}(\\mathrm{x}), \\quad \\text{where}\\ \\mathrm{x} \\in \\mathbb{R}^{3\\times H \\times W}, \\mathrm{z} \\in \\mathbb{R}^{H'W' \\times C}\n\\]\nä¸€å¼ å›¾ç‰‡è½¬åŒ–æˆäº† \\(H'W'\\) ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„å¤§å°ä¸º $Cã€‚\nSwin Transformer å¯ä»¥æœ‰å½“ä½œåŸºæœ¬çš„backboneï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ä¸‹æ¸¸è¿›è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š\n\nImage Classification\nObject Detection\nSemantic segmentation\n\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¦‚ä½•ç”¨Swin Transformeråœ¨ä¸åŒçš„ä»»åŠ¡ä¸­\n\n\n\nå¯¹äº \\(\\mathrm{z}\\) çš„ hidden statesï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€ä¸ªAverage Poolingï¼Œå¯¹äºæ¯ä¸€ä¸ªç‰¹å¾æ±‚å‡å€¼ï¼Œç„¶åå†å°†è¿™ä¸ªä¼ å…¥ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬Classificationäº†ã€‚ä¸ Vision-Transformer ä¸åŒçš„æ˜¯ï¼ŒSwin Transformer æ²¡æœ‰ [CLS] token æ¥å½“æ”¶é›†å…¨éƒ¨çš„ä¿¡æ¯ã€‚",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "Backbone (Swin Transformer)ï¼š\n\nStage 1: [N, C1, H/4, W/4]\nStage 2: [N, C2, H/8, W/8]\nStage 3: [N, C3, H/16, W/16]\nStage 4: [N, C4, H/32, W/32]\n\nå¯ä»¥å¾—åˆ° FPN(Lin et al. 2017) to create a pyramid of feature maps suitable for detection.\n{P2: [N, C, H/4, W/4], \n P3: [N, C, H/8, W/8], \n P4: [N, C, H/16, W/16], \n P5: [N, C, H/32, W/32]}\næœ‰äº†è¿™äº›FPN ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ç»“åˆä¸åŒçš„ç®—æ³•ï¼Œæ¥è¿›è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œæ¯”å¦‚\nä¾‹å­ 1ï¼šç›®æ ‡æ£€æµ‹ (Object Detection)\nä»¥ Swin Transformer + Faster R-CNN (Ren et al. 2016) ä¸ºä¾‹ï¼š 1. è¾“å…¥å›¾åƒï¼šä¸€å¼  800Ã—1333 çš„ COCO æ•°æ®é›†å›¾ç‰‡ã€‚\n3.  FPN (ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ)ï¼šå°†å¤šå°ºåº¦ç‰¹å¾èåˆï¼Œå½¢æˆç»Ÿä¸€çš„é‡‘å­—å¡”ç‰¹å¾ã€‚\n4.  RPN (Region Proposal Network)ï¼šåœ¨ç‰¹å¾å›¾ä¸Šç”Ÿæˆå€™é€‰åŒºåŸŸã€‚\n5.  RoI Headï¼šå¯¹å€™é€‰åŒºåŸŸè¿›è¡Œåˆ†ç±» (è½¦ã€äººã€ç‹—â€¦) å’Œè¾¹æ¡†å›å½’ã€‚\n6.  è¾“å‡ºï¼šé¢„æµ‹ç»“æœï¼Œä¾‹å¦‚ï¼š\nâ€¢   â€œä¸€è¾†è½¦â€ â†’ è¾¹æ¡† (x1,y1,x2,y2) + ç±»åˆ« â€œcarâ€\nâ€¢   â€œä¸€ä¸ªäººâ€ â†’ è¾¹æ¡† + ç±»åˆ« â€œpersonâ€\n ğŸ‘‰ åœ¨ COCO æ•°æ®é›†ä¸Šï¼ŒSwin-T + Faster R-CNNæ¯” ResNet-50 + Faster R-CNN çš„ mAP æé«˜çº¦ 5~6 ä¸ªç‚¹ã€‚\nè¯­ä¹‰åˆ†å‰² (Semantic Segmentation)  ä»¥ Swin Transformer + UPerNet(Xiao et al. 2018)ä¸ºä¾‹ï¼š 1. è¾“å…¥å›¾åƒï¼šä¸€å¼  512Ã—512 çš„ ADE20K æ•°æ®é›†å›¾ç‰‡ã€‚ 2. Backbone (Swin Transformer)ï¼šåŒæ ·è¾“å‡º 1/4, 1/8, 1/16, 1/32 å››ä¸ªå°ºåº¦ç‰¹å¾ã€‚ 3. FPN/UPerNet Headï¼š â€¢ å°†å¤šå±‚ç‰¹å¾èåˆï¼Œå¯¹åº”ä¸åŒè¯­ä¹‰å±‚çº§ã€‚ â€¢ åˆ©ç”¨èåˆåçš„ç‰¹å¾ç”Ÿæˆåƒç´ çº§é¢„æµ‹ã€‚ 4. é¢„æµ‹å›¾ (segmentation map)ï¼šå¤§å° 512Ã—512ï¼Œæ¯ä¸ªåƒç´ å±äºä¸€ä¸ªç±»åˆ«ã€‚ â€¢ [0,0] åƒç´  â†’ â€œskyâ€ â€¢ [100,150] åƒç´  â†’ â€œbuildingâ€ â€¢ [200,300] åƒç´  â†’ â€œroadâ€ 5. è¾“å‡ºï¼šå®Œæ•´çš„è¯­ä¹‰åˆ†å‰²å›¾ï¼Œæ¯ä¸ªåƒç´ éƒ½æœ‰ç±»åˆ«æ ‡ç­¾ã€‚\nğŸ‘‰ åœ¨ ADE20K ä¸Šï¼ŒSwin-L + UPerNet çš„ mIoU è¾¾åˆ° 53.5+ï¼Œæ¯”ä¼ ç»Ÿ CNN backbone æå‡æ˜¾è‘—ã€‚ å…·ä½“çš„å®ç°ç»†èŠ‚ï¼Œç­‰åˆ°ä»¥åæˆ‘ä»¬é˜…è¯»åˆ°å…³äºSegmentationçš„å†…å®¹åœ¨ï¼Œå†æ¥å®ç°",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#training-details",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#training-details",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "We employ an AdamW optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We include most of the augmentation and regularization strategies of in training, except for repeated augmentation and EMA, which do not enhance performance.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\n\nè®ºæ–‡ä¸­è¿˜ç”¨åˆ°äº† DropPath æ¥å½“ä½œä¸€ç§ Regularizationã€‚ DropPath ä¹Ÿç§°ä¹‹ä¸º Stochastic Depth (Huang et al. 2016) , å®ƒæ˜¯ä¸€ç§åº”ç”¨åœ¨Residual Networkï¼Œ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºä¸¢å¼ƒæ•´ä¸ª æ®‹å·®åˆ†æ”¯ (residual branch) æˆ– æ•´ä¸ªè·¯å¾„ (path)ã€‚å‡å°‘è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶è®©æ¨¡å‹å­¦ä¼šä¾èµ–ä¸åŒæ·±åº¦çš„è·¯å¾„ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚ \nä¸Dropout ä¸åŒçš„æ˜¯ï¼Œ Dropout ä¸¢å¼ƒçš„æ˜¯å•ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼Œ è€ŒDropPath ä¸¢å¼ƒçš„æ˜¯æ•´ä¸ªæ®‹å·®åˆ†æ”¯ / æ•´å±‚ Block\n\n\n\n\n\n\n\n\nç‰¹æ€§\nDropout (ç»å…¸)\nDropPath (Stochastic Depth)\n\n\n\n\nä¸¢å¼ƒå¯¹è±¡\nå•ä¸ªç¥ç»å…ƒçš„è¾“å‡º\næ•´ä¸ªæ®‹å·®åˆ†æ”¯ / æ•´å±‚ Block\n\n\nåº”ç”¨ç²’åº¦\né€å…ƒç´  (element-wise)\nå±‚çº§ (layer-wise)\n\n\nä½¿ç”¨åœºæ™¯\nå…¨è¿æ¥å±‚ã€CNNã€RNN ç­‰\næ®‹å·®ç½‘ç»œã€Transformer ç­‰\n\n\næ¨ç†é˜¶æ®µæ•ˆæœ\nä¸ä¸¢å¼ƒï¼Œä½¿ç”¨ç¼©æ”¾è¡¥å¿\nä¸ä¸¢å¼ƒï¼Œä¿ç•™å®Œæ•´è·¯å¾„\n\n\nä½œç”¨\nå‡å°‘ç¥ç»å…ƒè¿‡æ‹Ÿåˆ\né˜²æ­¢æ·±å±‚ç½‘ç»œè¿‡æ‹Ÿåˆã€æå‡ç¨³å®šæ€§\n\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n        \n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(x)\n        \n        shortcut = x \n        ... # FFN\n        x = shortcut + self.drop_path(x)\n        \n        ...\n        \n        return x \n\nğŸ“ TAKEAWAY DropPathï¼ˆä¹Ÿå« Stochastic Depthï¼‰æ˜¯ä¸€ç§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ƒåœ¨è®­ç»ƒæ—¶éšæœºè·³è¿‡ï¼ˆä¸¢å¼ƒï¼‰æ•´ä¸ªç½‘ç»œå±‚æˆ–åˆ†æ”¯çš„è®¡ç®—ï¼Œä»¥å‡å°‘è¿‡æ‹Ÿåˆå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n\n\n\nåœ¨æ­¤ï¼Œæˆ‘ä»¬åœ¨ä»‹ç»ä¸€ä¸ªè®­ç»ƒæ–¹æ³•ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒï¼Œå«åšGradient Checkpointåˆå«åšActivation Checkpointï¼Œ ç”¨PyTorhå®ç°ï¼Œæ˜¯å¾ˆå®¹æ˜“çš„ çš„ï¼Œæˆ‘ä»¬åªéœ€è¦call utils.checkpoint\næ­£å¸¸è®­ç»ƒæµç¨‹ï¼š åœ¨å‰å‘ä¼ æ’­ï¼ˆforwardï¼‰æ—¶ï¼Œæ¯ä¸€å±‚çš„ä¸­é—´æ¿€æ´»å€¼ï¼ˆactivationï¼‰éƒ½ä¼šä¿å­˜ä¸‹æ¥ï¼Œä»¥ä¾¿åå‘ä¼ æ’­ï¼ˆbackwardï¼‰æ—¶ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚ é—®é¢˜æ˜¯ï¼šä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ä¼šæ¶ˆè€—å¤§é‡æ˜¾å­˜ï¼ˆGPU memoryï¼‰ã€‚ â€¢ Gradient Checkpoint çš„æ€è·¯ï¼š å¹¶ä¸æ˜¯ä¿å­˜æ‰€æœ‰æ¿€æ´»å€¼ï¼Œè€Œæ˜¯åªåœ¨éƒ¨åˆ†å…³é”®èŠ‚ç‚¹ï¼ˆcheckpointï¼‰ä¿å­˜æ¿€æ´»ã€‚ å¯¹äºæœªä¿å­˜çš„æ¿€æ´»å€¼ï¼Œåœ¨åå‘ä¼ æ’­æ—¶é‡æ–°å†è·‘ä¸€æ¬¡å‰å‘è®¡ç®—æ¥å¾—åˆ°å®ƒä»¬ï¼Œä»è€ŒèŠ‚çœæ˜¾å­˜ã€‚\næ¢å¥è¯è¯´ï¼šç”¨è®¡ç®—æ¢æ˜¾å­˜ã€‚\nğŸ”¹ å·¥ä½œæœºåˆ¶ 1. åœ¨å‰å‘ä¼ æ’­æ—¶ï¼š â€¢ æ¨¡å‹è¢«åˆ‡åˆ†æˆè‹¥å¹²å—ï¼ˆsegmentsï¼‰ã€‚ â€¢ åªä¿å­˜æ¯ä¸€å—çš„è¾“å…¥ï¼Œä¸¢å¼ƒä¸­é—´çš„æ¿€æ´»ã€‚ 2. åœ¨åå‘ä¼ æ’­æ—¶ï¼š â€¢ éœ€è¦ç”¨åˆ°æ¢¯åº¦æ—¶ï¼Œé‡æ–°å¯¹é‚£ä¸€å—åšä¸€æ¬¡ forward æ¥æ¢å¤æ¿€æ´»ã€‚ â€¢ ç„¶åæ­£å¸¸è®¡ç®—æ¢¯åº¦ã€‚\nâ€¢   å¢åŠ è®¡ç®—å¼€é”€ï¼šå› ä¸ºè¦åœ¨ backward æ—¶é‡æ–°åšä¸€æ¬¡ forwardã€‚\nâ€¢   ä¸€èˆ¬ä¼šå¸¦æ¥ 20%ï½30% é¢å¤–çš„è®­ç»ƒæ—¶é—´ã€‚\nimport torch\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        def custom_forward(*inputs):\n            return self.layer2(self.layer1(*inputs))\n        \n        # å¯¹è¿™éƒ¨åˆ†ä½¿ç”¨ checkpoint\n        x = checkpoint(custom_forward, x)\n        return x\n\nğŸ“ TAKEAWAY  Gradient Checkpointing æ˜¯ä¸€ç§ ç”¨é¢å¤–è®¡ç®—æ¢æ˜¾å­˜ çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨å‰å‘ä¼ æ’­æ—¶å°‘å­˜æ¿€æ´»ï¼Œåå‘ä¼ æ’­æ—¶é‡ç®—ï¼Œèƒ½è®©å¤§æ¨¡å‹åœ¨æœ‰é™æ˜¾å­˜ä¸‹å®Œæˆè®­ç»ƒã€‚",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html",
    "href": "posts/05-dino/DINO.html",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "",
    "text": "åœ¨æˆ‘ä»¬PwCçš„ç¬¬äºŒç¯‡ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä»€ä¹ˆæ˜¯Vision Transformer ä»¥åŠå®ƒçš„åŸºæœ¬æ¶æ„å’Œå·¥ä½œåŸç†ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº† ViT ç›¸å¯¹äºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚ ä½†æ˜¯ViTå­˜åœ¨çš„ä¸»è¦é—®é¢˜æ˜¯éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼š\næˆ‘ä»¬åœ¨è®­ç»ƒCIFAR-10 Classificationé—®é¢˜æ—¶ä¹Ÿå¯ä»¥å‘ç°ï¼ŒViTåœ¨å…¶å‡†ç¡®åº¦åªæœ‰ 55% å·¦å³ã€‚ å…¶ä¸»è¦çš„åŸå› æ˜¯ï¼šViT æ²¡æœ‰ CNN çš„ inductive biasï¼ˆå·ç§¯çš„å¹³ç§»ä¸å˜æ€§ã€å±€éƒ¨æ„Ÿå—é‡ï¼‰ï¼Œéœ€è¦æ›´å¤šæ ‡æ³¨æ¥â€œå­¦ä¼šâ€è¿™äº›å…ˆéªŒã€‚\nè€ŒDINO (Caron et al. 2021) çš„æå‡ºï¼Œæ—¨åœ¨è§£å†³â€œåœ¨æ²¡æœ‰æ ‡ç­¾æ•°æ®æƒ…å†µä¸‹ï¼Œè®­ç»ƒ Vision Transformerï¼ˆViTï¼‰æå–æœ‰åŒºåˆ†æ€§ã€è¯­ä¹‰ä¸°å¯Œçš„è§†è§‰è¡¨ç¤ºâ€çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•æ—¨åœ¨åˆ©ç”¨Self-Supervised Learningæ–¹å¼ï¼Œé€šè¿‡æ¨¡å‹è‡ªèº«è’¸é¦å®ç°æœ‰æ•ˆé¢„è®­ç»ƒã€‚\nåœ¨æˆ‘ä»¬äº†è§£DINOä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£å‡ ä¸ªåŸºæœ¬æ¦‚å¿µï¼Œä»¥ä¾¿æˆ‘ä»¬æ›´å¥½çš„ç†è§£DINOã€‚",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#self-supervised-learningssl",
    "href": "posts/05-dino/DINO.html#self-supervised-learningssl",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.1 Self-Supervised Learning(SSL)",
    "text": "1.1 Self-Supervised Learning(SSL)\nSelf-Supervised Learningæ˜¯ä¸€ç§Un-supervised learningçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æœªæ ‡è®°æ•°æ®ä¸­çš„ç»“æ„ä¿¡æ¯æ¥è¿›è¡Œç‰¹å¾å­¦ä¹ ã€‚å®ƒé€šå¸¸é€šè¿‡è®¾è®¡é¢„æ–‡æœ¬ä»»åŠ¡ï¼ˆpretext tasksï¼‰æ¥å®ç°ã€‚é¢„æ–‡æœ¬ä»»åŠ¡æŒ‡çš„æ˜¯ï¼šåœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œäººä¸ºè®¾è®¡ä¸€ä¸ªâ€œä¼ªä»»åŠ¡â€ï¼Œè®©æ¨¡å‹é€šè¿‡è§£å†³è¿™ä¸ªä»»åŠ¡æ¥å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºã€‚ä¸¾ä¸ªä¾‹å­ï¼Œåœ¨ Language Modelä¸­ï¼ŒGPT-2(Radford et al., n.d.) é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼ˆNext Token Predictionï¼‰æ¥å­¦ä¹ è¯­è¨€è¡¨ç¤ºï¼Œè€ŒBERT(Devlin et al. 2019) é€šè¿‡æ©ç è¯­è¨€æ¨¡å‹ä»»åŠ¡ï¼ˆMasked Language Modelingï¼‰æ¥å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚è€Œå¯¹äºå›¾ç‰‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡\n\né®æŒ¡å›¾ç‰‡çš„åŒºåŸŸï¼ˆInpaintingï¼‰ï¼Œ æ¥è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ã€‚\næ‹¼å›¾ä»»åŠ¡ï¼ˆJigsaw Puzzleï¼‰ï¼šæŠŠå›¾ç‰‡åˆ†å‰²æˆå°å—ï¼Œæ‰“ä¹±é¡ºåºï¼Œè®©æ¨¡å‹é‡æ–°æ’åˆ—ã€‚\n\n\nä¸€å¥è¯æ€»ç»“Self-Supervised Learningå°±æ˜¯: è®©æ¨¡å‹å…ˆç©ä¸€äº›è‡ªå¸¦æ ‡ç­¾çš„å°ä»»åŠ¡ï¼Œä»è€Œå­¦ä¼šç†è§£æ•°æ®ã€‚",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#knowledge-distillation",
    "href": "posts/05-dino/DINO.html#knowledge-distillation",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.2 Knowledge Distillation",
    "text": "1.2 Knowledge Distillation\nçŸ¥è¯†è’¸é¦(Hinton, Vinyals, and Dean 2015)æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œé€šè¿‡å°†ä¸€ä¸ªå¤§å‹æ¨¡å‹ï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰çš„çŸ¥è¯†è½¬ç§»åˆ°ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰ä¸­ï¼Œä»è€Œæé«˜å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œé€šå¸¸æœ‰ä»¥ä¸‹å‡ ä¸ªç‰¹ç‚¹ï¼š\n\nTeacher æ¨¡å‹é€šå¸¸å®¹é‡å¤§ã€æ€§èƒ½å¥½ï¼Œä½†è®¡ç®—å¼€é”€é«˜ã€‚\nStudent æ¨¡å‹è¾ƒå°ï¼Œä½†é€šè¿‡å­¦ä¹  Teacher çš„çŸ¥è¯†ï¼Œå¯ä»¥åœ¨ä½æˆæœ¬ä¸‹æ¥è¿‘ Teacher çš„æ€§èƒ½ã€‚\n\nçŸ¥è¯†è’¸é¦æ ¸å¿ƒæ€æƒ³æ˜¯ï¼ŒTeacher è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ (soft targets) å¾€å¾€åŒ…å«ç€æ¯”ç¡¬æ ‡ç­¾ï¼ˆHard Labelï¼‰æ›´å¤šçš„ä¿¡æ¯ï¼Œæ¯”å¦‚ç±»é—´ç›¸ä¼¼æ€§ã€‚å…¶æŸå¤±å‡½æ•°å®šä¹‰ä¸º\n\\[\n\\mathcal{L}_{KD} =\n(1-\\lambda)\\,\n\\underbrace{\\mathcal{L}_{CE}(y, p_s)}_{\\text{Hard Label Loss}} +\n\\lambda \\, T^2 \\,\n\\underbrace{\\mathcal{L}_{KL}(p_t^T, p_s^T)}_{\\text{Soft Label Loss}}\n\\tag{1}\\]\nå…¶ä¸­ï¼š\n\n\\(p_s, p_t\\): æ˜¯ student/teacher çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒã€‚\n\\(T\\): æ˜¯æ¸©åº¦å‚æ•°ï¼Œç”¨äºæ§åˆ¶softmaxçš„å¹³æ»‘ç¨‹åº¦ã€‚\n\\(\\lambda\\): æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œç”¨äºå¹³è¡¡ä¸¤ç§æŸå¤±çš„è´¡çŒ®ã€‚\n\\(\\mathcal{L}_{CE}\\): æ˜¯äº¤å‰ç†µæŸå¤±å‡½æ•°, ç”¨äºè¡¡é‡å­¦ç”Ÿæ¨¡å‹è¾“å‡ºä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®è·ã€‚ \\[\n\\mathcal{L}_{CE}(y, p_s) = -\\sum_{i} y_i \\log(p_{s,i})\n\\tag{2}\\]\n\\(\\mathcal{L}_{KL}\\): æ˜¯Kullback-Leibleræ•£åº¦æŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡æ•™å¸ˆæ¨¡å‹è¾“å‡ºä¸å­¦ç”Ÿæ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®è·ã€‚\n\n\\[\n\\mathcal{L}_{KL}(p_t, p_s) = \\sum_{i} p_t(i) \\log\\frac{p_t(i)}{p_s(i)}\n\\tag{3}\\]\n\nTakeaway 2 çŸ¥è¯†è’¸é¦(Knowledge Distillation)æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©(Model Compression)æ–¹æ³•ï¼Œé€šè¿‡è®©å°æ¨¡å‹ï¼ˆstudentï¼‰å­¦ä¹ å¤§æ¨¡å‹ï¼ˆteacherï¼‰çš„è¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œç»§æ‰¿å…¶çŸ¥è¯†å¹¶æå‡å°æ¨¡å‹æ€§èƒ½ã€‚",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#exponential-moving-average-ema",
    "href": "posts/05-dino/DINO.html#exponential-moving-average-ema",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.3 Exponential Moving Average (EMA)",
    "text": "1.3 Exponential Moving Average (EMA)\næŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ˜¯ä¸€ç§ Weight Average çš„æ–¹æ³•ã€‚åœ¨DINOä¸­ï¼ŒEMAè¢«ç”¨æ¥æ›´æ–°æ•™å¸ˆç½‘ç»œçš„å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ ç¨³å®šã€‚å…·ä½“æ¥è¯´ï¼Œæ•™å¸ˆç½‘ç»œçš„å‚æ•°æ˜¯å­¦ç”Ÿç½‘ç»œå‚æ•°çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œè¿™æ ·å¯ä»¥é¿å…è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‰§çƒˆæ³¢åŠ¨ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚\n\\[\nx^{\\text{EMA}}_{t+1} = \\alpha x^{\\text{EMA}}_t + (1 - \\alpha) x_{t+1}, \\quad \\text{where} \\ \\alpha \\in [0, 1]\n\\tag{4}\\]\nEMA é€šå¸¸ç”¨æ¥æå‡æ¨¡å‹çš„generalizationèƒ½åŠ›ã€‚\n\nTakeaway 3 æŒ‡æ•°ç§»åŠ¨å¹³å‡(Exponential Moving Average)é€šè¿‡ç»™æ–°æ•°æ®æ›´é«˜æƒé‡ã€æ—§æ•°æ®æŒ‡æ•°è¡°å‡æ¥å¹³æ»‘æ›´æ–°ï¼Œä»è€Œè·å¾—æ›´ç¨³å®šçš„å‚æ•°æˆ–ä¿¡å·ã€‚\n\næœ‰äº†è¿™äº›å‚¨å¤‡çŸ¥è¯†ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹çœ‹DINOæ˜¯å¦‚ä½•åˆ©ç”¨è¿™äº›çŸ¥è¯†æ¥è®­ç»ƒæ¨¡å‹çš„ã€‚",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#æ•°æ®å¤„ç†",
    "href": "posts/05-dino/DINO.html#æ•°æ®å¤„ç†",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.1 æ•°æ®å¤„ç†",
    "text": "7.1 æ•°æ®å¤„ç†\nDINO åœ¨æ•°æ®å¤„ç†ä¸Šé‡‡ç”¨äº†å¤šè§†å›¾å¢å¼ºç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸€å¼ è¾“å…¥å›¾åƒï¼ŒDINO ä¼šç”Ÿæˆå¤šä¸ªä¸åŒçš„è§†å›¾ï¼Œè¿™äº›è§†å›¾é€šè¿‡ä¸åŒçš„æ•°æ®å¢å¼ºæŠ€æœ¯è·å¾—ï¼Œä¾‹å¦‚éšæœºè£å‰ªã€é¢œè‰²æŠ–åŠ¨ç­‰ã€‚è¿™äº›å¢å¼ºè§†å›¾å°†ä½œä¸º Student ç½‘ç»œçš„è¾“å…¥ã€‚\nåŒæ—¶ï¼ŒTeacher ç½‘ç»œåˆ™åªä½¿ç”¨å…¨å±€è§†å›¾ï¼Œå³å¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°å…¨å±€ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒStudent ç½‘ç»œå¯ä»¥å­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„å±€éƒ¨ç‰¹å¾ï¼ŒåŒæ—¶ä¹Ÿèƒ½ä¸ Teacher ç½‘ç»œçš„å…¨å±€ç‰¹å¾è¿›è¡Œå¯¹é½ã€‚\n\n\n\nå¯¹äºä¸€å¼ ç…§ç‰‡ï¼ŒDINOè¿›è¡Œä»¥ä¸‹å‡ ä¸ªæ“ä½œï¼š\n\né¢œè‰²æŠ–åŠ¨ï¼ˆColor Jitterï¼‰\né«˜æ–¯æ¨¡ç³Šï¼ˆGaussian Blurï¼‰\nå¤ªé˜³åŒ–ï¼ˆSolarizationï¼‰\n\nå¹¶ä¸”å¯¹æ¯ä¸ªè§†å›¾ï¼Œè¿›è¡ŒMulti-Cropçš„ç­–ç•¥ï¼š\n\nç”Ÿæˆ ä¸¤ä¸ª global cropï¼ˆå¤§è§†é‡ï¼‰ï¼Œ\nåŠ è‹¥å¹²ä¸ª local cropï¼ˆå°è§†é‡ï¼‰ï¼Œ\nç„¶åæ‰€æœ‰è£å‰ªè§†å›¾éƒ½è¾“å…¥åˆ° å­¦ç”Ÿç½‘ç»œ\nåªæœ‰ global views è¾“å…¥åˆ° æ•™å¸ˆç½‘ç»œ\n\n\n\n\n\n\n \n\n\nFigureÂ 2: DINO Image Pre-process Steps\n\n\n\n\n\n\nä¸‹é¢æ˜¯å‡ ä¸ªData Augmentationçš„ä¾‹å­\n\n\n\n\n\n\n\n\n\n\n\n(a) Original Image\n\n\n\n\n\n\n\n\n\n\n\n(b) Gaussian Blur\n\n\n\n\n\n\n\n\n\n\n\n(c) Solarized\n\n\n\n\n\n\n\nFigureÂ 3: ä¸¤ä¸ªä¸åŒçš„Data Augmentation Examples\n\n\n\nThe full augmentation pipeline\n\n\n\n\n\n\nFigureÂ 4: The Data Augmentation Pipeline",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#dino-v2",
    "href": "posts/05-dino/DINO.html#dino-v2",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.2 DINO V2",
    "text": "7.2 DINO V2\n\n\n\n\n\n\nFigureÂ 5\n\n\n\nâ€¢ DINO 1ï¼šæ„å»ºåŸºç¡€æ¡†æ¶ï¼Œå¼€å¯è‡ªç›‘ç£å­¦ä¹ åœ¨è§†è§‰ Transformer çš„åº”ç”¨ã€‚ â€¢ DINO 2ï¼šå…¨é¢æ‰©å±•è®­ç»ƒè§„æ¨¡ä¸æŠ€æœ¯ï¼Œä½¿æ¨¡å‹æˆä¸ºçœŸæ­£â€œå¼€ç®±å³ç”¨â€çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚ â€¢ DINO 3ï¼šè¿›ä¸€æ­¥æå‡è§„æ¨¡ä¸æŠ€æœ¯ï¼Œè§£å†³å¯†é›†ç‰¹å¾é€€åŒ–é—®é¢˜ï¼Œå¢å¼ºå¤šåœºæ™¯é€‚åº”æ€§ï¼Œå¹¶æ¨åŠ¨æ€§èƒ½è‡³æ–°é«˜åº¦ã€‚",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#dino-v3",
    "href": "posts/05-dino/DINO.html#dino-v3",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.3 DINO V3",
    "text": "7.3 DINO V3",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#dino-v1-vs.-dino-v2-vs.-dino-v3",
    "href": "posts/05-dino/DINO.html#dino-v1-vs.-dino-v2-vs.-dino-v3",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.4 DINO V1 vs.Â DINO V2 vs.Â DINO V3",
    "text": "7.4 DINO V1 vs.Â DINO V2 vs.Â DINO V3\n\n\nSummary of 4 Tokenization Algorithms\n\n\n\n\n\n\n\n\nDINO Version\næ ¸å¿ƒåˆ›æ–°ä¸æ”¹è¿›\næ•°æ®è§„æ¨¡ä¸æ¨¡å‹è§„æ¨¡\nåº”ç”¨ç‰¹æ€§ä¸ä¼˜åŠ¿\n\n\n\n\nDINO 1\nstudentâ€“teacher è‡ªç›‘ç£è’¸é¦ï¼Œattention å¯è§†åŒ–\nä¸­å°è§„æ¨¡æ•°æ®ä¸æ¨¡å‹\nå¯è§†åŒ–ç‰¹å¾å­¦ä¹ ã€è¯­ä¹‰åˆ†å‰²èƒ½åŠ›ã€ä½æˆæœ¬è®­ç»ƒ\n\n\nDINO 2\nå¤§è§„æ¨¡è’¸é¦ã€FlashAttentionã€æ­£åˆ™åŒ–ã€è¶…å¼ºæ³›åŒ–èƒ½åŠ›\n1.42 äº¿å›¾åƒè®­ç»ƒæ•°æ®ã€å¤šä¸ª ViT æ¶æ„\nå¤šä»»åŠ¡é€šç”¨ï¼Œæ— éœ€å¾®è°ƒï¼Œä»»åŠ¡è¦†ç›–å¹¿æ³›\n\n\nDINO 3\nGram anchoringã€è½´å‘ RoPEã€å¤šåˆ†è¾¨ç‡é²æ£’ã€å¤šæ¨¡å‹ç‰ˆæœ¬\n7B å‚æ•°æ¨¡å‹ + è¶…å¤§æ•°æ®ï¼ˆ1.7B å›¾åƒï¼‰\næ›´å¼ºå¯†é›†ç‰¹å¾è´¨é‡ï¼Œå¤šä»»åŠ¡æ€§èƒ½è¾¾æ–° SOTAï¼Œé€‚åº”æ€§æ›´å¼º\n\n\n\n\n\nQuestion: Answer: å› ä¸ºç¼ºå°‘ CNN çš„ inductive biasï¼ŒViT åªèƒ½ä¾é å¤§æ•°æ®æ¥å­¦ä¹ ç©ºé—´ä¸å˜æ€§å’Œå±€éƒ¨æ¨¡å¼ã€‚\n\n\nAnswer: åˆ©ç”¨æ•™å¸ˆâ€“å­¦ç”Ÿè’¸é¦ + å›¾åƒå¢å¼ºè§†å›¾ï¼Œå­¦ç”Ÿæ¨¡ä»¿æ•™å¸ˆè¾“å‡ºï¼Œä»è€Œå­¦ä¹ é²æ£’ç‰¹å¾ã€‚\n\n\nAnswer: è®©å­¦ç”Ÿå­¦ä¹ å±€éƒ¨ä¸å…¨å±€çš„è¯­ä¹‰å¯¹é½ï¼Œä»è€Œè·å¾—æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚\n\n\nAnswer: ViT çš„ self-attention åœ¨è‡ªç›‘ç£ä¸­ä¼šè‡ªç„¶èšåˆè¯­ä¹‰ä¸€è‡´çš„åŒºåŸŸï¼Œä»è€Œè¡¨ç°å‡ºå¯¹è±¡åˆ†å‰²ç°è±¡ã€‚\n\n\nCentering prevents one dimension to dominate but encourages collapse to the uniform distribution, while the sharpening has the opposite effect.  Emerging Properties in Self-Supervised Vision Transformers, p.4 \n\n\nOutput sharpening is obtained by using a low value for the temperature Ï„t in the teacher softmax normalization.  Emerging Properties in Self-Supervised Vision Transformers, p.4",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#safe-softmax",
    "href": "posts/05-dino/DINO.html#safe-softmax",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "8.1 Safe-Softmax",
    "text": "8.1 Safe-Softmax\ndef softmax(x: torch.Tensor, temp):\n    max =",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  }
]