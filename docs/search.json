[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This website aims to collect and showcase 100 important AI papers and their code implementations. Each paper is accompanied by a link for readers to explore further.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This website aims to collect and showcase 100 important AI papers and their code implementations. Each paper is accompanied by a link for readers to explore further.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#why-build-this-website",
    "href": "index.html#why-build-this-website",
    "title": "100 Papers with Code",
    "section": "Why build this website?",
    "text": "Why build this website?\nOn the one head, during my study, reading, and practice in the field of AI, I found that many important papers and code implementations were scattered in various places. To facilitate access for myself and others, I decided to build this website to consolidate these important papers and my implementations in one place. On the other hand, there is a gap between topic in papers and practical application. By providing code implementations, I hope to help bridge this gap and make it easier for practitioners to apply the latest research results.\nThere are some awesome resource about the paper and implementation such as:\n\nAnnotated Research Paper: Collection of simple PyTorch implementations of neural networks and related algorithms.\nPapers with Code (It was replace by Hugging Face now): The largest resource for finding machine learning papers, code and evaluation tables.\n\nBut why I still build this website?\n\nThe Annotated Research Paper focuses more on each component of the paper, it didnâ€™t provide the whole modeling and training process.\nThe Papers with Code provides the paper and code, but the code is sometime hard to understand, and it didnâ€™t provide the explanation of the code.\n\nThis website is built to fill these gaps by providing clear explanations and easy-to-understand code implementations for each paper.Each page(paper) will come with a self-contained Jupyter Notebook that can be run directly, making it easier for readers to understand and apply the concepts presented in the papers.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-website",
    "href": "index.html#how-to-use-this-website",
    "title": "100 Papers with Code",
    "section": "How to use this website?",
    "text": "How to use this website?\nThis website is designed to be user-friendly and accessible for anyone interested in understanding and implementing the latest research in AI and machine learning. Each paper has its own dedicated page that includes\n\nThe explanation of the paper.\nCode implementation.\n\nQ & A part (you can use as flash cards).\nThe further direction might be interesting to explore (According to my understanding).\nBelow is the list of papers that have been implemented or are planned to be documented. You can click on the paper title to view detailed content.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need (Transformer )\nTransformer æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ï¼Œåœ¨è¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ä½œä¸º GPTã€BERT ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒåŸºç¡€ï¼Œæ¨åŠ¨äº†å½“ä»Šç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ã€‚\n\nNLP / Transformer\n\n\n02\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ( Vision Transformer )\nVision Transformer (ViT) æ˜¯ä¸€ç§å°†å›¾åƒåˆ’åˆ†ä¸ºå°å—ï¼ˆpatchesï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸º token è¾“å…¥æ ‡å‡† Transformer æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»çš„æ¶æ„ï¼Œé¦–æ¬¡å®ç°äº†çº¯æ³¨æ„åŠ›æœºåˆ¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨ã€‚\n\nComputer Vision / Transformer\n\n\n03\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows ( Swin Transformer )\nSwin Transformer æ˜¯ä¸€ç§ä½¿ç”¨å±‚æ¬¡åŒ–ç»“æ„å’Œæ»‘åŠ¨çª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è§†è§‰ Transformer æ¨¡å‹ï¼Œæ—¢ä¿ç•™äº†å±€éƒ¨å»ºæ¨¡çš„é«˜æ•ˆæ€§ï¼Œåˆé€šè¿‡çª—å£åç§»å®ç°è·¨åŒºåŸŸä¿¡æ¯äº¤äº’ï¼Œé€‚ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¤šç§è§†è§‰ä»»åŠ¡ã€‚\n\nComputer Vision / Transformer\n\n\n04\nLearning Transferable Visual Models From Natural Language Supervision ( CLIP )\nCLIP æ˜¯ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡å›¾æ–‡å¯¹æ¯”å­¦ä¹ ï¼Œå°†å›¾åƒä¸è‡ªç„¶è¯­è¨€æ˜ å°„åˆ°åŒä¸€è¯­ä¹‰ç©ºé—´ï¼Œä»è€Œå®ç°é›¶æ ·æœ¬å›¾åƒè¯†åˆ«ä¸è·¨æ¨¡æ€æ£€ç´¢çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹\n\nComputer Vision / Transformer\n\n\n05\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness ( Flash Attention )\nFlashAttention æ˜¯ä¸€ç§ä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°ï¼Œé€šè¿‡å‡å°‘å†…å­˜è®¿é—®å’Œæå‡è®¡ç®—æ•ˆç‡ï¼Œå®ç°æ›´å¿«ã€æ›´èŠ‚çœèµ„æºçš„ Transformer æ¨ç†ä¸è®­ç»ƒã€‚\n\nTransformer / AI Engine",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html",
    "href": "posts/02-vision-transformer/Vision-Transformer.html",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "",
    "text": "åœ¨äº†è§£äº†ä»€ä¹ˆæ˜¯Transformerä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å°†Transformeråº”ç”¨äºComputer Visionã€‚Vision Transformerï¼ˆViTï¼‰(Dosovitskiy et al. 2021) æ˜¯ä¸€ä¸ªå°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒåˆ†ç±»çš„æ¨¡å‹ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å›¾åƒåˆ’åˆ†ä¸ºå°å—ï¼ˆpatchesï¼‰ï¼Œç„¶åå°†è¿™äº›å°å—è§†ä¸ºåºåˆ—æ•°æ®ï¼Œç±»ä¼¼äºå¤„ç†æ–‡æœ¬æ•°æ®ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.1 Patch Embedding",
    "text": "1.1 Patch Embedding\n\nThe standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches\\(x \\in \\mathbb{R}^{N \\times (P^{2} \\times C)}\\), where (\\(H, W\\)) is the resolution of the original image, \\(C\\) is the number of channels, (\\(P, P\\)) is the resolution of each image patch, and \\(N = HW/P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size \\(D\\) through all of its layers, so we flatten the patches and map to \\(D\\) dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nåœ¨Transformer è¿™ä¸€ç¯‡ï¼Œæˆ‘ä»¬äº†è§£åˆ°ï¼Œå®ƒæ˜¯ä½œç”¨äºSequence Modelingçš„ï¼Œå¾ˆæ˜¾ç„¶ï¼ŒImage ä¸æ˜¯ Sequenceçš„ã€‚å¾ˆç›´è§‚çš„ç¬¬ä¸€ç§æƒ³æ³•å°±æ˜¯ï¼Œå°†å›¾ç‰‡ç›´æ¥å±•å¼€ï¼Œä»äºŒç»´ (\\(3, H, W\\)) å±•å¼€æˆä¸€ç»´çš„ (\\(3, H \\times W\\)). è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°çš„å›¾ç‰‡çš„Sequence Modelã€‚å¦‚ä¸‹å›¾@fig-flat-imageæ‰€ç¤º\n\n\n\n\n\n\nFigureÂ 2\n\n\n\nè¿™ç§æ–¹æ³•æœ‰ä¸€ç§æ˜æ˜¾çš„é—®é¢˜å°±æ˜¯ï¼šSequenceçš„é•¿åº¦å¤ªé•¿ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äº \\(3\\times 256 \\times 256\\) çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬æœ‰ \\(256 \\times 256 = 65,336\\) ä¸ªtokensï¼Œé€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ‰€éœ€è¦çš„è®­ç»ƒæ—¶é•¿å¾ˆé•¿ã€‚å¹¶ä¸”å®ƒæ²¡æœ‰ç”¨åˆ°å›¾ç‰‡çš„ä¸€ä¸ªç‰¹æ€§ï¼šç›¸é‚»çš„pixel ä¹‹é—´ï¼Œæ˜¯æœ‰å¾ˆé«˜çš„correlationçš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¾ˆè‡ªç„¶çš„æƒ³åˆ°ï¼šå¦‚æœæŠŠç›¸é‚»çš„pixelså’Œåœ¨ä¸€ç»„ï¼Œç»„æˆä¸€ä¸ªpatchï¼Œè¿™æ ·ä¸å°±æ—¢å‡å°‘äº†tokensçš„æ•°é‡ï¼Œåˆç”¨åˆ°äº†pixelä¹‹é—´çš„correlationã€‚è¿™å°±æ˜¯Vision Transformer çš„Patch Embeddingã€‚ è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†ã€‚ æ¥ä¸‹æ¥æˆ‘ä»¬åªéœ€è¦ç”¨ï¼Œä¸€ä¸ªMLPï¼Œå°†æˆ‘ä»¬å±•å¼€çš„patchï¼Œæ˜ å°„åˆ° \\(D\\)- dimensionçš„ç©ºé—´ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä¼ å…¥Transformer æ¨¡å‹äº†ã€‚\næ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç æ€ä¹ˆå®ç°ï¼š\n# Load Image and resize it to certain size\nimage_path = IMAGE_PATN\nimg_bgr = cv2.imread(image_path)\nimg_resized = cv2.resize(img_bgr, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\nimg = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB) \n\n# Patchify \npatches = einops.rearrange(\n     img, \"(h ph) (w pw) c -&gt; (h w) ph pw c\", ph=PATCH_SIZE, pw=PATCH_SIZE\n)\né€šè¿‡è¿™ä¸ªPatchifyåªæœ‰ï¼Œæˆ‘ä»¬å¾—åˆ°å°†å›¾ç‰‡Patchåˆ°äº†\n\n\nåˆ†æˆäº†ä¸åŒçš„å°Patchã€‚\næ¥ä¸‹æ¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯ï¼Œå°†è¿™äº›Patch å±•å¼€ï¼Œç„¶åä¼ å…¥ä¸€ä¸ªMLPï¼Œ\nflat_patch = einops.rearrange(\n     patches, \"n ph pw c -&gt; n (ph pw c)\"\n)\n\n\nmlp = nn.Linear(PATCH_SIZE * PATCH_SIZE * 3, d_model)\npatch_embedding = mlp(flat_patch)\nåŒè¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å°±å¯ä»¥è§å›¾ç‰‡è½¬åŒ–ä¸ºTransformerå¯ä»¥æ¥å—çš„vectorã€‚ä¸è¿‡åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸ä¼šç”¨ä»¥ä¸Šçš„æ–¹å¼ï¼Œå› ä¸ºä¸Šé¢çš„æ–¹å¼å®ç°èµ·æ¥æ¯”è¾ƒæ…¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†Patch å’Œ Linear Projectå’Œåœ¨ä¸€èµ·ã€‚\n\n\n\n\n\n\nTip\n\n\n\nå°†å‡ ä¸ªtensor çš„operationæ“ä½œåˆæˆä¸€ä¸ªçš„æ–¹æ³•ï¼Œå«åškernel fusionï¼Œè¿™æ˜¯ä¸€ç§æé«˜è®­ç»ƒå’Œæ¨ç†ç´ çš„æ–¹æ³•\n\n\nåœ¨å®é™…çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ç”¨Convolution Layer ä»£æ›¿ Patch + Flatten+ Linear çš„æ–¹æ³•. å¦‚æœæˆ‘ä»¬ç”¨ä¸€ä¸ª å·ç§¯å±‚ï¼Œå‚æ•°è®¾ç½®ä¸ºï¼š â€¢ kernel_size = PATCH_SIZE ï¼ˆå·ç§¯æ ¸è¦†ç›–ä¸€ä¸ª patchï¼‰ â€¢ stride = PATCH_SIZE ï¼ˆä¸é‡å åœ°ç§»åŠ¨ï¼Œç›¸å½“äºåˆ‡ patchï¼‰ â€¢ in_channels = 3ï¼ˆRGBï¼‰ â€¢ out_channels = d_model\né‚£ä¹ˆå·ç§¯ä¼šï¼š 1. æŠŠè¾“å…¥å›¾ç‰‡åˆ†æˆ PATCH_SIZE x PATCH_SIZE çš„ä¸é‡å å—ï¼ˆå› ä¸º stride = kernel_sizeï¼‰ã€‚ 2. å¯¹æ¯ä¸ª patch åšä¸€æ¬¡çº¿æ€§æ˜ å°„ï¼ˆå› ä¸ºå·ç§¯æœ¬è´¨ä¸Šå°±æ˜¯å¯¹å±€éƒ¨åŒºåŸŸåšåŠ æƒæ±‚å’Œï¼Œç›¸å½“äº Linearï¼‰ã€‚ 3. è¾“å‡ºçš„ shape è‡ªåŠ¨å°±æ˜¯ (batch, num_patches, d_model)ã€‚\nè¿™æ­£å¥½ç­‰ä»·äº åˆ‡ patch + flatten + Linear çš„ç»„åˆã€‚\nä»£ç å¦‚ä¸‹ï¼š\nclass PatchEmbedding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.conv = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=config.hidden_dim,\n            kernel_size=config.patch_size,\n            stride=config.patch_size,\n            padding=\"valid\" if config.patch_size == 16 else \"same\",\n        )\n\n    def forward(self, imgs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        imgs: (batch_size, num_channels, height, width)\n        Returns: (batch_size,  num_patches_height, num_patches_width, hidden_dim)\n        \"\"\"\n        # (B, C, H, W) -&gt; (B, hidden_dim, H', W')\n        x = self.conv(imgs)\n\n        # (B, hidden_dim, H', W') -&gt; (B, hidden_dim, H' * W')\n        x = x.flatten(2)\n\n        # (B, hidden_dim, H' * W') -&gt; (B, H' * W', hidden_dim)\n        x = x.transpose(1, 2)\n        return x\nç”¨å·ç§¯çš„å¥½å¤„ï¼Œé™¤äº†å¯ä»¥æ›´é«˜æ•ˆçš„å®ç°Patch Embeddingï¼Œä»£ç æ›´åŠ ç®€æ´ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡æ”¹å˜ stride æ¥ä½¿ä¸€äº›Patch overlappingï¼Œè·å¾—ä¸€ä¸ªå¤šå°ºåº¦çš„ç»“æ„ï¼Œ\nThe image is convert along this process: \\[\n\\boxed{\n\\mathbf{x} \\in \\mathbb{R}^{C \\times H \\times W}\n\\quad \\xrightarrow{\\text{Patchify}} \\quad\n\\{ x_i \\in \\mathbb{R}^{C \\times P \\times P} \\}{i=1}^N\n\\quad \\xrightarrow{\\text{Flatten}} \\quad\n\\{ x_i \\in \\mathbb{R}^{(C \\cdot P \\cdot P)} \\}_{i=1}^N\n\\quad \\xrightarrow{\\text{Linear } W \\in \\mathbb{R}^{(C \\cdot P \\cdot P) \\times D}} \\quad\n\\{ z_i \\in \\mathbb{R}^{D} \\}_{i=1}^N\n}\n\\tag{1}\\]",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.2 Position Encoding",
    "text": "1.2 Position Encoding\nå°†å›¾ç‰‡è½¬åŒ–ä¸º Transformer çš„è¾“å…¥ä¹‹åï¼Œæ¥ä¸‹æ¥Transformerä¸­çš„å¦ä¸€ä¸ªç»„ä»¶å°±æ˜¯ä¼ å…¥ Position Informationã€‚æˆ‘ä»¬çŸ¥é“åœ¨Transformer ä¸­ï¼Œä»–ä»¬ç”¨çš„æ˜¯ sine-cosine position embeddingï¼Œåœ¨é‚£ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæåˆ°äº†ï¼Œè¿˜å­˜åœ¨å…¶ä»–ä¸åŒçš„Position Encodingçš„åŠæ³•ï¼ŒViT ç”¨çš„å°±æ˜¯å¦ä¸€ç§åŠæ³•ï¼ŒLearned Position Embeddingã€‚Learned Position Embeddingçš„æ–¹æ³•å¾ˆç®€å•ï¼Œä¹Ÿå¾ˆå¥½ç†è§£ï¼Œå¯¹äºæ¯ä¸€ä¸ªä½ç½®ï¼Œæˆ‘ä»¬ç»™ä»–ä¸€ä¸ªindexï¼Œå°†è¿™ä¸ªindexä¼ å…¥ä¸€ä¸ª Embedding Matrixï¼Œ æˆ‘ä»¬å°±å¾—åˆ°ä¸€ä¸ªPosition Embeddingã€‚ä¸è¿‡ä¸Token Embeddingä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬ä¼šç”¨åˆ°æ‰€æœ‰çš„Positionï¼Œä¹Ÿæ•´ä¸ªmatrixï¼Œ æ‰€ä»¥æˆ‘ä»¬ä¸ç”¨å®šindexï¼Œç›´æ¥å®šä¹‰æ•´ä¸ªEmbeddingï¼Œç„¶åå°†å®ƒä¼ å…¥Transformerä¸­ã€‚\nclass PositionalEncoding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.positional_embedding = nn.Parameter(\n            torch.randn(\n                1,\n                (config.image_size // config.patch_size) ** 2 + 1,\n                config.hidden_dim,\n            )\n        )\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Add positional encoding to the input tensor\n        batch_size = x.size(0)\n\n        pos_embedding = self.positional_embedding.expand(batch_size, -1, -1)\n        cls_token = self.cls_token.expand(batch_size, -1, -1)\n\n        x = torch.cat((cls_token, x), dim=1)\n        return x + pos_embedding\nä¸ºä»€ä¹ˆViTè¦ç”¨Learned Position Embeddingå‘¢ï¼Ÿåœ¨ViTè¿™ç¯‡æ–‡ç« ä¸­ï¼Œä»–ä»¬å°è¯•è¿‡ä¸åŒçš„Position Embeddingï¼Œæ¯”å¦‚ï¼š\n\nNo Positional Information\n1-dimensional Positional Embedding\n2-dimensional Positional Embedding\nRelative Positional Embedding\n\nå‘ç°ï¼Œé™¤äº†No Positional Informationä¹‹å¤–ï¼Œå…¶ä½™3ç§åœ¨Image Classificationä¸­çš„è¡¨ç°ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚\n\n\n\n\n\n\nFigureÂ 3\n\n\n\nè®ºæ–‡ä¸­è¡¨ç¤ºï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‰€éœ€è¦çš„ Positionçš„ä¿¡æ¯è¾ƒå°ï¼Œå¯¹äºä¸åŒç§ç±»çš„Position Embeddingçš„æ–¹æ³•ï¼Œå­¦ä¹ è¿™ä¸ªPosition Informationçš„èƒ½åŠ›ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚\n\nWe speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., \\(14 \\times 14\\) as opposed to \\(224 \\times 224\\), and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.18 \n\nä¸è¿‡ï¼Œå°½ç®¡Positionçš„æ–¹æ³•ä¸é‡è¦ï¼Œä½†æ˜¯ä¸åŒçš„è®­ç»ƒå‚æ•°ï¼Œè¿˜æ˜¯ä¼šå½±å“åˆ°å­¦ä¹ åˆ°çš„Position Information, ä¸‹å›¾æ‰€ç¤ºï¼š\n\n\n\n\n\n\nFigureÂ 4\n\n\n\n\n1.2.1 Extending Position Encoding\nå½“æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªPre-Trainingçš„æ¨¡å‹ï¼Œæˆ‘ä»¬æƒ³ç”¨å®ƒFine-Tuningåˆ°ä¸€ä¸ªä¸åŒå›¾ç‰‡å¤§å°çš„æ•°æ®åº“ï¼Œæˆ‘ä»¬æ”¹æ€ä¹ˆåšå‘¢ï¼Œç¬¬ä¸€ä¸ªæ–¹æ³•å½“ç„¶æ˜¯ï¼ŒResize æˆ‘ä»¬çš„å›¾ç‰‡ï¼Œåˆ°ViT Pre-trainingçš„å›¾ç‰‡å¤§å°ï¼Œä½†æ˜¯ï¼Œè¿™ä¸ªèƒ½å¯¼è‡´è¾ƒå¤§çš„å›¾ç‰‡ï¼Œå¤±å»å¾ˆå¤šç»†èŠ‚ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿æŒå›¾ç‰‡çš„å¤§å°ä¸å˜ï¼ŒåŒæ—¶è®©æ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬å°±éœ€è¦Extend Position Encodingï¼Œå› ä¸ºå½“Patch Sizeä¸å˜ï¼Œå›¾ç‰‡å¤§å°å˜äº†çš„è¯ï¼Œäº§ç”Ÿçš„Number of Patches ä¹Ÿæ˜¯ä¼šæ”¹å˜çš„ï¼Œè¿™æ ·ï¼Œå°±æ˜¯æŸå¤±ä¸€äº›ä¿¡æ¯ã€‚æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼Œæ‰¾åˆ°ä¸€ç§æ–¹æ³•ï¼Œå¢å¤§æˆ–è€…å‡å°Positionçš„æ•°é‡ã€‚ è¿™å°±æ˜¯æ‰€è°“çš„Position Interpolationã€‚\n2D interpolation of the pre-trained position embeddings â€¢ ViT åœ¨é¢„è®­ç»ƒæ—¶ï¼Œé€šå¸¸ç”¨å›ºå®šè¾“å…¥åˆ†è¾¨ç‡ï¼ˆæ¯”å¦‚ 224Ã—224ï¼‰ â†’ ç”Ÿæˆå›ºå®šæ•°é‡çš„ patchï¼ˆæ¯”å¦‚ 16Ã—16 patch â†’ 196 ä¸ª patchï¼‰ã€‚ â€¢ ä½†åœ¨ fine-tuning æ—¶ï¼Œè¾“å…¥å›¾ç‰‡å¯èƒ½å¤§å°ä¸ä¸€æ ·ï¼Œæ¯”å¦‚ 384Ã—384ï¼Œè¿™æ—¶ patch æ•°é‡å°±å˜äº†ã€‚ â€¢ è¿™ä¼šå¯¼è‡´åŸæœ¬çš„ ä½ç½®ç¼–ç  (position embeddings) å’Œæ–°çš„ patch æ•°é‡å¯¹ä¸ä¸Šã€‚ â€¢ è§£å†³åŠæ³•ï¼šå¯¹é¢„è®­ç»ƒå¥½çš„ä½ç½®ç¼–ç åš äºŒç»´æ’å€¼ (2D interpolation)ï¼Œæ ¹æ® patch åœ¨åŸå›¾ä¸­çš„ç©ºé—´ä½ç½®ï¼ŒæŠŠä½ç½®ç¼–ç æ‹‰ä¼¸/ç¼©æ”¾åˆ°æ–°çš„åˆ†è¾¨ç‡ã€‚\n\nThe Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.4 \n\ndef interpolate_pos_encoding(self, x, w, h):\n    npatch = x.shape[1] - 1\n    N = self.pos_embed.shape[1] - 1\n    if npatch == N and w == h:\n        return self.pos_embed\n    class_pos_embed = self.pos_embed[:, 0]\n    patch_pos_embed = self.pos_embed[:, 1:]\n    dim = x.shape[-1]\n    w0 = w // self.patch_embed.patch_size\n    h0 = h // self.patch_embed.patch_size\n    \n    patch_pos_embed = F.interpolate(\n        patch_pos_embed.reshape(\n            1, \n            int(math.sqrt(N)), \n            int(math.sqrt(N)), \n            dim\n        ).permute(0, 3, 1, 2),\n        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n        mode='bicubic',\n    )\n    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.3 [CLS] Tokens & MLP Head",
    "text": "1.3 [CLS] Tokens & MLP Head\nåœ¨ Transformer è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬äº†è§£åˆ°ï¼šæ¯è¾“å…¥ä¸€ä¸ªtokenï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„tokenã€‚è¿™å°±æ˜¯è¯´ï¼Œå¯¹äºæ¯ä¸ªpatchï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„Tokensï¼Œé‚£ä¹ˆï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©å“ªä¸€ä¸ªtokenä½œä¸ºæˆ‘ä»¬å›¾ç‰‡çš„è¡¨ç¤ºå‘¢ã€‚ BERT (Devlin et al. 2019)ï¼Œ ç”¨äº†ä¸€ä¸ª [CLS], æ¥è¡¨ç¤ºä¸€ä¸ªå¥å­ã€‚åŒç†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ·»åŠ ä¸€ä¸ª [CLS] token, æ¥è¡¨ç¤ºä¸€å¼ å›¾ç‰‡ã€‚åŒæ—¶ï¼Œå¯¹äº [CLS] token, æˆ‘ä»¬ä¹Ÿè¦åœ¨ç»™ä»–ä¸€ä¸ªè¡¨ç¤ºä½ç½®çš„ä¿¡æ¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨Position Encodingä¸Šï¼Œæˆ‘ä»¬æœ‰ (config.image_size // config.patch_size) ** 2 + 1, ä½ç½®ä¿¡æ¯ï¼Œå…¶ä¸­ +1 å°±æ˜¯ [CLS] çš„ä½ç½®ä¿¡æ¯ã€‚ æ€»ç»“ä¸€ä¸‹ [CLS] token çš„ä½œç”¨å°±æ˜¯ç”¨æ¥èšåˆæ‰€æœ‰çš„Patchçš„æ¶ˆæ¯ï¼Œç„¶åç”¨æ¥Image çš„Representationã€‚\næˆ‘ä»¬æƒ³ä¸€ä¸‹ï¼Œé™¤äº†åŠ ä¸€ä¸ª [CLS] tokenï¼Œä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰å…¶ä»–åŠæ³•æ¥è¡¨ç¤ºå›¾ç‰‡å—ã€‚æœ‰ä¸€ç§å¾ˆè‡ªç„¶çš„æ–¹æ³•å°±æ˜¯ï¼Œå°†æ‰€æœ‰çš„patchçš„æ¶ˆæ¯æ”¶é›†èµ·æ¥ï¼Œç„¶åå»ä¸€ä¸ªå¹³å‡å€¼æ¥è¡¨ç¤ºè¿™ä¸ªå›¾ç‰‡ã€‚ç±»ä¼¼äºä¼ ç»Ÿçš„ConvNet(e.g.Â ResNet) æˆ‘ä»¬å¯ä»¥é€šè¿‡ AvgPooling æ¥å®ç°ã€‚ ä¸è¿‡è®ºæ–‡ä¸­æåˆ°ï¼Œ å¯¹äºä¸¤ç§ä¸åŒçš„Image Representationï¼Œéœ€è¦æœ‰ä¸åŒçš„Learning Rate æ¥è®­ç»ƒè¿™ä¸ªç½‘ç»œã€‚\nOther content \næœ‰äº†Image Representä¹‹åï¼Œæˆ‘ä»¬åªéœ€è¦å°†è¿™ä¸ªä¼ å…¥ä¸€ä¸ªç®€å•çš„MLPï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªClassifierã€‚MLPçš„è¾“å…¥æ˜¯hidden dimï¼Œè¾“å…¥åˆ™æ˜¯æˆ‘ä»¬Number of Classesã€‚ä¸åŒçš„Index è¡¨ç¤ºä¸åŒçš„Classsesã€‚\n\nAn initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classifierâ€”just like ResNetâ€™s final feature mapâ€”performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate,  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.17 \n\n\nBoth during pre-training and fine-tuning, a classification head is attached to \\(\\mathrm{z}_{L}^{0}\\). The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nclass ClassifierHead(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n        self.fc2 = nn.Linear(config.mlp_dim, config.num_classes)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_classes)\n        \"\"\"\n        # Use the CLS token for classification\n        cls_token = x[:, 0, :]\n        x = F.relu(self.fc1(cls_token))\n        \n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.4 Transformer Encoder Block",
    "text": "1.4 Transformer Encoder Block\nè‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»è®²å®Œäº†ViTä¸Transformerçš„ä¸»è¦ä¸åŒä¹‹å¤„ã€‚æ¥ä¸‹æ¥ï¼Œå°±æ˜¯Transformerçš„Encoderã€‚ \nè¿™éƒ¨åˆ†ï¼Œå’ŒTransformeråŸæœ¬çš„Encoderå¾ˆç±»ä¼¼ï¼Œåªä¸è¿‡æœ‰å‡ å¤„ä¸åŒï¼š\n\nPre-Norm: åœ¨ViTåŒï¼Œè¾“å…¥å…ˆè¿›è¡Œä¸€ä¸ªLayerNormï¼Œç„¶ååœ¨ä¼ å…¥MHAæˆ–è€…MLPä¸­ï¼Œåè§‚åœ¨TransformeråŸæœ¬çš„Encoderä¸­ï¼Œæˆ‘ä»¬æ˜¯å…ˆå°†MHAæˆ–è€…MLPçš„è¾“å‡ºä¸è¾“å…¥åŠ åœ¨ä¸€èµ·ï¼Œä¹‹åå†è¿›è¡Œä¸€ä¸ªNormalizationã€‚è¿™å«åšPost-Norm\nMLPçš„å®ç°ï¼šåœ¨Transformer Encoderä¸­ï¼Œç”¨çš„æ˜¯ ReLU, è€Œåœ¨ViTä¸­ï¼Œç”¨çš„æ˜¯ GELU\n\né™¤æ­¤ä¹‹å¤–ï¼Œå…¶ä»–éƒ¨åˆ†éƒ½æ˜¯ä¸€æ ·çš„ã€‚ä¸€ä¸‹æ˜¯ViT Encoderçš„å®ç°ï¼š\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.norm1 = LayerNorm(config.hidden_dim)\n        self.mha = MHA(config)\n        \n        self.norm2 = LayerNorm(config.hidden_dim)\n        self.ffn = FFN(config)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Multi-head attention\n        redisual = x\n        x = self.norm1(x)\n        x = redisual + self.mha(x)\n\n        # Feed-forward network\n        redisual = x\n        x = self.norm2(x)\n        x = x + self.ffn(x)\n\n        return x",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.5 CNN vs.Â ViTï¼š Inductive bias",
    "text": "1.5 CNN vs.Â ViTï¼š Inductive bias\nè‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»å®Œäº†Vision Transformerï¼Œæˆ‘ä»¬æ¥ä»Inductive Bias çš„æ–¹é¢ï¼Œçœ‹çœ‹ CNN å’Œ ViT æœ‰ä»€ä¹ˆä¸åŒ\n\n\n\n\n\n\nä»€ä¹ˆæ˜¯Inductive Bias\n\n\n\nåœ¨æ·±åº¦å­¦ä¹ é‡Œï¼ŒInductive Biasï¼ˆå½’çº³åç½®ï¼‰æ˜¯æŒ‡æ¨¡å‹åœ¨å­¦ä¹ ä¹‹å‰ï¼Œå› ç»“æ„æˆ–è®¾è®¡è€Œè‡ªå¸¦çš„å‡è®¾æˆ–å…ˆéªŒã€‚\n\n\nå¯¹äºå›¾åƒæ¥è¯´ï¼Œå¸¸è§çš„å…ˆéªŒå°±æ˜¯ï¼š\n\nå±€éƒ¨åƒç´ æ˜¯ç›¸å…³çš„ï¼ˆlocalityï¼‰\nç›¸é‚»åŒºåŸŸçš„æ¨¡å¼æœ‰è§„å¾‹ï¼ˆ2D neighborhoodï¼‰\nç‰©ä½“æ— è®ºå‡ºç°åœ¨å›¾åƒå“ªé‡Œï¼Œè¯†åˆ«æ–¹å¼åº”è¯¥ä¸€æ ·ï¼ˆtranslation equivarianceï¼‰\n\nğŸ”¹ 2. CNN çš„ç»“æ„æ€ä¹ˆä½“ç°è¿™äº›åç½®ï¼Ÿ 1. å±€éƒ¨æ€§ (Locality) â€¢ å·ç§¯æ ¸ï¼ˆä¾‹å¦‚ 3Ã—3ï¼‰åªå’Œå±€éƒ¨åƒç´ æ‰“äº¤é“ï¼Œè€Œä¸æ˜¯å…¨å›¾ã€‚ â€¢ è¿™æ„å‘³ç€æ¨¡å‹â€œç›¸ä¿¡â€å›¾åƒçš„é‡è¦ç‰¹å¾æ¥è‡ªå±€éƒ¨é‚»åŸŸï¼Œè€Œä¸æ˜¯é¥è¿œåŒºåŸŸã€‚ 2. äºŒç»´é‚»åŸŸç»“æ„ (2D structure) â€¢ å·ç§¯æ“ä½œæ˜¯æ²¿ç€ å›¾åƒçš„äºŒç»´ç½‘æ ¼è¿›è¡Œçš„ï¼Œå¤©ç„¶åˆ©ç”¨äº†å›¾åƒçš„è¡Œåˆ—ç»“æ„ã€‚ â€¢ è¿™å’Œæ–‡æœ¬ï¼ˆåºåˆ— 1Dï¼‰ä¸ä¸€æ ·ï¼ŒCNN æ˜ç¡®çŸ¥é“è¾“å…¥æ˜¯ 2D æ’åˆ—çš„ã€‚ 3. å¹³ç§»ç­‰å˜æ€§ (Translation equivariance) â€¢ å·ç§¯æ ¸çš„å‚æ•°åœ¨æ•´å¼ å›¾å…±äº«ã€‚ â€¢ æ‰€ä»¥çŒ«åœ¨å·¦ä¸Šè§’è¿˜æ˜¯å³ä¸‹è§’ï¼Œå·ç§¯æ ¸éƒ½èƒ½æ£€æµ‹åˆ°â€œçŒ«è€³æœµâ€ã€‚ â€¢ è¿™è®© CNN è‡ªåŠ¨å…·æœ‰â€œè¯†åˆ«ä½ç½®æ— å…³â€çš„èƒ½åŠ›ã€‚\nè¿™äº›æ€§è´¨ä¸æ˜¯æ¨¡å‹é€šè¿‡è®­ç»ƒå­¦å‡ºæ¥çš„ï¼Œè€Œæ˜¯å› ä¸º å·ç§¯æ“ä½œæœ¬èº«çš„æ•°å­¦ç»“æ„å°±å¸¦æ¥çš„ï¼š â€¢ kernel çš„å±€éƒ¨è¿æ¥ â†’ å±€éƒ¨æ€§ â€¢ kernel æ»‘åŠ¨è¦†ç›–å…¨å›¾ â†’ å¹³ç§»ç­‰å˜æ€§ â€¢ æ“ä½œåœ¨äºŒç»´ç©ºé—´å®šä¹‰ â†’ é‚»åŸŸç»“æ„ â€¢ æ‰€ä»¥ï¼Œå“ªæ€•ä½ ä¸ç»™ CNN å–‚å¤ªå¤šæ•°æ®ï¼Œå®ƒä¹Ÿä¼šåˆ©ç”¨è¿™äº›åç½®å»å­¦ä¹ ç‰¹å¾ã€‚\nè€Œå¯¹äº ViT æ¥è¯´ï¼š ViT çš„å½’çº³åç½®éå¸¸å¼±ï¼Œå‡ ä¹å®Œå…¨ä¾èµ–æ•°æ®å’Œè®­ç»ƒæ¥å­¦ä¹ ã€‚ 1. Patch åˆ‡åˆ† (Patchification) â€¢ ViT å”¯ä¸€çš„â€œå›¾åƒå…ˆéªŒâ€ä¹‹ä¸€å°±æ˜¯æŠŠè¾“å…¥å›¾ç‰‡åˆ‡æˆ patchã€‚ â€¢ è¿™ä¸€æ“ä½œéšå«äº†ï¼šå›¾åƒæ˜¯ä¸€ä¸ªäºŒç»´ç»“æ„ï¼Œå¯ä»¥è¢«åˆ†å—å¤„ç†ã€‚ 2. ä½ç½®ç¼–ç  (Positional Embeddings) â€¢ Transformer æœ¬èº«åªå¤„ç†åºåˆ—ï¼Œæ²¡æœ‰ç©ºé—´ç»“æ„çš„æ¦‚å¿µã€‚ â€¢ ViT é€šè¿‡åŠ ä½ç½®ç¼–ç å‘Šè¯‰æ¨¡å‹ patch åœ¨å›¾åƒä¸­çš„ç›¸å¯¹ä½ç½®ã€‚ â€¢ åœ¨è¾“å…¥åˆ†è¾¨ç‡å˜åŒ–æ—¶ï¼Œä¼šåš äºŒç»´æ’å€¼ (2D interpolation) æ¥é€‚é…ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§äººå·¥å¼•å…¥çš„ 2D å…ˆéªŒã€‚ 3. å…¶ä»–éƒ¨åˆ† â€¢ é™¤äº†ä»¥ä¸Šä¸¤ç‚¹ï¼ŒViT çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ å…¨å±€çš„ (global)ï¼Œæ²¡æœ‰å±€éƒ¨æ€§çº¦æŸã€‚ â€¢ æ²¡æœ‰åƒ CNN é‚£æ ·å†…ç½®çš„å¹³ç§»ç­‰å˜æ€§æˆ–å±€éƒ¨é‚»åŸŸç»“æ„ã€‚\nè¿™æ ·å°±æ˜¯ä¸ºä»€ä¹ˆViTéœ€è¦æ›´å¤šæ•°æ®å’Œè®¡ç®—æ‰èƒ½å­¦åˆ°åŒæ ·çš„ç©ºé—´å½’çº³è§„å¾‹ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.6 ViT Model Variants",
    "text": "1.6 ViT Model Variants\nViT æœ‰3ç§ä¸åŒçš„åŸºæœ¬å˜å½¢ï¼Œ å¦‚ä¸‹å›¾æ‰€ç¤º \nViTçš„åå­—é€šå¸¸è¡¨ç¤ºä¸º: ViT-L/16: æ„æ€æ˜¯ï¼ŒViT-Largeï¼Œç„¶åç”¨çš„16 Patch Sizeã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒPatch Sizeè¶Šå¤§ï¼Œæˆ‘ä»¬å¾—åˆ°çš„tokenså°±è¶Šå°‘ï¼Œä¹Ÿå°±æ˜¯éœ€è¦æ›´å°‘çš„è®­ç»ƒæ—¶å®ç°ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#å‡å°‘tokens",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#å‡å°‘tokens",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "3.1 å‡å°‘Tokens",
    "text": "3.1 å‡å°‘Tokens\n\nPatch Merge\nPatch Shuffle",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "3.2 Vision Language Model",
    "text": "3.2 Vision Language Model\næˆ‘ä»¬ä»¥åŠå­¦ä¹ äº†ViT for computer Visionï¼Œ Transformer for NLPï¼Œ æ¥ä¸‹æ¥æœ‰ä»€ä¹ˆåŠæ³•è®©è¿™ä¸¤ç§æ¨¡å‹ç»“åˆèµ·æ¥å‘¢ï¼Ÿ CLIP (2021): å°† ViT èåˆåˆ° vision-language é¢„è®­ç»ƒä¸­ã€‚",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#axial-attentionè½´å‘æ³¨æ„åŠ›",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#axial-attentionè½´å‘æ³¨æ„åŠ›",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "4.1 Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰",
    "text": "4.1 Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰\nåœ¨å¤„ç† å›¾åƒæˆ–è§†é¢‘ è¿™ç±»é«˜ç»´è¾“å…¥æ—¶ï¼Œå¦‚æœç›´æ¥å¯¹æ‰€æœ‰åƒç´ åš å…¨å±€ self-attentionï¼Œå¤æ‚åº¦æ˜¯ \\(\\mathcal{O}(H^2 W^2)\\) ï¼ˆ\\(H, W\\) æ˜¯é«˜å’Œå®½ï¼‰ã€‚å½“å›¾åƒå¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªä»£ä»·å¤ªé«˜ã€‚ æ ¸å¿ƒæƒ³æ³•ï¼šæŠŠäºŒç»´ attention æ‹†æˆä¸¤æ¬¡ä¸€ç»´ attentionï¼ˆæ²¿ç€å›¾åƒçš„ä¸¤ä¸ªâ€œè½´â€åˆ†åˆ«åšï¼‰ã€‚ 1. Row-wise Attentionï¼ˆè¡Œæ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€æ°´å¹³æ–¹å‘ï¼ˆå®½åº¦è½´ Wï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€è¡Œçš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦ï¼š\\(\\mathcal{O}(H \\cdot W^2)\\)ã€‚ 2. Column-wise Attentionï¼ˆåˆ—æ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€å‚ç›´æ–¹å‘ï¼ˆé«˜åº¦è½´ Hï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€åˆ—çš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦ï¼š \\(\\mathcal{O}(W \\cdot H^2)\\)ã€‚\nç»„åˆèµ·æ¥ï¼Œç›¸å½“äºåœ¨ H å’Œ W ä¸¤ä¸ªè½´ä¸Šéƒ½åšäº†å…¨å±€ä¾èµ–å»ºæ¨¡ã€‚\n\n\n\n\n\n\nFigureÂ 5",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "5.1 Swin V2",
    "text": "5.1 Swin V2",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#è®­ç»ƒæŠ€å·§",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#è®­ç»ƒæŠ€å·§",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "5.2 è®­ç»ƒæŠ€å·§",
    "text": "5.2 è®­ç»ƒæŠ€å·§\n\nDropout Path\nGradient Checkpoint:\n\néœ€è¦å¯é‡ç°å‰å‘ â€¢ è¢« checkpoint çš„æ¨¡å—å¿…é¡»æ˜¯ çº¯å‡½æ•°ï¼Œå³è¾“å‡ºåªä¾èµ–è¾“å…¥ï¼Œä¸èƒ½ä¾èµ–éšæœºæ•°ã€å…¨å±€çŠ¶æ€ã€‚\n\n\nCite to CLIP",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/04-clip/CLIP.html",
    "href": "posts/04-clip/CLIP.html",
    "title": "04: Learning Transferable Visual Models From Natural Language Supervision(CLIP)",
    "section": "",
    "text": "1 CLIP\nThis is the link to the Vision-Transformer\nThis is another link to the Transformer\nTHis is link to the Swin-Transformer\n\n\n2 Summary\n\n\n3 Key Concepts\n\n\n4 Q & A\n\n\n5 æ‰©å±•\n\n\n\n\n Back to top",
    "crumbs": [
      "04 Clip",
      "04: Learning Transferable Visual Models From Natural Language Supervision(**CLIP**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/Transformer.html",
    "href": "posts/01-transformer/Transformer.html",
    "title": "Attention is All You Need(Transformer)",
    "section": "",
    "text": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations  Attention_is_all_you_need, p.2  THis\n\n\n\n\n Back to top",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html",
    "href": "00-how-to-read-paper.html",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "åœ¨æ·±å…¥å­¦ä¹ äººå·¥æ™ºèƒ½ç›¸å…³è®ºæ–‡ä¹‹å‰ï¼ŒæŒæ¡é«˜æ•ˆä¸”ç³»ç»Ÿçš„é˜…è¯»æ–¹æ³•è‡³å…³é‡è¦ã€‚è®ºæ–‡é˜…è¯»ç»å…¸æŒ‡å— How to Read a Paper ä¸­æå‡ºäº†â€œä¸‰éé˜…è¯»æ³•â€ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†æ¸…æ™°çš„å®è·µè·¯å¾„ï¼š\n\n\n\n\nç¬¬ä¸€éï¼šå¿«é€Ÿæµè§ˆï¼Œè·å–è®ºæ–‡çš„æ•´ä½“ç»“æ„å’Œæ ¸å¿ƒç»“è®ºã€‚èšç„¦äºæ ‡é¢˜ï¼ˆtitleï¼‰ã€æ‘˜è¦ï¼ˆabstractï¼‰ã€å¼•è¨€ï¼ˆintroductionï¼‰ä¸ç»“è®ºï¼ˆconclusionï¼‰ï¼Œä»å®è§‚ä¸Šäº†è§£æ–‡ç« çš„ç ”ç©¶æ–¹å‘ã€‚\nç¬¬äºŒéï¼šç»†è¯»è®ºæ–‡ï¼Œå…³æ³¨ç†è®ºæ¨å¯¼ã€å®éªŒè®¾ç½®ä¸å…³é”®å›¾è¡¨ã€‚é‡ç‚¹ç†è§£è®ºæ–‡æ‰€æå‡ºçš„æ–¹æ³•ã€å®éªŒè®¾è®¡ã€ç»“æœåˆ†æåŠå…¶æ”¯æ’‘é€»è¾‘ã€‚\nç¬¬ä¸‰éï¼šæ‰¹åˆ¤æ€§é˜…è¯»ï¼Œç³»ç»Ÿåˆ†æè®ºæ–‡çš„ä¼˜åŠ£ï¼Œæå‡ºå»ºè®¾æ€§é—®é¢˜ï¼Œåæ€è¯¥æ–¹æ³•æ˜¯å¦å…·æœ‰é€šç”¨æ€§æˆ–æ˜¯å¦èƒ½åº”ç”¨äºè‡ªèº«ç ”ç©¶ã€‚\n\n\n\nListingÂ 1: ä¸‰éé˜…è¯»æ³•\n\n\n\néœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œé˜…è¯»è®ºæ–‡æ˜¯ä¸€ä¸ªå¾ªç¯è¿­ä»£ã€é€æ­¥æ·±å…¥çš„è¿‡ç¨‹ã€‚åœ¨åå¤é˜…è¯»ä¸æ€è€ƒä¸­ï¼Œæˆ‘ä»¬ä¼šä¸æ–­ä¿®æ­£ç†è§£ã€åŠ æ·±è®¤çŸ¥ã€‚ä¸æ­¤åŒæ—¶ï¼Œé€šè¿‡ä¸ä»–äººäº¤æµã€å‚ä¸è®¨è®ºï¼Œæœ‰åŠ©äºæ‹“å®½è§†è§’ã€æ·±åŒ–æ€è€ƒã€‚ï¼ˆæœ¬é¡¹ç›®ç½‘ç«™ä¹Ÿå› æ­¤è€Œç”Ÿï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä»¬æä¾›ä¸€ä¸ªå…±åŒäº¤æµå­¦ä¹ çš„å¹³å°ï¼‰\nä¸ºæå‡é˜…è¯»æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ç²¾è¯»é˜¶æ®µï¼Œä½¿ç”¨é¢œè‰²æ ‡è®°ä¸åŒå†…å®¹ã€‚ä¾‹å¦‚åœ¨ Harvard CS197 AI Research Experiences çš„ Lecture 3 ä¸­ç»™å‡ºçš„ç­–ç•¥ï¼š\n\né»„è‰²ï¼šçªå‡ºè®ºæ–‡æ‰€è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜æˆ–æŒ‘æˆ˜ã€‚\nç»¿è‰²ï¼šæ ‡è®°è®ºæ–‡ä¸­çš„æåˆ°çš„é€šç”¨æ¦‚å¿µã€‚\nç²‰è‰²ï¼šå¯¹åº”æå‡ºçš„ç®—æ³•æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ã€‚\næ©™è‰²ï¼šç”¨äºæ ‡è®°è®ºæ–‡çš„åˆ›æ–°ç‚¹ä¸è´¡çŒ®ã€‚\n\nä¾‹å¦‚å¦‚ä¸‹ç¤ºæ„å›¾ï¼š\n\n\n\n\n\n\nFigureÂ 1: Highlight Example of paper (Attention is all you need)\n\n\n\nè¿™å¥—æ–¹æ³•å¹¶éå”¯ä¸€æ ‡å‡†ï¼Œå…³é”®åœ¨äºæ„å»ºä¸€å¥—é€‚åˆè‡ªèº«è®¤çŸ¥æ–¹å¼çš„å¯è§†åŒ–æ ‡è®°ä½“ç³»ã€‚ç»Ÿä¸€çš„æ ‡æ³¨é£æ ¼ï¼Œæœ‰åŠ©äºåœ¨åæœŸå›é¡¾æˆ–è·¨è®ºæ–‡æ¯”è¾ƒæ—¶é«˜æ•ˆå®šä½å…³é”®ä¿¡æ¯ã€‚æœ¬é¡¹ç›®æ‰€æ”¶å½•çš„æ¯ç¯‡è®ºæ–‡ç¬”è®°ä¹Ÿå°†é‡‡ç”¨è¿™ä¸€ç»“æ„åŒ–é«˜äº®æ ‡è®°æ–¹æ³•ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#è®ºæ–‡é˜…è¯»æŒ‡å—",
    "href": "00-how-to-read-paper.html#è®ºæ–‡é˜…è¯»æŒ‡å—",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "åœ¨æ·±å…¥å­¦ä¹ äººå·¥æ™ºèƒ½ç›¸å…³è®ºæ–‡ä¹‹å‰ï¼ŒæŒæ¡é«˜æ•ˆä¸”ç³»ç»Ÿçš„é˜…è¯»æ–¹æ³•è‡³å…³é‡è¦ã€‚è®ºæ–‡é˜…è¯»ç»å…¸æŒ‡å— How to Read a Paper ä¸­æå‡ºäº†â€œä¸‰éé˜…è¯»æ³•â€ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†æ¸…æ™°çš„å®è·µè·¯å¾„ï¼š\n\n\n\n\nç¬¬ä¸€éï¼šå¿«é€Ÿæµè§ˆï¼Œè·å–è®ºæ–‡çš„æ•´ä½“ç»“æ„å’Œæ ¸å¿ƒç»“è®ºã€‚èšç„¦äºæ ‡é¢˜ï¼ˆtitleï¼‰ã€æ‘˜è¦ï¼ˆabstractï¼‰ã€å¼•è¨€ï¼ˆintroductionï¼‰ä¸ç»“è®ºï¼ˆconclusionï¼‰ï¼Œä»å®è§‚ä¸Šäº†è§£æ–‡ç« çš„ç ”ç©¶æ–¹å‘ã€‚\nç¬¬äºŒéï¼šç»†è¯»è®ºæ–‡ï¼Œå…³æ³¨ç†è®ºæ¨å¯¼ã€å®éªŒè®¾ç½®ä¸å…³é”®å›¾è¡¨ã€‚é‡ç‚¹ç†è§£è®ºæ–‡æ‰€æå‡ºçš„æ–¹æ³•ã€å®éªŒè®¾è®¡ã€ç»“æœåˆ†æåŠå…¶æ”¯æ’‘é€»è¾‘ã€‚\nç¬¬ä¸‰éï¼šæ‰¹åˆ¤æ€§é˜…è¯»ï¼Œç³»ç»Ÿåˆ†æè®ºæ–‡çš„ä¼˜åŠ£ï¼Œæå‡ºå»ºè®¾æ€§é—®é¢˜ï¼Œåæ€è¯¥æ–¹æ³•æ˜¯å¦å…·æœ‰é€šç”¨æ€§æˆ–æ˜¯å¦èƒ½åº”ç”¨äºè‡ªèº«ç ”ç©¶ã€‚\n\n\n\nListingÂ 1: ä¸‰éé˜…è¯»æ³•\n\n\n\néœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œé˜…è¯»è®ºæ–‡æ˜¯ä¸€ä¸ªå¾ªç¯è¿­ä»£ã€é€æ­¥æ·±å…¥çš„è¿‡ç¨‹ã€‚åœ¨åå¤é˜…è¯»ä¸æ€è€ƒä¸­ï¼Œæˆ‘ä»¬ä¼šä¸æ–­ä¿®æ­£ç†è§£ã€åŠ æ·±è®¤çŸ¥ã€‚ä¸æ­¤åŒæ—¶ï¼Œé€šè¿‡ä¸ä»–äººäº¤æµã€å‚ä¸è®¨è®ºï¼Œæœ‰åŠ©äºæ‹“å®½è§†è§’ã€æ·±åŒ–æ€è€ƒã€‚ï¼ˆæœ¬é¡¹ç›®ç½‘ç«™ä¹Ÿå› æ­¤è€Œç”Ÿï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä»¬æä¾›ä¸€ä¸ªå…±åŒäº¤æµå­¦ä¹ çš„å¹³å°ï¼‰\nä¸ºæå‡é˜…è¯»æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ç²¾è¯»é˜¶æ®µï¼Œä½¿ç”¨é¢œè‰²æ ‡è®°ä¸åŒå†…å®¹ã€‚ä¾‹å¦‚åœ¨ Harvard CS197 AI Research Experiences çš„ Lecture 3 ä¸­ç»™å‡ºçš„ç­–ç•¥ï¼š\n\né»„è‰²ï¼šçªå‡ºè®ºæ–‡æ‰€è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜æˆ–æŒ‘æˆ˜ã€‚\nç»¿è‰²ï¼šæ ‡è®°è®ºæ–‡ä¸­çš„æåˆ°çš„é€šç”¨æ¦‚å¿µã€‚\nç²‰è‰²ï¼šå¯¹åº”æå‡ºçš„ç®—æ³•æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ã€‚\næ©™è‰²ï¼šç”¨äºæ ‡è®°è®ºæ–‡çš„åˆ›æ–°ç‚¹ä¸è´¡çŒ®ã€‚\n\nä¾‹å¦‚å¦‚ä¸‹ç¤ºæ„å›¾ï¼š\n\n\n\n\n\n\nFigureÂ 1: Highlight Example of paper (Attention is all you need)\n\n\n\nè¿™å¥—æ–¹æ³•å¹¶éå”¯ä¸€æ ‡å‡†ï¼Œå…³é”®åœ¨äºæ„å»ºä¸€å¥—é€‚åˆè‡ªèº«è®¤çŸ¥æ–¹å¼çš„å¯è§†åŒ–æ ‡è®°ä½“ç³»ã€‚ç»Ÿä¸€çš„æ ‡æ³¨é£æ ¼ï¼Œæœ‰åŠ©äºåœ¨åæœŸå›é¡¾æˆ–è·¨è®ºæ–‡æ¯”è¾ƒæ—¶é«˜æ•ˆå®šä½å…³é”®ä¿¡æ¯ã€‚æœ¬é¡¹ç›®æ‰€æ”¶å½•çš„æ¯ç¯‡è®ºæ–‡ç¬”è®°ä¹Ÿå°†é‡‡ç”¨è¿™ä¸€ç»“æ„åŒ–é«˜äº®æ ‡è®°æ–¹æ³•ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#å·¥å…·å‡†å¤‡",
    "href": "00-how-to-read-paper.html#å·¥å…·å‡†å¤‡",
    "title": "00: Preparation for Following",
    "section": "2 å·¥å…·å‡†å¤‡",
    "text": "2 å·¥å…·å‡†å¤‡\nç§‘å­¦é˜…è¯»ç¦»ä¸å¼€åˆé€‚çš„å·¥å…·æ”¯æ’‘ã€‚ä»¥ä¸‹æ˜¯æ¨èçš„å·¥å…·ä½“ç³»ï¼Œæ¶µç›–æ–‡çŒ®ç®¡ç†ã€ç¬”è®°æ•´ç†ã€ä»£ç æ‰§è¡Œç­‰å¤šä¸ªç»´åº¦ã€‚\n\n2.1 æ–‡çŒ®ç®¡ç†ï¼šZotero\néšç€è®ºæ–‡ç§¯ç´¯çš„å¢å¤šï¼Œç³»ç»Ÿçš„æ–‡çŒ®ç®¡ç†å·¥å…·ä¸å¯æˆ–ç¼ºã€‚Zotero æ˜¯ä¸€æ¬¾å…è´¹ä¸”å¼€æºçš„æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œæ”¯æŒè‡ªåŠ¨å¯¼å…¥ã€åˆ†ç»„ç®¡ç†ä¸å¤šæ ¼å¼å¼•ç”¨ï¼ˆå¦‚ BibTeXï¼‰ã€‚å…¶å¯æ‰©å±•æ€§æå¼ºï¼Œæ”¯æŒæ’ä»¶ä¸ä¸»é¢˜å®šåˆ¶ã€‚\n\n\n\n\n\n\nFigureÂ 2: Example of Zotero\n\n\n\næ¨èæ’ä»¶ï¼š\n\nBetter BibTexï¼šå¢å¼º BibTeX å¯¼å‡ºåŠŸèƒ½ï¼Œä¾¿äºä¸ LaTeX æ— ç¼é›†æˆã€‚\nEthereal Styleï¼šä¸º Zotero æä¾›ç¾è§‚çš„ UI é£æ ¼ï¼Œæå‡ä½¿ç”¨ä½“éªŒã€‚\n\nå°½ç®¡ Zotero å­˜åœ¨ä¸€å®šå­¦ä¹ æ›²çº¿ï¼Œä½†å…¶é•¿æœŸä»·å€¼è¿œè¶…åˆæœŸæŠ•å…¥ã€‚è‹¥ä»…å¸Œæœ›ä¸´æ—¶é˜…è¯»ï¼ŒPDF é˜…è¯»å™¨äº¦å¯ï¼›ä½†ä»ç§‘ç ”è§†è§’å‡ºå‘ï¼Œå»ºè®®å°½æ—©æŠ•å…¥å­¦ä¹ ä¸ä½¿ç”¨ã€‚\næ­¤å¤–ï¼ŒZotero Chrome Connector æ’ä»¶å¯å®ç°ä¸€é”®å¯¼å…¥ç½‘é¡µæ–‡çŒ®ï¼Œæå¤§æå‡æ–‡çŒ®æ”¶é›†æ•ˆç‡ï¼š\n\n\n\n\n\n\nFigureÂ 3: Zotero Chrome Connector\n\n\n\nå¦‚ FigureÂ 3 æ‰€ç¤ºï¼Œåªéœ€ç‚¹å‡»æ’ä»¶æŒ‰é’®ï¼Œå³å¯å°†å½“å‰ç½‘é¡µå†…å®¹å¯¼å…¥è‡³æ–‡çŒ®åº“ã€‚\n\n\n2.2 ç¬”è®°è®°å½•ï¼šObsidian\nObsidian æ˜¯ä¸€æ¬¾åŸºäº Markdown çš„ç¬”è®°ç³»ç»Ÿï¼Œæ”¯æŒåŒå‘é“¾æ¥ä¸å›¾è°±è§†å›¾ï¼Œç‰¹åˆ«é€‚åˆç”¨äºæ„å»ºä¸ªäººçŸ¥è¯†ä½“ç³»ã€‚\n\n\n\n\n\n\nFigureÂ 4: Obsidian Example\n\n\n\næ¨èæ’ä»¶ï¼š\n\nobsidian-latex-suiteï¼šæä¾› LaTeX å¿«æ·è¾“å…¥ä¸å…¬å¼é¢„è§ˆåŠŸèƒ½ï¼Œæ˜¾è‘—æé«˜æ•°å­¦è¡¨è¾¾æ•ˆç‡ã€‚\nHighlightr Pluginï¼šæ”¯æŒè‡ªå®šä¹‰é«˜äº®é¢œè‰²ï¼Œä¾¿äºåˆ†ç±»ä¿¡æ¯æ ‡æ³¨ã€‚\n\n \néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿‡åº¦ç¾åŒ–ç•Œé¢æˆ–æ’ä»¶å †å å¯èƒ½åè€Œåˆ†æ•£æ³¨æ„åŠ›ã€‚å»ºè®®ä»¥â€œç»“æ„æ¸…æ™°ã€å†…å®¹ä¸ºæœ¬â€ä¸ºé¦–è¦åŸåˆ™ã€‚\nå¯¹äºä¸ä½¿ç”¨ Obsidian çš„ç”¨æˆ·ï¼Œä¹Ÿå¯é€‰æ‹©ï¼š\n\n\n\n\n\n\n\n\n\n\n\n(a) Notion Home Page\n\n\n\n\n\n\n\n\n\n\n\n(b) FeiShu Home Page\n\n\n\n\n\n\n\nFigureÂ 5: Home Page of Notion and FeiShu\n\n\n\n\nNotionï¼šå¦‚ FigureÂ 5 (a) æ‰€ç¤ºï¼Œé€‚åˆå¤šäººåä½œä¸å¯è§†åŒ–ç¼–è¾‘ã€‚\né£ä¹¦ï¼šå¦‚ FigureÂ 5 (b) æ‰€ç¤ºï¼ŒåŠŸèƒ½å…¨é¢ï¼Œé€‚åˆä¼ä¸šçº§æ–‡æ¡£ç®¡ç†ã€‚\n\n\n\n2.3 ä»£ç æ‰§è¡Œï¼šJupyter Notebook\nåœ¨â€œPaper with Codeâ€ç†å¿µä¸‹ï¼Œæ¯ç¯‡è®ºæ–‡å°†é…å¥— Jupyter Notebook å®ç°æ ¸å¿ƒç®—æ³•ã€‚å…¶äº¤äº’å¼æ–‡æ¡£ç‰¹æ€§ï¼Œä½¿å…¶æˆä¸ºå­¦ä¹ ä¸éªŒè¯ä»£ç çš„ç†æƒ³å¹³å°ã€‚\n\n\n\n\n\n\nNote\n\n\n\nè‹¥å¯¹ Jupyter Notebook ä¸ç†Ÿæ‚‰ï¼Œæ¨èå‚è€ƒ å®˜æ–¹æ–‡æ¡£ï¼Œä»¥å¿«é€Ÿå…¥é—¨ã€‚\n\n\nç›¸åº”çš„ä»£ç ï¼Œæˆ‘ä¼šæ”¾åœ¨GitHubçš„ä»“åº“ä¸­\n\n\n\n\n\n\nFigureÂ 6: The preview of GitHub Page\n\n\n\n\n\n2.4 GPU å¹³å°ï¼šäº‘ç«¯æ‰§è¡Œç¯å¢ƒ\næ·±åº¦å­¦ä¹ æ¨¡å‹å¸¸éœ€ GPU åŠ é€Ÿï¼Œè‹¥æœ¬åœ°æ—  GPU å¯ä½¿ç”¨ä»¥ä¸‹å¹³å°ï¼š\n\nGoogle Colabï¼šGoogle æä¾›çš„å…è´¹äº‘ç«¯ Notebook å¹³å°ï¼Œæ”¯æŒ GPU ä¸ TPUã€‚\nKaggle Kernelsï¼šæ”¯æŒ GPU çš„æ•°æ®ç§‘å­¦å¹³å°ï¼Œé€‚åˆå¿«é€Ÿå®éªŒã€‚\n\nå›½å†…å¯é€‰å¹³å°ï¼š\n\nAutoDLï¼šé€‚åˆå›½å†…ç”¨æˆ·ï¼Œé…ç½®ç®€å•ï¼Œæ”¯æŒå®šåˆ¶åŒ–éƒ¨ç½²ã€‚\n\nå…¶ä»–æ¨èï¼š\n\nRunPodã€Lambda Labsï¼šæä¾›ç¨³å®šã€ä½å»¶è¿Ÿçš„ GPU è®­ç»ƒç¯å¢ƒï¼Œé€‚åˆä¸­å¤§å‹å®éªŒä»»åŠ¡ã€‚\n\n\né€šè¿‡åˆç†é…ç½®ä¸Šè¿°å·¥å…·ï¼Œå¯ä»¥æ„å»ºå‡ºä¸€ä¸ªç³»ç»ŸåŒ–ã€é«˜æ•ˆçš„è®ºæ–‡å­¦ä¹ ä¸ç ”ç©¶æµç¨‹ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæ¯ç¯‡è®ºæ–‡å°†é™„å¸¦ä»£ç å®ç°ã€ç»“æ„è§£æä¸æ‰¹åˆ¤æ€§æ€è€ƒï¼Œæ¬¢è¿å…±åŒå­¦ä¹ äº¤æµã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#æ€»ç»“",
    "href": "00-how-to-read-paper.html#æ€»ç»“",
    "title": "00: Preparation for Following",
    "section": "3 æ€»ç»“",
    "text": "3 æ€»ç»“\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é«˜æ•ˆé˜…è¯»è®ºæ–‡çš„æ–¹æ³•è®ºä¸å·¥å…·ä½“ç³»ã€‚é€šè¿‡â€œä¸‰éé˜…è¯»æ³•â€ ListingÂ 1ï¼Œ æˆ‘ä»¬å¯ä»¥ç³»ç»Ÿåœ°ç†è§£è®ºæ–‡å†…å®¹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ‰¹åˆ¤æ€§æ€è€ƒã€‚åŒæ—¶ï¼Œå€ŸåŠ© Zotero SectionÂ 2.1ã€ObsidianSectionÂ 2.2 ç­‰å·¥å…·ï¼Œå¯ä»¥æœ‰æ•ˆç®¡ç†æ–‡çŒ®ã€è®°å½•ç¬”è®°ä¸æ‰§è¡Œä»£ç ã€‚ åœ¨åç»­ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åº”ç”¨è¿™äº›æ–¹æ³•ä¸å·¥å…·ï¼Œæ·±å…¥åˆ†ææ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ã€å®éªŒè®¾è®¡ä¸åˆ›æ–°è´¡çŒ®ã€‚å¸Œæœ›é€šè¿‡æœ¬é¡¹ç›®çš„å­¦ä¹ ï¼Œèƒ½å¤Ÿå¸®åŠ©å¤§å®¶æ›´å¥½åœ°æŒæ¡äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‰æ²¿ç ”ç©¶åŠ¨æ€ï¼Œå¹¶åœ¨å®è·µä¸­ä¸æ–­æå‡è‡ªå·±çš„ç§‘ç ”èƒ½åŠ›ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  }
]