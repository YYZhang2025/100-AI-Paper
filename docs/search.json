[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This website aims to collect and showcase 100 important AI papers and their code implementations. Each paper is accompanied by a link for readers to explore further.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This website aims to collect and showcase 100 important AI papers and their code implementations. Each paper is accompanied by a link for readers to explore further.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#why-build-this-website",
    "href": "index.html#why-build-this-website",
    "title": "100 Papers with Code",
    "section": "Why build this website?",
    "text": "Why build this website?\nOn the one head, during my study, reading, and practice in the field of AI, I found that many important papers and code implementations were scattered in various places. To facilitate access for myself and others, I decided to build this website to consolidate these important papers and my implementations in one place. On the other hand, there is a gap between topic in papers and practical application. By providing code implementations, I hope to help bridge this gap and make it easier for practitioners to apply the latest research results.\nThere are some awesome resource about the paper and implementation such as:\n\nAnnotated Research Paper: Collection of simple PyTorch implementations of neural networks and related algorithms.\nPapers with Code (It was replace by Hugging Face now): The largest resource for finding machine learning papers, code and evaluation tables.\n\nBut why I still build this website?\n\nThe Annotated Research Paper focuses more on each component of the paper, it didn’t provide the whole modeling and training process.\nThe Papers with Code provides the paper and code, but the code is sometime hard to understand, and it didn’t provide the explanation of the code.\n\nThis website is built to fill these gaps by providing clear explanations and easy-to-understand code implementations for each paper.Each page(paper) will come with a self-contained Jupyter Notebook that can be run directly, making it easier for readers to understand and apply the concepts presented in the papers.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-website",
    "href": "index.html#how-to-use-this-website",
    "title": "100 Papers with Code",
    "section": "How to use this website?",
    "text": "How to use this website?\nThis website is designed to be user-friendly and accessible for anyone interested in understanding and implementing the latest research in AI and machine learning. Each paper has its own dedicated page that includes\n\nThe explanation of the paper.\nCode implementation.\n\nQ & A part (you can use as flash cards).\nThe further direction might be interesting to explore (According to my understanding).\nBelow is the list of papers that have been implemented or are planned to be documented. You can click on the paper title to view detailed content.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need (Transformer )\nTransformer 是一种基于自注意力机制的深度学习架构，能够并行处理序列，在语言、视觉和多模态任务中表现出色，并且作为 GPT、BERT 等大型语言模型（LLM）的核心基础，推动了当今生成式人工智能的快速发展。\n\nNLP / Transformer\n\n\n02\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ( Vision Transformer )\nVision Transformer (ViT) 是一种将图像划分为小块（patches），并将其作为 token 输入标准 Transformer 模型进行图像分类的架构，首次实现了纯注意力机制在视觉任务中的成功应用。\n\nComputer Vision / Transformer\n\n\n03\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows ( Swin Transformer )\nSwin Transformer 是一种使用层次化结构和滑动窗口自注意力机制的视觉 Transformer 模型，既保留了局部建模的高效性，又通过窗口偏移实现跨区域信息交互，适用于图像分类、目标检测和语义分割等多种视觉任务。\n\nComputer Vision / Transformer\n\n\n04\nLearning Transferable Visual Models From Natural Language Supervision ( CLIP )\nCLIP 是一种利用大规模图文对比学习，将图像与自然语言映射到同一语义空间，从而实现零样本图像识别与跨模态检索的多模态基础模型\n\nComputer Vision / Transformer\n\n\n05\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness ( Flash Attention )\nFlashAttention 是一种优化的注意力机制实现，通过减少内存访问和提升计算效率，实现更快、更节省资源的 Transformer 推理与训练。\n\nTransformer / AI Engine",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html",
    "href": "posts/02-vision-transformer/Vision-Transformer.html",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "",
    "text": "在了解了什么是Transformer之后，我们来看看如何将Transformer应用于Computer Vision。Vision Transformer（ViT）(Dosovitskiy et al. 2021) 是一个将Transformer架构应用于图像分类的模型。它的核心思想是将图像划分为小块（patches），然后将这些小块视为序列数据，类似于处理文本数据。",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.1 Patch Embedding",
    "text": "1.1 Patch Embedding\n\nThe standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches\\(x \\in \\mathbb{R}^{N \\times (P^{2} \\times C)}\\), where (\\(H, W\\)) is the resolution of the original image, \\(C\\) is the number of channels, (\\(P, P\\)) is the resolution of each image patch, and \\(N = HW/P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size \\(D\\) through all of its layers, so we flatten the patches and map to \\(D\\) dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\n在Transformer 这一篇，我们了解到，它是作用于Sequence Modeling的，很显然，Image 不是 Sequence的。很直观的第一种想法就是，将图片直接展开，从二维 (\\(3, H, W\\)) 展开成一维的 (\\(3, H \\times W\\)). 这样我们就得到的图片的Sequence Model。如下图@fig-flat-image所示\n\n\n\n\n\n\nFigure 2\n\n\n\n这种方法有一种明显的问题就是：Sequence的长度太长，举个例子，对于 \\(3\\times 256 \\times 256\\) 的图片，我们有 \\(256 \\times 256 = 65,336\\) 个tokens，通过这种方法，所需要的训练时长很长。并且它没有用到图片的一个特性：相邻的pixel 之间，是有很高的correlation的。所以我们很自然的想到：如果把相邻的pixels和在一组，组成一个patch，这样不就既减少了tokens的数量，又用到了pixel之间的correlation。这就是Vision Transformer 的Patch Embedding。 这样我们就得到了。 接下来我们只需要用，一个MLP，将我们展开的patch，映射到 \\(D\\)- dimension的空间，这样我们就可以传入Transformer 模型了。\n接下来我们来看看代码怎么实现：\n# Load Image and resize it to certain size\nimage_path = IMAGE_PATN\nimg_bgr = cv2.imread(image_path)\nimg_resized = cv2.resize(img_bgr, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\nimg = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB) \n\n# Patchify \npatches = einops.rearrange(\n     img, \"(h ph) (w pw) c -&gt; (h w) ph pw c\", ph=PATCH_SIZE, pw=PATCH_SIZE\n)\n通过这个Patchify只有，我们得到将图片Patch到了\n\n\n分成了不同的小Patch。\n接下来我们要做的就是，将这些Patch 展开，然后传入一个MLP，\nflat_patch = einops.rearrange(\n     patches, \"n ph pw c -&gt; n (ph pw c)\"\n)\n\n\nmlp = nn.Linear(PATCH_SIZE * PATCH_SIZE * 3, d_model)\npatch_embedding = mlp(flat_patch)\n同这种方式，我们就可以见图片转化为Transformer可以接受的vector。不过在实际操作中，我们并不会用以上的方式，因为上面的方式实现起来比较慢，我们可以将Patch 和 Linear Project和在一起。\n\n\n\n\n\n\nTip\n\n\n\n将几个tensor 的operation操作合成一个的方法，叫做kernel fusion，这是一种提高训练和推理素的方法\n\n\n在实际的代码中，我们用Convolution Layer 代替 Patch + Flatten+ Linear 的方法. 如果我们用一个 卷积层，参数设置为： • kernel_size = PATCH_SIZE （卷积核覆盖一个 patch） • stride = PATCH_SIZE （不重叠地移动，相当于切 patch） • in_channels = 3（RGB） • out_channels = d_model\n那么卷积会： 1. 把输入图片分成 PATCH_SIZE x PATCH_SIZE 的不重叠块（因为 stride = kernel_size）。 2. 对每个 patch 做一次线性映射（因为卷积本质上就是对局部区域做加权求和，相当于 Linear）。 3. 输出的 shape 自动就是 (batch, num_patches, d_model)。\n这正好等价于 切 patch + flatten + Linear 的组合。\n代码如下：\nclass PatchEmbedding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.conv = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=config.hidden_dim,\n            kernel_size=config.patch_size,\n            stride=config.patch_size,\n            padding=\"valid\" if config.patch_size == 16 else \"same\",\n        )\n\n    def forward(self, imgs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        imgs: (batch_size, num_channels, height, width)\n        Returns: (batch_size,  num_patches_height, num_patches_width, hidden_dim)\n        \"\"\"\n        # (B, C, H, W) -&gt; (B, hidden_dim, H', W')\n        x = self.conv(imgs)\n\n        # (B, hidden_dim, H', W') -&gt; (B, hidden_dim, H' * W')\n        x = x.flatten(2)\n\n        # (B, hidden_dim, H' * W') -&gt; (B, H' * W', hidden_dim)\n        x = x.transpose(1, 2)\n        return x\n用卷积的好处，除了可以更高效的实现Patch Embedding，代码更加简洁之外，我们还可以通过改变 stride 来使一些Patch overlapping，获得一个多尺度的结构，\nThe image is convert along this process: \\[\n\\boxed{\n\\mathbf{x} \\in \\mathbb{R}^{C \\times H \\times W}\n\\quad \\xrightarrow{\\text{Patchify}} \\quad\n\\{ x_i \\in \\mathbb{R}^{C \\times P \\times P} \\}{i=1}^N\n\\quad \\xrightarrow{\\text{Flatten}} \\quad\n\\{ x_i \\in \\mathbb{R}^{(C \\cdot P \\cdot P)} \\}_{i=1}^N\n\\quad \\xrightarrow{\\text{Linear } W \\in \\mathbb{R}^{(C \\cdot P \\cdot P) \\times D}} \\quad\n\\{ z_i \\in \\mathbb{R}^{D} \\}_{i=1}^N\n}\n\\tag{1}\\]",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.2 Position Encoding",
    "text": "1.2 Position Encoding\n将图片转化为 Transformer 的输入之后，接下来Transformer中的另一个组件就是传入 Position Information。我们知道在Transformer 中，他们用的是 sine-cosine position embedding，在那篇文章中，我们也提到了，还存在其他不同的Position Encoding的办法，ViT 用的就是另一种办法，Learned Position Embedding。Learned Position Embedding的方法很简单，也很好理解，对于每一个位置，我们给他一个index，将这个index传入一个 Embedding Matrix， 我们就得到一个Position Embedding。不过与Token Embedding不同的是，我们会用到所有的Position，也整个matrix， 所以我们不用定index，直接定义整个Embedding，然后将它传入Transformer中。\nclass PositionalEncoding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.positional_embedding = nn.Parameter(\n            torch.randn(\n                1,\n                (config.image_size // config.patch_size) ** 2 + 1,\n                config.hidden_dim,\n            )\n        )\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Add positional encoding to the input tensor\n        batch_size = x.size(0)\n\n        pos_embedding = self.positional_embedding.expand(batch_size, -1, -1)\n        cls_token = self.cls_token.expand(batch_size, -1, -1)\n\n        x = torch.cat((cls_token, x), dim=1)\n        return x + pos_embedding\n为什么ViT要用Learned Position Embedding呢？在ViT这篇文章中，他们尝试过不同的Position Embedding，比如：\n\nNo Positional Information\n1-dimensional Positional Embedding\n2-dimensional Positional Embedding\nRelative Positional Embedding\n\n发现，除了No Positional Information之外，其余3种在Image Classification中的表现，都是差不多的。\n\n\n\n\n\n\nFigure 3\n\n\n\n论文中表示，可能是因为所需要的 Position的信息较小，对于不同种类的Position Embedding的方法，学习这个Position Information的能力，都是差不多的。\n\nWe speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., \\(14 \\times 14\\) as opposed to \\(224 \\times 224\\), and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.18 \n\n不过，尽管Position的方法不重要，但是不同的训练参数，还是会影响到学习到的Position Information, 下图所示：\n\n\n\n\n\n\nFigure 4\n\n\n\n\n1.2.1 Extending Position Encoding\n当我们有了一个Pre-Training的模型，我们想用它Fine-Tuning到一个不同图片大小的数据库，我们改怎么做呢，第一个方法当然是，Resize 我们的图片，到ViT Pre-training的图片大小，但是，这个能导致较大的图片，失去很多细节。如果我们想保持图片的大小不变，同时让模型训练，我们就需要Extend Position Encoding，因为当Patch Size不变，图片大小变了的话，产生的Number of Patches 也是会改变的，这样，就是损失一些信息。我们需要做的是，找到一种方法，增大或者减小Position的数量。 这就是所谓的Position Interpolation。\n2D interpolation of the pre-trained position embeddings • ViT 在预训练时，通常用固定输入分辨率（比如 224×224） → 生成固定数量的 patch（比如 16×16 patch → 196 个 patch）。 • 但在 fine-tuning 时，输入图片可能大小不一样，比如 384×384，这时 patch 数量就变了。 • 这会导致原本的 位置编码 (position embeddings) 和新的 patch 数量对不上。 • 解决办法：对预训练好的位置编码做 二维插值 (2D interpolation)，根据 patch 在原图中的空间位置，把位置编码拉伸/缩放到新的分辨率。\n\nThe Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.4 \n\ndef interpolate_pos_encoding(self, x, w, h):\n    npatch = x.shape[1] - 1\n    N = self.pos_embed.shape[1] - 1\n    if npatch == N and w == h:\n        return self.pos_embed\n    class_pos_embed = self.pos_embed[:, 0]\n    patch_pos_embed = self.pos_embed[:, 1:]\n    dim = x.shape[-1]\n    w0 = w // self.patch_embed.patch_size\n    h0 = h // self.patch_embed.patch_size\n    \n    patch_pos_embed = F.interpolate(\n        patch_pos_embed.reshape(\n            1, \n            int(math.sqrt(N)), \n            int(math.sqrt(N)), \n            dim\n        ).permute(0, 3, 1, 2),\n        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n        mode='bicubic',\n    )\n    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.3 [CLS] Tokens & MLP Head",
    "text": "1.3 [CLS] Tokens & MLP Head\n在 Transformer 这一节，我们了解到：每输入一个token，Transformer会输出对应的token。这就是说，对于每个patch，Transformer会输出对应的Tokens，那么，我们应该选择哪一个token作为我们图片的表示呢。 BERT (Devlin et al. 2019)， 用了一个 [CLS], 来表示一个句子。同理，我们也可以添加一个 [CLS] token, 来表示一张图片。同时，对于 [CLS] token, 我们也要在给他一个表示位置的信息。这就是为什么在Position Encoding上，我们有 (config.image_size // config.patch_size) ** 2 + 1, 位置信息，其中 +1 就是 [CLS] 的位置信息。 总结一下 [CLS] token 的作用就是用来聚合所有的Patch的消息，然后用来Image 的Representation。\n我们想一下，除了加一个 [CLS] token，之外，我们还有其他办法来表示图片吗。有一种很自然的方法就是，将所有的patch的消息收集起来，然后去一个平均值来表示这个图片。类似于传统的ConvNet(e.g. ResNet) 我们可以通过 AvgPooling 来实现。 不过论文中提到， 对于两种不同的Image Representation，需要有不同的Learning Rate 来训练这个网络。\nOther content \n有了Image Represent之后，我们只需要将这个传入一个简单的MLP，我们就可以得到一个Classifier。MLP的输入是hidden dim，输入则是我们Number of Classes。不同的Index 表示不同的Classses。\n\nAn initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classifier—just like ResNet’s final feature map—performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate,  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.17 \n\n\nBoth during pre-training and fine-tuning, a classification head is attached to \\(\\mathrm{z}_{L}^{0}\\). The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nclass ClassifierHead(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n        self.fc2 = nn.Linear(config.mlp_dim, config.num_classes)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_classes)\n        \"\"\"\n        # Use the CLS token for classification\n        cls_token = x[:, 0, :]\n        x = F.relu(self.fc1(cls_token))\n        \n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.4 Transformer Encoder Block",
    "text": "1.4 Transformer Encoder Block\n至此，我们已经讲完了ViT与Transformer的主要不同之处。接下来，就是Transformer的Encoder。 \n这部分，和Transformer原本的Encoder很类似，只不过有几处不同：\n\nPre-Norm: 在ViT同，输入先进行一个LayerNorm，然后在传入MHA或者MLP中，反观在Transformer原本的Encoder中，我们是先将MHA或者MLP的输出与输入加在一起，之后再进行一个Normalization。这叫做Post-Norm\nMLP的实现：在Transformer Encoder中，用的是 ReLU, 而在ViT中，用的是 GELU\n\n除此之外，其他部分都是一样的。一下是ViT Encoder的实现：\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.norm1 = LayerNorm(config.hidden_dim)\n        self.mha = MHA(config)\n        \n        self.norm2 = LayerNorm(config.hidden_dim)\n        self.ffn = FFN(config)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Multi-head attention\n        redisual = x\n        x = self.norm1(x)\n        x = redisual + self.mha(x)\n\n        # Feed-forward network\n        redisual = x\n        x = self.norm2(x)\n        x = x + self.ffn(x)\n\n        return x",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.5 CNN vs. ViT： Inductive bias",
    "text": "1.5 CNN vs. ViT： Inductive bias\n至此，我们已经介绍完了Vision Transformer，我们来从Inductive Bias 的方面，看看 CNN 和 ViT 有什么不同\n\n\n\n\n\n\n什么是Inductive Bias\n\n\n\n在深度学习里，Inductive Bias（归纳偏置）是指模型在学习之前，因结构或设计而自带的假设或先验。\n\n\n对于图像来说，常见的先验就是：\n\n局部像素是相关的（locality）\n相邻区域的模式有规律（2D neighborhood）\n物体无论出现在图像哪里，识别方式应该一样（translation equivariance）\n\n🔹 2. CNN 的结构怎么体现这些偏置？ 1. 局部性 (Locality) • 卷积核（例如 3×3）只和局部像素打交道，而不是全图。 • 这意味着模型“相信”图像的重要特征来自局部邻域，而不是遥远区域。 2. 二维邻域结构 (2D structure) • 卷积操作是沿着 图像的二维网格进行的，天然利用了图像的行列结构。 • 这和文本（序列 1D）不一样，CNN 明确知道输入是 2D 排列的。 3. 平移等变性 (Translation equivariance) • 卷积核的参数在整张图共享。 • 所以猫在左上角还是右下角，卷积核都能检测到“猫耳朵”。 • 这让 CNN 自动具有“识别位置无关”的能力。\n这些性质不是模型通过训练学出来的，而是因为 卷积操作本身的数学结构就带来的： • kernel 的局部连接 → 局部性 • kernel 滑动覆盖全图 → 平移等变性 • 操作在二维空间定义 → 邻域结构 • 所以，哪怕你不给 CNN 喂太多数据，它也会利用这些偏置去学习特征。\n而对于 ViT 来说： ViT 的归纳偏置非常弱，几乎完全依赖数据和训练来学习。 1. Patch 切分 (Patchification) • ViT 唯一的“图像先验”之一就是把输入图片切成 patch。 • 这一操作隐含了：图像是一个二维结构，可以被分块处理。 2. 位置编码 (Positional Embeddings) • Transformer 本身只处理序列，没有空间结构的概念。 • ViT 通过加位置编码告诉模型 patch 在图像中的相对位置。 • 在输入分辨率变化时，会做 二维插值 (2D interpolation) 来适配，这也是一种人工引入的 2D 先验。 3. 其他部分 • 除了以上两点，ViT 的注意力机制是 全局的 (global)，没有局部性约束。 • 没有像 CNN 那样内置的平移等变性或局部邻域结构。\n这样就是为什么ViT需要更多数据和计算才能学到同样的空间归纳规律。",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "1.6 ViT Model Variants",
    "text": "1.6 ViT Model Variants\nViT 有3种不同的基本变形， 如下图所示 \nViT的名字通常表示为: ViT-L/16: 意思是，ViT-Large，然后用的16 Patch Size。 需要注意的是，Patch Size越大，我们得到的tokens就越少，也就是需要更少的训练时实现。",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#减少tokens",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#减少tokens",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "3.1 减少Tokens",
    "text": "3.1 减少Tokens\n\nPatch Merge\nPatch Shuffle",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "3.2 Vision Language Model",
    "text": "3.2 Vision Language Model\n我们以及学习了ViT for computer Vision， Transformer for NLP， 接下来有什么办法让这两种模型结合起来呢？ CLIP (2021): 将 ViT 融合到 vision-language 预训练中。",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer/Vision-Transformer.html#axial-attention轴向注意力",
    "href": "posts/02-vision-transformer/Vision-Transformer.html#axial-attention轴向注意力",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)",
    "section": "4.1 Axial Attention（轴向注意力）",
    "text": "4.1 Axial Attention（轴向注意力）\n在处理 图像或视频 这类高维输入时，如果直接对所有像素做 全局 self-attention，复杂度是 \\(\\mathcal{O}(H^2 W^2)\\) （\\(H, W\\) 是高和宽）。当图像很大时，这个代价太高。 核心想法：把二维 attention 拆成两次一维 attention（沿着图像的两个“轴”分别做）。 1. Row-wise Attention（行注意力） • 沿着水平方向（宽度轴 W）做注意力，每一行的像素互相关注。 • 复杂度：\\(\\mathcal{O}(H \\cdot W^2)\\)。 2. Column-wise Attention（列注意力） • 沿着垂直方向（高度轴 H）做注意力，每一列的像素互相关注。 • 复杂度： \\(\\mathcal{O}(W \\cdot H^2)\\)。\n组合起来，相当于在 H 和 W 两个轴上都做了全局依赖建模。\n\n\n\n\n\n\nFigure 5",
    "crumbs": [
      "02 Vision Transformer",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(**ViT**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "“Swin V2” (Liu et al. 2022) 是在原始 Swin Transformer 的基础上，为了更好地 扩展模型容量（更多参数）、处理高分辨率输入 以及 提高训练稳定性 所做的一系列改进。 在视觉任务中，Transformer 模型若要变得更强（更多参数、更高分辨率输入、更多层数）就会遇到几个挑战： 1. 训练不稳定：随着模型变深、通道变宽，内部激活的幅度可能急剧增长，导致梯度、数值不稳定。 2. 分辨率迁移问题：模型在低分辨率下预训练（例如 224×224）后，用在高分辨率（例如 1,536×1,536）或不同窗口尺寸时表现会下降。 3. 对标注数据的过度依赖：大模型需要大量标注数据才能训练得好。\nSwin V2 就是为了克服这些障碍，支持训练超大模型（如 30 亿参数级别），同时能处理大尺寸输入 \n\n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(self.norm1(x))\n        \n        shortcut = x \n        ... # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n        \n        ...\n        \n        return x \n\n\n\nclass WindowAttention:\n    def __init__(self,):\n    \n        ...\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n        ...\n    \n    \n    def forward(self, x):\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01))).exp()\n        attn = attn * logit_scale\n\n\n\nlog-spaced continuous position bias approach to address the issue in transferring across window resolutions\n\\[\n\\begin{split}\n\\widehat{\\Delta x} &= \\operatorname{sign}(x) \\cdot \\log(1 + |\\Delta x|) \\\\\n\\widehat{\\Delta y} &= \\operatorname{sign}(y) \\cdot \\log(1 + |\\Delta y|)\n\\end{split}\n\\]\nself.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                             nn.ReLU(inplace=True),\n                             nn.Linear(512, num_heads, bias=False))\n                             \ndef forward(self, x):\n    relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#训练技巧",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#训练技巧",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "5.2 训练技巧",
    "text": "5.2 训练技巧\n\nDropout Path\nGradient Checkpoint:\n\n需要可重现前向 • 被 checkpoint 的模块必须是 纯函数，即输出只依赖输入，不能依赖随机数、全局状态。\n\n\nCite to CLIP",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/04-clip/CLIP.html",
    "href": "posts/04-clip/CLIP.html",
    "title": "04: Learning Transferable Visual Models From Natural Language Supervision(CLIP)",
    "section": "",
    "text": "1 CLIP\nThis is the link to the Vision-Transformer\nThis is another link to the Transformer\nTHis is link to the Swin-Transformer\n\n\n2 Summary\n\n\n3 Key Concepts\n\n\n4 Q & A\n\n\n5 扩展\n\n\n\n\n Back to top",
    "crumbs": [
      "04 Clip",
      "04: Learning Transferable Visual Models From Natural Language Supervision(**CLIP**)"
    ]
  },
  {
    "objectID": "posts/01-transformer/Transformer.html",
    "href": "posts/01-transformer/Transformer.html",
    "title": "Attention is All You Need(Transformer)",
    "section": "",
    "text": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations  Attention_is_all_you_need, p.2  THis\n\n\n\n\n Back to top",
    "crumbs": [
      "01 Transformer",
      "Attention is All You Need(**Transformer**)"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html",
    "href": "00-how-to-read-paper.html",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "在深入学习人工智能相关论文之前，掌握高效且系统的阅读方法至关重要。论文阅读经典指南 How to Read a Paper 中提出了“三遍阅读法”，为我们提供了清晰的实践路径：\n\n\n\n\n第一遍：快速浏览，获取论文的整体结构和核心结论。聚焦于标题（title）、摘要（abstract）、引言（introduction）与结论（conclusion），从宏观上了解文章的研究方向。\n第二遍：细读论文，关注理论推导、实验设置与关键图表。重点理解论文所提出的方法、实验设计、结果分析及其支撑逻辑。\n第三遍：批判性阅读，系统分析论文的优劣，提出建设性问题，反思该方法是否具有通用性或是否能应用于自身研究。\n\n\n\nListing 1: 三遍阅读法\n\n\n\n需要强调的是，阅读论文是一个循环迭代、逐步深入的过程。在反复阅读与思考中，我们会不断修正理解、加深认知。与此同时，通过与他人交流、参与讨论，有助于拓宽视角、深化思考。（本项目网站也因此而生，旨在为研究者们提供一个共同交流学习的平台）\n为提升阅读效率，我们建议在精读阶段，使用颜色标记不同内容。例如在 Harvard CS197 AI Research Experiences 的 Lecture 3 中给出的策略：\n\n黄色：突出论文所试图解决的核心问题或挑战。\n绿色：标记论文中的提到的通用概念。\n粉色：对应提出的算法方法或技术路径。\n橙色：用于标记论文的创新点与贡献。\n\n例如如下示意图：\n\n\n\n\n\n\nFigure 1: Highlight Example of paper (Attention is all you need)\n\n\n\n这套方法并非唯一标准，关键在于构建一套适合自身认知方式的可视化标记体系。统一的标注风格，有助于在后期回顾或跨论文比较时高效定位关键信息。本项目所收录的每篇论文笔记也将采用这一结构化高亮标记方法。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#论文阅读指南",
    "href": "00-how-to-read-paper.html#论文阅读指南",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "在深入学习人工智能相关论文之前，掌握高效且系统的阅读方法至关重要。论文阅读经典指南 How to Read a Paper 中提出了“三遍阅读法”，为我们提供了清晰的实践路径：\n\n\n\n\n第一遍：快速浏览，获取论文的整体结构和核心结论。聚焦于标题（title）、摘要（abstract）、引言（introduction）与结论（conclusion），从宏观上了解文章的研究方向。\n第二遍：细读论文，关注理论推导、实验设置与关键图表。重点理解论文所提出的方法、实验设计、结果分析及其支撑逻辑。\n第三遍：批判性阅读，系统分析论文的优劣，提出建设性问题，反思该方法是否具有通用性或是否能应用于自身研究。\n\n\n\nListing 1: 三遍阅读法\n\n\n\n需要强调的是，阅读论文是一个循环迭代、逐步深入的过程。在反复阅读与思考中，我们会不断修正理解、加深认知。与此同时，通过与他人交流、参与讨论，有助于拓宽视角、深化思考。（本项目网站也因此而生，旨在为研究者们提供一个共同交流学习的平台）\n为提升阅读效率，我们建议在精读阶段，使用颜色标记不同内容。例如在 Harvard CS197 AI Research Experiences 的 Lecture 3 中给出的策略：\n\n黄色：突出论文所试图解决的核心问题或挑战。\n绿色：标记论文中的提到的通用概念。\n粉色：对应提出的算法方法或技术路径。\n橙色：用于标记论文的创新点与贡献。\n\n例如如下示意图：\n\n\n\n\n\n\nFigure 1: Highlight Example of paper (Attention is all you need)\n\n\n\n这套方法并非唯一标准，关键在于构建一套适合自身认知方式的可视化标记体系。统一的标注风格，有助于在后期回顾或跨论文比较时高效定位关键信息。本项目所收录的每篇论文笔记也将采用这一结构化高亮标记方法。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#工具准备",
    "href": "00-how-to-read-paper.html#工具准备",
    "title": "00: Preparation for Following",
    "section": "2 工具准备",
    "text": "2 工具准备\n科学阅读离不开合适的工具支撑。以下是推荐的工具体系，涵盖文献管理、笔记整理、代码执行等多个维度。\n\n2.1 文献管理：Zotero\n随着论文积累的增多，系统的文献管理工具不可或缺。Zotero 是一款免费且开源的文献管理平台，支持自动导入、分组管理与多格式引用（如 BibTeX）。其可扩展性极强，支持插件与主题定制。\n\n\n\n\n\n\nFigure 2: Example of Zotero\n\n\n\n推荐插件：\n\nBetter BibTex：增强 BibTeX 导出功能，便于与 LaTeX 无缝集成。\nEthereal Style：为 Zotero 提供美观的 UI 风格，提升使用体验。\n\n尽管 Zotero 存在一定学习曲线，但其长期价值远超初期投入。若仅希望临时阅读，PDF 阅读器亦可；但从科研视角出发，建议尽早投入学习与使用。\n此外，Zotero Chrome Connector 插件可实现一键导入网页文献，极大提升文献收集效率：\n\n\n\n\n\n\nFigure 3: Zotero Chrome Connector\n\n\n\n如 Figure 3 所示，只需点击插件按钮，即可将当前网页内容导入至文献库。\n\n\n2.2 笔记记录：Obsidian\nObsidian 是一款基于 Markdown 的笔记系统，支持双向链接与图谱视图，特别适合用于构建个人知识体系。\n\n\n\n\n\n\nFigure 4: Obsidian Example\n\n\n\n推荐插件：\n\nobsidian-latex-suite：提供 LaTeX 快捷输入与公式预览功能，显著提高数学表达效率。\nHighlightr Plugin：支持自定义高亮颜色，便于分类信息标注。\n\n \n需要注意的是，过度美化界面或插件堆叠可能反而分散注意力。建议以“结构清晰、内容为本”为首要原则。\n对于不使用 Obsidian 的用户，也可选择：\n\n\n\n\n\n\n\n\n\n\n\n(a) Notion Home Page\n\n\n\n\n\n\n\n\n\n\n\n(b) FeiShu Home Page\n\n\n\n\n\n\n\nFigure 5: Home Page of Notion and FeiShu\n\n\n\n\nNotion：如 Figure 5 (a) 所示，适合多人协作与可视化编辑。\n飞书：如 Figure 5 (b) 所示，功能全面，适合企业级文档管理。\n\n\n\n2.3 代码执行：Jupyter Notebook\n在“Paper with Code”理念下，每篇论文将配套 Jupyter Notebook 实现核心算法。其交互式文档特性，使其成为学习与验证代码的理想平台。\n\n\n\n\n\n\nNote\n\n\n\n若对 Jupyter Notebook 不熟悉，推荐参考 官方文档，以快速入门。\n\n\n相应的代码，我会放在GitHub的仓库中\n\n\n\n\n\n\nFigure 6: The preview of GitHub Page\n\n\n\n\n\n2.4 GPU 平台：云端执行环境\n深度学习模型常需 GPU 加速，若本地无 GPU 可使用以下平台：\n\nGoogle Colab：Google 提供的免费云端 Notebook 平台，支持 GPU 与 TPU。\nKaggle Kernels：支持 GPU 的数据科学平台，适合快速实验。\n\n国内可选平台：\n\nAutoDL：适合国内用户，配置简单，支持定制化部署。\n\n其他推荐：\n\nRunPod、Lambda Labs：提供稳定、低延迟的 GPU 训练环境，适合中大型实验任务。\n\n\n通过合理配置上述工具，可以构建出一个系统化、高效的论文学习与研究流程。在接下来的章节中，每篇论文将附带代码实现、结构解析与批判性思考，欢迎共同学习交流。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#总结",
    "href": "00-how-to-read-paper.html#总结",
    "title": "00: Preparation for Following",
    "section": "3 总结",
    "text": "3 总结\n在本节中，我们介绍了高效阅读论文的方法论与工具体系。通过“三遍阅读法” Listing 1， 我们可以系统地理解论文内容，并在此基础上进行批判性思考。同时，借助 Zotero Section 2.1、ObsidianSection 2.2 等工具，可以有效管理文献、记录笔记与执行代码。 在后续章节中，我们将应用这些方法与工具，深入分析每篇论文的核心思想、实验设计与创新贡献。希望通过本项目的学习，能够帮助大家更好地掌握人工智能领域的前沿研究动态，并在实践中不断提升自己的科研能力。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html",
    "href": "posts/03-swin-transformer/Swin-Transformer.html",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "在阅读Swin Transformer 这篇文章之前，让我们回顾一下 Vision-Transformer 讲的是什么： ViT 在处理图像时，会将整张图像分割成固定大小的patch，并进行global self-attention的计算，从而捕捉图像中的全局信息。 然而， 这种方法存在两个核心的问题：\n\n计算复杂度高: ViT的计算复杂度为 \\(\\mathcal{O}(( \\frac{HW}{P^2})^2)\\)，其中\\(H\\)和\\(W\\)分别是图像的高度和宽度，而\\(P\\)是patch的大小。对高分辨率图像（High Resolution)（如检测或分割任务）来说，token 数量巨大，计算和显存开销难以承受\n缺乏局部特征建模: ViT在进行全局自注意力计算时，可能会忽略图像中的局部特征\n没有金字塔式层级结构: CNN 的层级结构（从低层局部特征到高层语义特征）非常适合处理多尺度目标, ViT 直接用固定大小 patch flatten 成序列，缺乏层次表示，难以适应密集预测任务（如目标检测、语义分割）。\n\n\nWe observe that significant challenges in transferring its high performance in the language domain to the visual domain can be explained by differences between the two modalities. One of these differences involves scale … visual elements can vary substantially in scale … Another difference is the much higher resolution of pixels in images compared to words in passages of text … as the computational complexity of its self-attention is quadratic to image size.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.2 \n\n为了解决这些问题， Swin Transformer (Liu, Lin, et al. 2021) 提出一种新的基于 Vision Transformer的架构它通过引入层次化的特征表示(Hierarchical Architecture) 和 移动窗口机制(Shifted Window MSA, SW-MSA)，来有效地捕捉图像中的局部和全局信息， 并且通过局部窗口(Window-based Multi-head Self Attention, W-MSA) 注意力，来降低训练的时间复杂度。\n\n接下来，我们来详细介绍 Swin Transformer 的架构和关键技术。我们首先来看Swin Transformer 的Attention的实现。\n\n\nW-MHA 的核心思想是：\n\n把图像划分成固定大小的窗口（window），比如 7×7 patch 的窗口。\n在窗口内的 token 之间做局部自注意力，而不是在整张图像的所有 token 之间做全局注意力。\n每个窗口独立计算 Multi-Head Attention → 降低计算量，并且我们可以并行的计算\n\n这样一来：\n\n单个窗口 token 数量固定 = \\(M^{2}\\)（如 7×7=49）。\n注意力计算复杂度从 \\(\\mathcal{O}((hw)^{2}C)\\) 降低为 \\(\\mathcal{O}(M^{2}hwC)\\)，其中 \\(M \\ll \\sqrt{ N }\\)。\n\n除了降低计算复杂度之外，W-MHA，还有保留CNN 在图像处理中强大的一点是 局部感受野 和 平移不变性。\n\nW-MHA 通过窗口限制，使得注意力机制也具备类似的局部归纳偏置（inductive bias），适合图像建模。\n\n\nFor efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n\n\n\nW-MHA 很好，但是它存在的一个问题就是：\n\n窗口之间是相互独立的，缺少跨窗口的信息交流。这会导致，模型只能看见局部，不能获得全局的信息。\n\n\nThe window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n为了解决这个问题，Swin- Transformer提出来 Shifted Window Mulit-Head-Attention (SW-MHA) 窗口位置相对前一层平移，比如 7×7 窗口 → 平移 3 个 patch。 这样，新的窗口会跨越原来的边界，token 会和相邻窗口的 token 一起计算注意力。 相当于强制跨窗口交互，让信息可以在不同区域之间流动。\n 如上如所示，我们将Window通过向左上角移动，通过给图片增加Padding来，但是这种办法显然会增加计算的复杂度。Swin Transformer用了一种很聪明的办法，叫做 Cycling Shift，这种方法就是将将一个张量或图像在某个维度上做 平移，但不是把移出去的部分丢掉，而是 重新从另一边补回来。就像“环形队列”或“钟表走一圈又回到起点”。 如下图所示 \n可以看到，通过Cycling Shift，我们得到的每个window的内容，和之前是一样的，但是所需要的Window的数量，小了很多，这也就意味着，所需要的时间复杂度，也小了很多。\n\n不过Cycling Shift也有一个问题，就是同一个窗口里面，可能有来自不同图片的信息，这些信息在原图片上不是相邻的，自然不应该相互交流信息。我们可以将图片，抽象成下图的形式。组织Attention交流，很自然的一种方法是利用Mask，就像Transformer里的Causal Mask一样。但是，这个Mask长什么样子呢\n\n我们可以看一下Mask，如下图所示，有颜色的区域表示Mask == 1， 在此为了更好的\n\n\n\n\n\nSwin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n\n\\[\n\\begin{split}\n\\hat{z}^l &= \\text{W-MSA} \\left( \\text{LN} \\left( z^{l-1} \\right) \\right) + z^{l-1} \\\\\nz^l &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^l \\right) \\right) + \\hat{z}^l \\\\\n\\hat{z}^{l+1} &= \\text{SW-MSA} \\left( \\text{LN} \\left( z^l \\right) \\right) + z^l \\\\\nz^{l+1} &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^{l+1} \\right) \\right) + \\hat{z}^{l+1}\n\\end{split}\n\\]\n将W-MSA 和 SW-MSA叠在一起，就得到了Transformer Block，当然，还有一个MLP，Layer Normalization，在此就不赘述了。\n\n\n\n讲完了W-MHA，和SW-MHA，我们就理解了Swin- Transformer中最难理解，也是最终的部分，接下来我们看看其他简单的部分。 Patch Merge , 图中绿色的部分，逐步降低 token 数量（降采样），同时增加特征维度的操作。这类似于CNN中的操作，随着层数的增加，分辨率逐步降低、通道数逐步增加，这样既减少了计算量，又能提取层级特征。具体的实现：\n\n分组：将相邻的 2×2 patch 合并成一个新的 patch。\n\n假设输入特征大小为 (H, W, C)。\n每 2×2 的 patch → 合并为 1 个新 token。\n新特征图大小变为 (H/2, W/2, 4C)。\n\n线性变换:\n\n将合并后的 4C 维特征通过一个 线性层 (Linear Projection)，降到 2C 维。\n输出维度翻倍（2C），以补偿分辨率减半带来的信息损失。 🔹 为什么提出 Patch Merging\n\n\n分层表示 (Hierarchical Representation) • 模仿 CNN 的金字塔结构，从局部细节逐步聚合到全局语义。 • 有利于下游任务（检测、分割）中不同尺度的目标建模。\n计算效率 • token 数量逐层减少 → Attention 的复杂度大幅下降。 • 保证模型可扩展到大分辨率图像。\n语义信息聚合 • 通过合并相邻 patch，模型能把更大感受野的信息整合到新的 token 中。\n\n\nx = x.view(B, H, W, C)\n\nx0 = x[:, 0::2, 0::2, :]  # (B, H/2, W/2, C)\nx1 = x[:, 1::2, 0::2, :]  # (B, H/2, W/2, C)\nx2 = x[:, 0::2, 1::2, :]  # (B, H/2, W/2, C)\nx3 = x[:, 1::2, 1::2, :]  # (B, H/2, W/2, C)\n\nx = torch.cat([x0, x1, x2, x3], -1)  # (B, H/2, W/2, 4*C)\nx = x.view(B, -1, 4 * C)  # (B, H/2*W/2, 4*C)\n\nx = self.reduction(x)  # (B, H/2*W/2, 2*C)\n\n\n\n与Transformer 和 Vision-Transformer 中不同的是，Swin Transformer利用的是Relative Position Encoding。\n\\[\n\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^{T}}{\\sqrt{ d }} +B\\right) V\n\\]\n1.  定义偏置表 (relative_position_bias_table)\n•   大小是 (2*Wh-1) * (2*Ww-1, num_heads)\n•   意味着窗口内的任意两个 token 的相对位置 (dx, dy)，都有一个可学习的偏置值（每个 head 一份）。\n•   例如窗口是 7×7 → 相对位置范围是 [-6,6]，所以表大小是 13×13=169，每个位置存一组偏置\n\n\n2.  计算相对位置索引 (relative_position_index)\n•   首先生成窗口内每个 token 的坐标。\n•   然后做差，得到任意两个 token 的相对坐标 (dx, dy)。\n•   再映射成表的索引（通过移位和哈希成一个整数 index）。\n•   结果是一个 (Wh*Ww, Wh*Ww) 的矩阵，每个元素存两个 token 之间在 bias 表里的索引。\n\n\n•   在图像里，相对位置比绝对位置更重要：\n•   比如一个像素的左邻和右邻很相似，无论这个像素在图像的哪个地方。\n\n\n和 Vision-Transformer 一样，当输入的图片和训练时不一样，我们可以通过 bi-cubic interpolation 来增大Relative Position\n\nThe learnt relative position bias in pre-training can be also used to initialize a model for fine-tuning with a different window size through bi-cubic interpolation  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\n\n\n\n除了以上几个，Swin Transformer 中还有其他Component，比如 ：\n\nPatch Embedding\nLinear Projection\nFeedForward\nLayer Normalization 在此，就不赘述了，有需要的同学，请参考前一篇 Vision-Transformer， 或者 Transformer\n\n\n\n\n当一场图片传入Swin Transformer， 它可以提取出图片的特征。 \\[\n\\mathrm{z} = f_{\\theta}(\\mathrm{x}), \\quad \\text{where}\\ \\mathrm{x} \\in \\mathbb{R}^{3\\times H \\times W}, \\mathrm{z} \\in \\mathbb{R}^{H'W' \\times C}\n\\]\n一张图片转化成了 \\(H'W'\\) 个特征，每个特征的大小为 $C。\nSwin Transformer 可以有当作基本的backbone，在此基础上，我们可以对下游进行不同的任务，比如：\n\nImage Classification\nObject Detection\nSemantic segmentation\n\n接下来，我们将如何用Swin Transformer在不同的任务中\n\n\n\n对于 \\(\\mathrm{z}\\) 的 hidden states，我们可以进行一个Average Pooling，对于每一个特征求均值，然后再将这个传入一个分类头，就可以得到我们Classification了。与 Vision-Transformer 不同的是，Swin Transformer 没有 [CLS] token 来当收集全部的信息。\n\n\n\n\nBackbone (Swin Transformer)：\n\nStage 1: [N, C1, H/4, W/4]\nStage 2: [N, C2, H/8, W/8]\nStage 3: [N, C3, H/16, W/16]\nStage 4: [N, C4, H/32, W/32]\n\n可以得到 FPN(Lin et al. 2017) to create a pyramid of feature maps suitable for detection.\n{P2: [N, C, H/4, W/4], \n P3: [N, C, H/8, W/8], \n P4: [N, C, H/16, W/16], \n P5: [N, C, H/32, W/32]}\n有了这些FPN 之后，我们可以结合不同的算法，来进行不同的任务，比如\n例子 1：目标检测 (Object Detection)\n以 Swin Transformer + Faster R-CNN (Ren et al. 2016) 为例： 1. 输入图像：一张 800×1333 的 COCO 数据集图片。\n3.  FPN (特征金字塔网络)：将多尺度特征融合，形成统一的金字塔特征。\n4.  RPN (Region Proposal Network)：在特征图上生成候选区域。\n5.  RoI Head：对候选区域进行分类 (车、人、狗…) 和边框回归。\n6.  输出：预测结果，例如：\n•   “一辆车” → 边框 (x1,y1,x2,y2) + 类别 “car”\n•   “一个人” → 边框 + 类别 “person”\n 👉 在 COCO 数据集上，Swin-T + Faster R-CNN比 ResNet-50 + Faster R-CNN 的 mAP 提高约 5~6 个点。\n语义分割 (Semantic Segmentation)  以 Swin Transformer + UPerNet(Xiao et al. 2018)为例： 1. 输入图像：一张 512×512 的 ADE20K 数据集图片。 2. Backbone (Swin Transformer)：同样输出 1/4, 1/8, 1/16, 1/32 四个尺度特征。 3. FPN/UPerNet Head： • 将多层特征融合，对应不同语义层级。 • 利用融合后的特征生成像素级预测。 4. 预测图 (segmentation map)：大小 512×512，每个像素属于一个类别。 • [0,0] 像素 → “sky” • [100,150] 像素 → “building” • [200,300] 像素 → “road” 5. 输出：完整的语义分割图，每个像素都有类别标签。\n👉 在 ADE20K 上，Swin-L + UPerNet 的 mIoU 达到 53.5+，比传统 CNN backbone 提升显著。 具体的实现细节，等到以后我们阅读到关于Segmentation的内容在，再来实现\n\n\n\n\nWe employ an AdamW optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We include most of the augmentation and regularization strategies of in training, except for repeated augmentation and EMA, which do not enhance performance.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\n\n论文中还用到了 DropPath 来当作一种 Regularization。 DropPath 也称之为 Stochastic Depth (Huang et al. 2016) , 它是一种应用在Residual Network， 在训练过程中，随机丢弃整个 残差分支 (residual branch) 或 整个路径 (path)。减少过拟合，同时让模型学会依赖不同深度的路径，提升训练稳定性。 \n与Dropout 不同的是， Dropout 丢弃的是单个神经元的输出， 而DropPath 丢弃的是整个残差分支 / 整层 Block\n\n\n\n\n\n\n\n\n特性\nDropout (经典)\nDropPath (Stochastic Depth)\n\n\n\n\n丢弃对象\n单个神经元的输出\n整个残差分支 / 整层 Block\n\n\n应用粒度\n逐元素 (element-wise)\n层级 (layer-wise)\n\n\n使用场景\n全连接层、CNN、RNN 等\n残差网络、Transformer 等\n\n\n推理阶段效果\n不丢弃，使用缩放补偿\n不丢弃，保留完整路径\n\n\n作用\n减少神经元过拟合\n防止深层网络过拟合、提升稳定性\n\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n        \n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(x)\n        \n        shortcut = x \n        ... # FFN\n        x = shortcut + self.drop_path(x)\n        \n        ...\n        \n        return x \n\n📝 TAKEAWAY DropPath（也叫 Stochastic Depth）是一种正则化方法，它在训练时随机跳过（丢弃）整个网络层或分支的计算，以减少过拟合并提高模型的泛化能力。\n\n\n\n\n在此，我们在介绍一个训练方法，用于加速训练，叫做Gradient Checkpoint又叫做Activation Checkpoint， 用PyTorh实现，是很容易的 的，我们只需要call utils.checkpoint\n正常训练流程： 在前向传播（forward）时，每一层的中间激活值（activation）都会保存下来，以便反向传播（backward）时用来计算梯度。 问题是：保存所有中间激活值会消耗大量显存（GPU memory）。 • Gradient Checkpoint 的思路： 并不是保存所有激活值，而是只在部分关键节点（checkpoint）保存激活。 对于未保存的激活值，在反向传播时重新再跑一次前向计算来得到它们，从而节省显存。\n换句话说：用计算换显存。\n🔹 工作机制 1. 在前向传播时： • 模型被切分成若干块（segments）。 • 只保存每一块的输入，丢弃中间的激活。 2. 在反向传播时： • 需要用到梯度时，重新对那一块做一次 forward 来恢复激活。 • 然后正常计算梯度。\n•   增加计算开销：因为要在 backward 时重新做一次 forward。\n•   一般会带来 20%～30% 额外的训练时间。\nimport torch\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        def custom_forward(*inputs):\n            return self.layer2(self.layer1(*inputs))\n        \n        # 对这部分使用 checkpoint\n        x = checkpoint(custom_forward, x)\n        return x\n\n📝 TAKEAWAY  Gradient Checkpointing 是一种 用额外计算换显存 的方法，通过在前向传播时少存激活，反向传播时重算，能让大模型在有限显存下完成训练。\n\n\n\n\n\n“Swin V2” (Liu et al. 2022) 是在原始 Swin Transformer 的基础上，为了更好地 扩展模型容量（更多参数）、处理高分辨率输入 以及 提高训练稳定性 所做的一系列改进。 在视觉任务中，Transformer 模型若要变得更强（更多参数、更高分辨率输入、更多层数）就会遇到几个挑战： 1. 训练不稳定：随着模型变深、通道变宽，内部激活的幅度可能急剧增长，导致梯度、数值不稳定。 2. 分辨率迁移问题：模型在低分辨率下预训练（例如 224×224）后，用在高分辨率（例如 1,536×1,536）或不同窗口尺寸时表现会下降。 3. 对标注数据的过度依赖：大模型需要大量标注数据才能训练得好。\nSwin V2 就是为了克服这些障碍，支持训练超大模型（如 30 亿参数级别），同时能处理大尺寸输入 \n\n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(self.norm1(x))\n        \n        shortcut = x \n        ... # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n        \n        ...\n        \n        return x \n\n\n\nclass WindowAttention:\n    def __init__(self,):\n    \n        ...\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n        ...\n    \n    \n    def forward(self, x):\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01))).exp()\n        attn = attn * logit_scale\n\n\n\nlog-spaced continuous position bias approach to address the issue in transferring across window resolutions\n\\[\n\\begin{split}\n\\widehat{\\Delta x} &= \\operatorname{sign}(x) \\cdot \\log(1 + |\\Delta x|) \\\\\n\\widehat{\\Delta y} &= \\operatorname{sign}(y) \\cdot \\log(1 + |\\Delta y|)\n\\end{split}\n\\]\nself.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                             nn.ReLU(inplace=True),\n                             nn.Linear(512, num_heads, bias=False))\n                             \ndef forward(self, x):\n    relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "W-MHA 的核心思想是：\n\n把图像划分成固定大小的窗口（window），比如 7×7 patch 的窗口。\n在窗口内的 token 之间做局部自注意力，而不是在整张图像的所有 token 之间做全局注意力。\n每个窗口独立计算 Multi-Head Attention → 降低计算量，并且我们可以并行的计算\n\n这样一来：\n\n单个窗口 token 数量固定 = \\(M^{2}\\)（如 7×7=49）。\n注意力计算复杂度从 \\(\\mathcal{O}((hw)^{2}C)\\) 降低为 \\(\\mathcal{O}(M^{2}hwC)\\)，其中 \\(M \\ll \\sqrt{ N }\\)。\n\n除了降低计算复杂度之外，W-MHA，还有保留CNN 在图像处理中强大的一点是 局部感受野 和 平移不变性。\n\nW-MHA 通过窗口限制，使得注意力机制也具备类似的局部归纳偏置（inductive bias），适合图像建模。\n\n\nFor efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "W-MHA 很好，但是它存在的一个问题就是：\n\n窗口之间是相互独立的，缺少跨窗口的信息交流。这会导致，模型只能看见局部，不能获得全局的信息。\n\n\nThe window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n为了解决这个问题，Swin- Transformer提出来 Shifted Window Mulit-Head-Attention (SW-MHA) 窗口位置相对前一层平移，比如 7×7 窗口 → 平移 3 个 patch。 这样，新的窗口会跨越原来的边界，token 会和相邻窗口的 token 一起计算注意力。 相当于强制跨窗口交互，让信息可以在不同区域之间流动。\n 如上如所示，我们将Window通过向左上角移动，通过给图片增加Padding来，但是这种办法显然会增加计算的复杂度。Swin Transformer用了一种很聪明的办法，叫做 Cycling Shift，这种方法就是将将一个张量或图像在某个维度上做 平移，但不是把移出去的部分丢掉，而是 重新从另一边补回来。就像“环形队列”或“钟表走一圈又回到起点”。 如下图所示 \n可以看到，通过Cycling Shift，我们得到的每个window的内容，和之前是一样的，但是所需要的Window的数量，小了很多，这也就意味着，所需要的时间复杂度，也小了很多。\n\n不过Cycling Shift也有一个问题，就是同一个窗口里面，可能有来自不同图片的信息，这些信息在原图片上不是相邻的，自然不应该相互交流信息。我们可以将图片，抽象成下图的形式。组织Attention交流，很自然的一种方法是利用Mask，就像Transformer里的Causal Mask一样。但是，这个Mask长什么样子呢\n\n我们可以看一下Mask，如下图所示，有颜色的区域表示Mask == 1， 在此为了更好的",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "Swin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n\n\\[\n\\begin{split}\n\\hat{z}^l &= \\text{W-MSA} \\left( \\text{LN} \\left( z^{l-1} \\right) \\right) + z^{l-1} \\\\\nz^l &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^l \\right) \\right) + \\hat{z}^l \\\\\n\\hat{z}^{l+1} &= \\text{SW-MSA} \\left( \\text{LN} \\left( z^l \\right) \\right) + z^l \\\\\nz^{l+1} &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^{l+1} \\right) \\right) + \\hat{z}^{l+1}\n\\end{split}\n\\]\n将W-MSA 和 SW-MSA叠在一起，就得到了Transformer Block，当然，还有一个MLP，Layer Normalization，在此就不赘述了。",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#patch-merge",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#patch-merge",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "讲完了W-MHA，和SW-MHA，我们就理解了Swin- Transformer中最难理解，也是最终的部分，接下来我们看看其他简单的部分。 Patch Merge , 图中绿色的部分，逐步降低 token 数量（降采样），同时增加特征维度的操作。这类似于CNN中的操作，随着层数的增加，分辨率逐步降低、通道数逐步增加，这样既减少了计算量，又能提取层级特征。具体的实现：\n\n分组：将相邻的 2×2 patch 合并成一个新的 patch。\n\n假设输入特征大小为 (H, W, C)。\n每 2×2 的 patch → 合并为 1 个新 token。\n新特征图大小变为 (H/2, W/2, 4C)。\n\n线性变换:\n\n将合并后的 4C 维特征通过一个 线性层 (Linear Projection)，降到 2C 维。\n输出维度翻倍（2C），以补偿分辨率减半带来的信息损失。 🔹 为什么提出 Patch Merging\n\n\n分层表示 (Hierarchical Representation) • 模仿 CNN 的金字塔结构，从局部细节逐步聚合到全局语义。 • 有利于下游任务（检测、分割）中不同尺度的目标建模。\n计算效率 • token 数量逐层减少 → Attention 的复杂度大幅下降。 • 保证模型可扩展到大分辨率图像。\n语义信息聚合 • 通过合并相邻 patch，模型能把更大感受野的信息整合到新的 token 中。\n\n\nx = x.view(B, H, W, C)\n\nx0 = x[:, 0::2, 0::2, :]  # (B, H/2, W/2, C)\nx1 = x[:, 1::2, 0::2, :]  # (B, H/2, W/2, C)\nx2 = x[:, 0::2, 1::2, :]  # (B, H/2, W/2, C)\nx3 = x[:, 1::2, 1::2, :]  # (B, H/2, W/2, C)\n\nx = torch.cat([x0, x1, x2, x3], -1)  # (B, H/2, W/2, 4*C)\nx = x.view(B, -1, 4 * C)  # (B, H/2*W/2, 4*C)\n\nx = self.reduction(x)  # (B, H/2*W/2, 2*C)",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "与Transformer 和 Vision-Transformer 中不同的是，Swin Transformer利用的是Relative Position Encoding。\n\\[\n\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^{T}}{\\sqrt{ d }} +B\\right) V\n\\]\n1.  定义偏置表 (relative_position_bias_table)\n•   大小是 (2*Wh-1) * (2*Ww-1, num_heads)\n•   意味着窗口内的任意两个 token 的相对位置 (dx, dy)，都有一个可学习的偏置值（每个 head 一份）。\n•   例如窗口是 7×7 → 相对位置范围是 [-6,6]，所以表大小是 13×13=169，每个位置存一组偏置\n\n\n2.  计算相对位置索引 (relative_position_index)\n•   首先生成窗口内每个 token 的坐标。\n•   然后做差，得到任意两个 token 的相对坐标 (dx, dy)。\n•   再映射成表的索引（通过移位和哈希成一个整数 index）。\n•   结果是一个 (Wh*Ww, Wh*Ww) 的矩阵，每个元素存两个 token 之间在 bias 表里的索引。\n\n\n•   在图像里，相对位置比绝对位置更重要：\n•   比如一个像素的左邻和右邻很相似，无论这个像素在图像的哪个地方。\n\n\n和 Vision-Transformer 一样，当输入的图片和训练时不一样，我们可以通过 bi-cubic interpolation 来增大Relative Position\n\nThe learnt relative position bias in pre-training can be also used to initialize a model for fine-tuning with a different window size through bi-cubic interpolation  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#others",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#others",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "除了以上几个，Swin Transformer 中还有其他Component，比如 ：\n\nPatch Embedding\nLinear Projection\nFeedForward\nLayer Normalization 在此，就不赘述了，有需要的同学，请参考前一篇 Vision-Transformer， 或者 Transformer",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "当一场图片传入Swin Transformer， 它可以提取出图片的特征。 \\[\n\\mathrm{z} = f_{\\theta}(\\mathrm{x}), \\quad \\text{where}\\ \\mathrm{x} \\in \\mathbb{R}^{3\\times H \\times W}, \\mathrm{z} \\in \\mathbb{R}^{H'W' \\times C}\n\\]\n一张图片转化成了 \\(H'W'\\) 个特征，每个特征的大小为 $C。\nSwin Transformer 可以有当作基本的backbone，在此基础上，我们可以对下游进行不同的任务，比如：\n\nImage Classification\nObject Detection\nSemantic segmentation\n\n接下来，我们将如何用Swin Transformer在不同的任务中\n\n\n\n对于 \\(\\mathrm{z}\\) 的 hidden states，我们可以进行一个Average Pooling，对于每一个特征求均值，然后再将这个传入一个分类头，就可以得到我们Classification了。与 Vision-Transformer 不同的是，Swin Transformer 没有 [CLS] token 来当收集全部的信息。",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "Backbone (Swin Transformer)：\n\nStage 1: [N, C1, H/4, W/4]\nStage 2: [N, C2, H/8, W/8]\nStage 3: [N, C3, H/16, W/16]\nStage 4: [N, C4, H/32, W/32]\n\n可以得到 FPN(Lin et al. 2017) to create a pyramid of feature maps suitable for detection.\n{P2: [N, C, H/4, W/4], \n P3: [N, C, H/8, W/8], \n P4: [N, C, H/16, W/16], \n P5: [N, C, H/32, W/32]}\n有了这些FPN 之后，我们可以结合不同的算法，来进行不同的任务，比如\n例子 1：目标检测 (Object Detection)\n以 Swin Transformer + Faster R-CNN (Ren et al. 2016) 为例： 1. 输入图像：一张 800×1333 的 COCO 数据集图片。\n3.  FPN (特征金字塔网络)：将多尺度特征融合，形成统一的金字塔特征。\n4.  RPN (Region Proposal Network)：在特征图上生成候选区域。\n5.  RoI Head：对候选区域进行分类 (车、人、狗…) 和边框回归。\n6.  输出：预测结果，例如：\n•   “一辆车” → 边框 (x1,y1,x2,y2) + 类别 “car”\n•   “一个人” → 边框 + 类别 “person”\n 👉 在 COCO 数据集上，Swin-T + Faster R-CNN比 ResNet-50 + Faster R-CNN 的 mAP 提高约 5~6 个点。\n语义分割 (Semantic Segmentation)  以 Swin Transformer + UPerNet(Xiao et al. 2018)为例： 1. 输入图像：一张 512×512 的 ADE20K 数据集图片。 2. Backbone (Swin Transformer)：同样输出 1/4, 1/8, 1/16, 1/32 四个尺度特征。 3. FPN/UPerNet Head： • 将多层特征融合，对应不同语义层级。 • 利用融合后的特征生成像素级预测。 4. 预测图 (segmentation map)：大小 512×512，每个像素属于一个类别。 • [0,0] 像素 → “sky” • [100,150] 像素 → “building” • [200,300] 像素 → “road” 5. 输出：完整的语义分割图，每个像素都有类别标签。\n👉 在 ADE20K 上，Swin-L + UPerNet 的 mIoU 达到 53.5+，比传统 CNN backbone 提升显著。 具体的实现细节，等到以后我们阅读到关于Segmentation的内容在，再来实现",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/03-swin-transformer/Swin-Transformer.html#training-details",
    "href": "posts/03-swin-transformer/Swin-Transformer.html#training-details",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "We employ an AdamW optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We include most of the augmentation and regularization strategies of in training, except for repeated augmentation and EMA, which do not enhance performance.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\n\n论文中还用到了 DropPath 来当作一种 Regularization。 DropPath 也称之为 Stochastic Depth (Huang et al. 2016) , 它是一种应用在Residual Network， 在训练过程中，随机丢弃整个 残差分支 (residual branch) 或 整个路径 (path)。减少过拟合，同时让模型学会依赖不同深度的路径，提升训练稳定性。 \n与Dropout 不同的是， Dropout 丢弃的是单个神经元的输出， 而DropPath 丢弃的是整个残差分支 / 整层 Block\n\n\n\n\n\n\n\n\n特性\nDropout (经典)\nDropPath (Stochastic Depth)\n\n\n\n\n丢弃对象\n单个神经元的输出\n整个残差分支 / 整层 Block\n\n\n应用粒度\n逐元素 (element-wise)\n层级 (layer-wise)\n\n\n使用场景\n全连接层、CNN、RNN 等\n残差网络、Transformer 等\n\n\n推理阶段效果\n不丢弃，使用缩放补偿\n不丢弃，保留完整路径\n\n\n作用\n减少神经元过拟合\n防止深层网络过拟合、提升稳定性\n\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n        \n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(x)\n        \n        shortcut = x \n        ... # FFN\n        x = shortcut + self.drop_path(x)\n        \n        ...\n        \n        return x \n\n📝 TAKEAWAY DropPath（也叫 Stochastic Depth）是一种正则化方法，它在训练时随机跳过（丢弃）整个网络层或分支的计算，以减少过拟合并提高模型的泛化能力。\n\n\n\n\n在此，我们在介绍一个训练方法，用于加速训练，叫做Gradient Checkpoint又叫做Activation Checkpoint， 用PyTorh实现，是很容易的 的，我们只需要call utils.checkpoint\n正常训练流程： 在前向传播（forward）时，每一层的中间激活值（activation）都会保存下来，以便反向传播（backward）时用来计算梯度。 问题是：保存所有中间激活值会消耗大量显存（GPU memory）。 • Gradient Checkpoint 的思路： 并不是保存所有激活值，而是只在部分关键节点（checkpoint）保存激活。 对于未保存的激活值，在反向传播时重新再跑一次前向计算来得到它们，从而节省显存。\n换句话说：用计算换显存。\n🔹 工作机制 1. 在前向传播时： • 模型被切分成若干块（segments）。 • 只保存每一块的输入，丢弃中间的激活。 2. 在反向传播时： • 需要用到梯度时，重新对那一块做一次 forward 来恢复激活。 • 然后正常计算梯度。\n•   增加计算开销：因为要在 backward 时重新做一次 forward。\n•   一般会带来 20%～30% 额外的训练时间。\nimport torch\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        def custom_forward(*inputs):\n            return self.layer2(self.layer1(*inputs))\n        \n        # 对这部分使用 checkpoint\n        x = checkpoint(custom_forward, x)\n        return x\n\n📝 TAKEAWAY  Gradient Checkpointing 是一种 用额外计算换显存 的方法，通过在前向传播时少存激活，反向传播时重算，能让大模型在有限显存下完成训练。",
    "crumbs": [
      "03 Swin Transformer",
      "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(**Swin Transformer**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html",
    "href": "posts/05-dino/DINO.html",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "",
    "text": "在我们PwC的第二篇，我们学习了什么是Vision Transformer 以及它的基本架构和工作原理。我们还讨论了 ViT 相对于传统卷积神经网络（CNN）的优势和劣势。 但是ViT存在的主要问题是需要大量标注数据：\n我们在训练CIFAR-10 Classification问题时也可以发现，ViT在其准确度只有 55% 左右。 其主要的原因是：ViT 没有 CNN 的 inductive bias（卷积的平移不变性、局部感受野），需要更多标注来“学会”这些先验。\n而DINO (Caron et al. 2021) 的提出，旨在解决“在没有标签数据情况下，训练 Vision Transformer（ViT）提取有区分性、语义丰富的视觉表示”的问题。该方法旨在利用Self-Supervised Learning方式，通过模型自身蒸馏实现有效预训练。\n在我们了解DINO之前，我们需要先了解几个基本概念，以便我们更好的理解DINO。",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#self-supervised-learningssl",
    "href": "posts/05-dino/DINO.html#self-supervised-learningssl",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.1 Self-Supervised Learning(SSL)",
    "text": "1.1 Self-Supervised Learning(SSL)\nSelf-Supervised Learning是一种Un-supervised learning的方法，通过利用未标记数据中的结构信息来进行特征学习。它通常通过设计预文本任务（pretext tasks）来实现。预文本任务指的是：在没有人工标注的情况下，人为设计一个“伪任务”，让模型通过解决这个任务来学习有用的特征表示。举个例子，在 Language Model中，GPT-2(Radford et al., n.d.) 通过预测下一个单词（Next Token Prediction）来学习语言表示，而BERT(Devlin et al. 2019) 通过掩码语言模型任务（Masked Language Modeling）来学习上下文表示。而对于图片，我们可以通过\n\n遮挡图片的区域（Inpainting）， 来进行自监督学习。\n拼图任务（Jigsaw Puzzle）：把图片分割成小块，打乱顺序，让模型重新排列。\n\n\n一句话总结Self-Supervised Learning就是: 让模型先玩一些自带标签的小任务，从而学会理解数据。",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#knowledge-distillation",
    "href": "posts/05-dino/DINO.html#knowledge-distillation",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.2 Knowledge Distillation",
    "text": "1.2 Knowledge Distillation\n知识蒸馏(Hinton, Vinyals, and Dean 2015)是一种模型压缩技术，通过将一个大型模型（教师模型）的知识转移到一个较小的模型（学生模型）中，从而提高学生模型的性能。这两个模型，通常有以下几个特点：\n\nTeacher 模型通常容量大、性能好，但计算开销高。\nStudent 模型较小，但通过学习 Teacher 的知识，可以在低成本下接近 Teacher 的性能。\n\n知识蒸馏核心思想是，Teacher 输出概率分布 (soft targets) 往往包含着比硬标签（Hard Label）更多的信息，比如类间相似性。其损失函数定义为\n\\[\n\\mathcal{L}_{KD} =\n(1-\\lambda)\\,\n\\underbrace{\\mathcal{L}_{CE}(y, p_s)}_{\\text{Hard Label Loss}} +\n\\lambda \\, T^2 \\,\n\\underbrace{\\mathcal{L}_{KL}(p_t^T, p_s^T)}_{\\text{Soft Label Loss}}\n\\tag{1}\\]\n其中：\n\n\\(p_s, p_t\\): 是 student/teacher 的输出概率分布。\n\\(T\\): 是温度参数，用于控制softmax的平滑程度。\n\\(\\lambda\\): 是一个超参数，用于平衡两种损失的贡献。\n\\(\\mathcal{L}_{CE}\\): 是交叉熵损失函数, 用于衡量学生模型输出与真实标签之间的差距。 \\[\n\\mathcal{L}_{CE}(y, p_s) = -\\sum_{i} y_i \\log(p_{s,i})\n\\tag{2}\\]\n\\(\\mathcal{L}_{KL}\\): 是Kullback-Leibler散度损失函数，用于衡量教师模型输出与学生模型输出之间的差距。\n\n\\[\n\\mathcal{L}_{KL}(p_t, p_s) = \\sum_{i} p_t(i) \\log\\frac{p_t(i)}{p_s(i)}\n\\tag{3}\\]\n\nTakeaway 2 知识蒸馏(Knowledge Distillation)是一种模型压缩(Model Compression)方法，通过让小模型（student）学习大模型（teacher）的输出分布，从而继承其知识并提升小模型性能。",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#exponential-moving-average-ema",
    "href": "posts/05-dino/DINO.html#exponential-moving-average-ema",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.3 Exponential Moving Average (EMA)",
    "text": "1.3 Exponential Moving Average (EMA)\n指数移动平均（EMA）是一种 Weight Average 的方法。在DINO中，EMA被用来更新教师网络的参数，使其在训练过程中更加稳定。具体来说，教师网络的参数是学生网络参数的指数加权平均，这样可以避免训练过程中的剧烈波动，从而提高模型的鲁棒性。\n\\[\nx^{\\text{EMA}}_{t+1} = \\alpha x^{\\text{EMA}}_t + (1 - \\alpha) x_{t+1}, \\quad \\text{where} \\ \\alpha \\in [0, 1]\n\\tag{4}\\]\nEMA 通常用来提升模型的generalization能力。\n\nTakeaway 3 指数移动平均(Exponential Moving Average)通过给新数据更高权重、旧数据指数衰减来平滑更新，从而获得更稳定的参数或信号。\n\n有了这些储备知识，接下来让我们看看DINO是如何利用这些知识来训练模型的。",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#数据处理",
    "href": "posts/05-dino/DINO.html#数据处理",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.1 数据处理",
    "text": "7.1 数据处理\nDINO 在数据处理上采用了多视图增强策略。具体来说，对于每一张输入图像，DINO 会生成多个不同的视图，这些视图通过不同的数据增强技术获得，例如随机裁剪、颜色抖动等。这些增强视图将作为 Student 网络的输入。\n同时，Teacher 网络则只使用全局视图，即对整个图像进行一次前向传播，得到全局特征表示。通过这种方式，Student 网络可以学习到更丰富的局部特征，同时也能与 Teacher 网络的全局特征进行对齐。\n\n\n\n对于一张照片，DINO进行以下几个操作：\n\n颜色抖动（Color Jitter）\n高斯模糊（Gaussian Blur）\n太阳化（Solarization）\n\n并且对每个视图，进行Multi-Crop的策略：\n\n生成 两个 global crop（大视野），\n加若干个 local crop（小视野），\n然后所有裁剪视图都输入到 学生网络\n只有 global views 输入到 教师网络\n\n\n\n\n\n\n \n\n\nFigure 2: DINO Image Pre-process Steps\n\n\n\n\n\n\n下面是几个Data Augmentation的例子\n\n\n\n\n\n\n\n\n\n\n\n(a) Original Image\n\n\n\n\n\n\n\n\n\n\n\n(b) Gaussian Blur\n\n\n\n\n\n\n\n\n\n\n\n(c) Solarized\n\n\n\n\n\n\n\nFigure 3: 两个不同的Data Augmentation Examples\n\n\n\nThe full augmentation pipeline\n\n\n\n\n\n\nFigure 4: The Data Augmentation Pipeline",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#dino-v2",
    "href": "posts/05-dino/DINO.html#dino-v2",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.2 DINO V2",
    "text": "7.2 DINO V2\n\n\n\n\n\n\nFigure 5\n\n\n\n• DINO 1：构建基础框架，开启自监督学习在视觉 Transformer 的应用。 • DINO 2：全面扩展训练规模与技术，使模型成为真正“开箱即用”的视觉基础模型。 • DINO 3：进一步提升规模与技术，解决密集特征退化问题，增强多场景适应性，并推动性能至新高度。",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#dino-v3",
    "href": "posts/05-dino/DINO.html#dino-v3",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.3 DINO V3",
    "text": "7.3 DINO V3",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#dino-v1-vs.-dino-v2-vs.-dino-v3",
    "href": "posts/05-dino/DINO.html#dino-v1-vs.-dino-v2-vs.-dino-v3",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.4 DINO V1 vs. DINO V2 vs. DINO V3",
    "text": "7.4 DINO V1 vs. DINO V2 vs. DINO V3\n\n\nSummary of 4 Tokenization Algorithms\n\n\n\n\n\n\n\n\nDINO Version\n核心创新与改进\n数据规模与模型规模\n应用特性与优势\n\n\n\n\nDINO 1\nstudent–teacher 自监督蒸馏，attention 可视化\n中小规模数据与模型\n可视化特征学习、语义分割能力、低成本训练\n\n\nDINO 2\n大规模蒸馏、FlashAttention、正则化、超强泛化能力\n1.42 亿图像训练数据、多个 ViT 架构\n多任务通用，无需微调，任务覆盖广泛\n\n\nDINO 3\nGram anchoring、轴向 RoPE、多分辨率鲁棒、多模型版本\n7B 参数模型 + 超大数据（1.7B 图像）\n更强密集特征质量，多任务性能达新 SOTA，适应性更强\n\n\n\n\n\nQuestion: Answer: 因为缺少 CNN 的 inductive bias，ViT 只能依靠大数据来学习空间不变性和局部模式。\n\n\nAnswer: 利用教师–学生蒸馏 + 图像增强视图，学生模仿教师输出，从而学习鲁棒特征。\n\n\nAnswer: 让学生学习局部与全局的语义对齐，从而获得更丰富的上下文表示。\n\n\nAnswer: ViT 的 self-attention 在自监督中会自然聚合语义一致的区域，从而表现出对象分割现象。\n\n\nCentering prevents one dimension to dominate but encourages collapse to the uniform distribution, while the sharpening has the opposite effect.  Emerging Properties in Self-Supervised Vision Transformers, p.4 \n\n\nOutput sharpening is obtained by using a low value for the temperature τt in the teacher softmax normalization.  Emerging Properties in Self-Supervised Vision Transformers, p.4",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  },
  {
    "objectID": "posts/05-dino/DINO.html#safe-softmax",
    "href": "posts/05-dino/DINO.html#safe-softmax",
    "title": "Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "8.1 Safe-Softmax",
    "text": "8.1 Safe-Softmax\ndef softmax(x: torch.Tensor, temp):\n    max =",
    "crumbs": [
      "05 Dino",
      "Emerging Properties in Self-Supervised Vision Transformers(**DINO**)"
    ]
  }
]